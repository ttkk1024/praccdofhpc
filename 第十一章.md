# 第十一章：OpenMP并行编程

## 11.1 OpenMP概述

### 11.1.1 OpenMP简介

OpenMP（Open Multi-Processing）是一种用于共享内存并行编程的API标准，支持C/C++和Fortran语言。它通过编译指令（pragma）实现并行化，具有以下特点：

**核心特性：**
- **共享内存模型**：所有线程共享同一内存空间
- **Fork-Join模型**：程序在主线程和并行区域之间切换
- **编译器指令**：通过`#pragma`指令控制并行化
- **可移植性**：跨平台支持（Linux、Windows、macOS）

**适用场景：**
- 多核CPU并行计算
- 循环并行化
- 任务并行
- 数据并行

### 11.1.2 OpenMP架构模型

```
主线程 (Master Thread)
    ↓
创建线程团队 (Team of Threads)
    ↓
并行区域执行 (Parallel Region)
    ↓
线程同步 (Synchronization)
    ↓
合并到主线程 (Join)
```

**线程层次结构：**
- **主线程（Master Thread）**：ID为0的线程
- **工作线程（Worker Threads）**：ID从1开始的并行线程
- **线程团队（Thread Team）**：执行并行区域的所有线程

## 11.2 基本指令详解

### 11.2.1 并行区域指令

#### 基本并行区域
```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("主线程开始执行\n");

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("线程 %d/%d 正在执行并行代码\n", thread_id, num_threads);
    }

    printf("主线程继续执行\n");
    return 0;
}
```

**输出示例：**
```
主线程开始执行
线程 0/4 正在执行并行代码
线程 1/4 正在执行并行代码
线程 2/4 正在执行并行代码
线程 3/4 正在执行并行代码
主线程继续执行
```

#### 设置线程数量
```c
#include <omp.h>

int main() {
    // 方法1：通过环境变量设置
    // export OMP_NUM_THREADS=8

    // 方法2：通过函数设置
    omp_set_num_threads(6);

    #pragma omp parallel
    {
        printf("线程 %d 正在执行\n", omp_get_thread_num());
    }

    return 0;
}
```

### 11.2.2 循环并行化指令

#### 基本并行循环
```c
#include <omp.h>
#include <stdio.h>

#define N 1000

int main() {
    double start_time, end_time;
    double sum = 0.0;

    // 串行版本
    start_time = omp_get_wtime();
    for (int i = 0; i < N; i++) {
        sum += i * i;
    }
    end_time = omp_get_wtime();
    printf("串行计算结果: %f, 时间: %f秒\n", sum, end_time - start_time);

    // 并行版本
    sum = 0.0;
    start_time = omp_get_wtime();

    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        double local_sum = i * i;
        #pragma omp atomic
        sum += local_sum;
    }

    end_time = omp_get_wtime();
    printf("并行计算结果: %f, 时间: %f秒\n", sum, end_time - start_time);

    return 0;
}
```

#### 循环调度策略
```c
#include <omp.h>
#include <stdio.h>

#define N 100

int main() {
    int work[N];

    // 静态调度
    #pragma omp parallel for schedule(static, 10)
    for (int i = 0; i < N; i++) {
        work[i] = i * i;
        printf("线程 %d 处理任务 %d\n", omp_get_thread_num(), i);
    }

    // 动态调度
    #pragma omp parallel for schedule(dynamic, 5)
    for (int i = 0; i < N; i++) {
        work[i] += i;
        printf("线程 %d 动态处理任务 %d\n", omp_get_thread_num(), i);
    }

    // 运行时调度
    #pragma omp parallel for schedule(runtime)
    for (int i = 0; i < N; i++) {
        work[i] *= 2;
        printf("线程 %d 运行时调度处理任务 %d\n", omp_get_thread_num(), i);
    }

    return 0;
}
```

**调度策略详解：**
- **static**：编译时分配，负载均衡但可能不灵活
- **dynamic**：运行时分配，负载均衡但有调度开销
- **guided**：动态调整块大小，平衡负载和开销
- **runtime**：通过环境变量`OMP_SCHEDULE`控制

### 11.2.3 Reduction操作

#### 基本Reduction
```c
#include <omp.h>
#include <stdio.h>
#include <math.h>

#define N 1000000

int main() {
    double pi = 0.0;
    double start_time, end_time;

    // 串行计算π
    start_time = omp_get_wtime();
    for (int i = 0; i < N; i++) {
        double x = (i + 0.5) / N;
        pi += 4.0 / (1.0 + x * x);
    }
    pi /= N;
    end_time = omp_get_wtime();
    printf("串行π计算: %f, 时间: %f秒\n", pi, end_time - start_time);

    // 并行Reduction计算π
    pi = 0.0;
    start_time = omp_get_wtime();

    #pragma omp parallel for reduction(+:pi)
    for (int i = 0; i < N; i++) {
        double x = (i + 0.5) / N;
        pi += 4.0 / (1.0 + x * x);
    }
    pi /= N;

    end_time = omp_get_wtime();
    printf("并行π计算: %f, 时间: %f秒\n", pi, end_time - start_time);

    return 0;
}
```

#### 复杂Reduction操作
```c
#include <omp.h>
#include <stdio.h>
#include <limits.h>

#define N 1000000

int main() {
    int array[N];
    int min_val = INT_MAX;
    int max_val = INT_MIN;
    long long sum = 0;

    // 初始化数组
    for (int i = 0; i < N; i++) {
        array[i] = rand() % 1000;
    }

    // 并行Reduction
    #pragma omp parallel for reduction(min:min_val) reduction(max:max_val) reduction(+:sum)
    for (int i = 0; i < N; i++) {
        if (array[i] < min_val) min_val = array[i];
        if (array[i] > max_val) max_val = array[i];
        sum += array[i];
    }

    printf("最小值: %d, 最大值: %d, 总和: %lld\n", min_val, max_val, sum);
    printf("平均值: %f\n", (double)sum / N);

    return 0;
}
```

## 11.3 数据共享属性

### 11.3.1 数据共享分类

#### Shared（共享变量）
```c
#include <omp.h>
#include <stdio.h>

#define N 100

int main() {
    int shared_array[N];
    int shared_counter = 0;

    // 初始化数组
    for (int i = 0; i < N; i++) {
        shared_array[i] = i;
    }

    #pragma omp parallel for shared(shared_array, shared_counter)
    for (int i = 0; i < N; i++) {
        shared_array[i] *= 2;
        #pragma omp atomic
        shared_counter++;
    }

    printf("共享计数器: %d\n", shared_counter);
    printf("数组前5个元素: ");
    for (int i = 0; i < 5; i++) {
        printf("%d ", shared_array[i]);
    }
    printf("\n");

    return 0;
}
```

#### Private（私有变量）
```c
#include <omp.h>
#include <stdio.h>

#define N 10

int main() {
    int private_var = 100;
    int shared_result = 0;

    printf("主线程中 private_var = %d\n", private_var);

    #pragma omp parallel for private(private_var) reduction(+:shared_result)
    for (int i = 0; i < N; i++) {
        private_var = i * 10;
        printf("线程 %d: private_var = %d\n", omp_get_thread_num(), private_var);
        shared_result += private_var;
    }

    printf("主线程中 private_var = %d\n", private_var);
    printf("共享结果: %d\n", shared_result);

    return 0;
}
```

#### Firstprivate和Lastprivate
```c
#include <omp.h>
#include <stdio.h>

#define N 10

int main() {
    int first_val = 42;
    int last_val = 0;

    printf("初始值: first_val = %d, last_val = %d\n", first_val, last_val);

    #pragma omp parallel for firstprivate(first_val) lastprivate(last_val)
    for (int i = 0; i < N; i++) {
        first_val += i;
        last_val = first_val * 2;
        printf("线程 %d: first_val = %d, last_val = %d\n",
               omp_get_thread_num(), first_val, last_val);
    }

    printf("循环后: first_val = %d, last_val = %d\n", first_val, last_val);

    return 0;
}
```

### 11.3.2 默认数据属性

#### default(shared)
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int x = 10;
    int y = 20;

    #pragma omp parallel default(shared)
    {
        printf("线程 %d: x = %d, y = %d\n", omp_get_thread_num(), x, y);
        x += omp_get_thread_num();
        y += omp_get_thread_num() * 10;
    }

    printf("最终结果: x = %d, y = %d\n", x, y);
    return 0;
}
```

#### default(none)
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int shared_var = 100;
    int private_var = 200;

    #pragma omp parallel default(none) shared(shared_var) private(private_var)
    {
        printf("线程 %d: shared_var = %d, private_var = %d\n",
               omp_get_thread_num(), shared_var, private_var);

        shared_var += omp_get_thread_num();
        private_var = omp_get_thread_num() * 100;
    }

    printf("最终: shared_var = %d\n", shared_var);
    return 0;
}
```

## 11.4 同步机制详解

### 11.4.1 Critical区域

#### 基本Critical
```c
#include <omp.h>
#include <stdio.h>

#define N 1000

int main() {
    int counter = 0;
    int array[N];

    // 初始化数组
    for (int i = 0; i < N; i++) {
        array[i] = i;
    }

    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        // 临界区：保护counter
        #pragma omp critical
        {
            counter++;
            if (counter % 100 == 0) {
                printf("已处理 %d 个元素\n", counter);
            }
        }

        array[i] *= 2;
    }

    printf("最终计数: %d\n", counter);
    return 0;
}
```

#### 命名Critical区域
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int bank_balance = 1000;
    int stock_balance = 500;

    #pragma omp parallel for
    for (int i = 0; i < 100; i++) {
        // 银行账户操作
        #pragma omp critical(bank_operation)
        {
            bank_balance += 10;
            printf("线程 %d: 银行余额更新为 %d\n",
                   omp_get_thread_num(), bank_balance);
        }

        // 股票账户操作
        #pragma omp critical(stock_operation)
        {
            stock_balance += 5;
            printf("线程 %d: 股票余额更新为 %d\n",
                   omp_get_thread_num(), stock_balance);
        }
    }

    printf("最终银行余额: %d, 股票余额: %d\n", bank_balance, stock_balance);
    return 0;
}
```

### 11.4.2 Atomic操作

#### 基本Atomic
```c
#include <omp.h>
#include <stdio.h>

#define N 1000000

int main() {
    long long sum = 0;
    long long product = 1;
    int max_val = 0;

    #pragma omp parallel for
    for (int i = 1; i <= N; i++) {
        // 原子加法
        #pragma omp atomic
        sum += i;

        // 原子乘法（不是所有编译器都支持）
        #pragma omp atomic
        product *= i;

        // 原子最大值
        #pragma omp atomic update
        {
            if (i > max_val) max_val = i;
        }
    }

    printf("和: %lld, 最大值: %d\n", sum, max_val);
    return 0;
}
```

#### Atomic与Critical对比
```c
#include <omp.h>
#include <stdio.h>
#include <time.h>

#define N 1000000

int main() {
    clock_t start, end;
    double cpu_time_used;

    // 使用Critical
    int counter1 = 0;
    start = clock();

    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        #pragma omp critical
        {
            counter1++;
        }
    }

    end = clock();
    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("Critical耗时: %f秒, 结果: %d\n", cpu_time_used, counter1);

    // 使用Atomic
    int counter2 = 0;
    start = clock();

    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        #pragma omp atomic
        counter2++;
    }

    end = clock();
    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("Atomic耗时: %f秒, 结果: %d\n", cpu_time_used, counter2);

    return 0;
}
```

### 11.4.3 Barrier同步

#### 显式Barrier
```c
#include <omp.h>
#include <stdio.h>
#include <unistd.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        int thread_id = omp_get_thread_num();

        printf("线程 %d 开始第一阶段工作\n", thread_id);
        // 第一阶段工作
        sleep(1 + thread_id);

        #pragma omp barrier
        printf("线程 %d 完成第一阶段，等待其他线程\n", thread_id);

        printf("线程 %d 开始第二阶段工作\n", thread_id);
        // 第二阶段工作
        sleep(2);

        #pragma omp barrier
        printf("线程 %d 完成第二阶段，等待最终同步\n", thread_id);

        printf("线程 %d 完成所有工作\n", thread_id);
    }

    return 0;
}
```

#### Barrier与Work-sharing对比
```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("=== 使用Work-sharing（隐式同步）===\n");

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();

        #pragma omp for
        for (int i = 0; i < 10; i++) {
            printf("线程 %d 处理任务 %d\n", thread_id, i);
            sleep(1);
        }

        // 隐式barrier：所有线程完成for循环后才继续
        printf("线程 %d 完成所有任务\n", thread_id);
    }

    printf("\n=== 使用显式Barrier ===\n");

    #pragma omp parallel num_threads(3)
    {
        int thread_id = omp_get_thread_num();

        // 第一阶段：不同线程处理不同数量的任务
        if (thread_id == 0) {
            for (int i = 0; i < 5; i++) {
                printf("线程 %d 处理任务 %d\n", thread_id, i);
                sleep(1);
            }
        } else if (thread_id == 1) {
            for (int i = 0; i < 3; i++) {
                printf("线程 %d 处理任务 %d\n", thread_id, i + 10);
                sleep(1);
            }
        } else {
            for (int i = 0; i < 7; i++) {
                printf("线程 %d 处理任务 %d\n", thread_id, i + 20);
                sleep(1);
            }
        }

        #pragma omp barrier
        printf("线程 %d 等待其他线程完成第一阶段\n", thread_id);

        // 第二阶段：所有线程继续工作
        printf("线程 %d 开始第二阶段\n", thread_id);
        sleep(2);
    }

    return 0;
}
```

### 11.4.4 Master和Single

#### Master指令
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        // 只有主线程执行
        #pragma omp master
        {
            printf("这是主线程（ID=0）执行的代码\n");
            printf("当前线程数量: %d\n", omp_get_num_threads());
        }

        // 所有线程都执行
        printf("线程 %d 执行普通代码\n", omp_get_thread_num());

        // 主线程执行I/O操作
        #pragma omp master
        {
            printf("线程 %d 负责I/O输出\n", omp_get_thread_num());
        }
    }

    return 0;
}
```

#### Single指令
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        printf("线程 %d 进入并行区域\n", omp_get_thread_num());

        #pragma omp single
        {
            printf("线程 %d 执行Single区域\n", omp_get_thread_num());
            printf("其他线程等待...\n");
            sleep(2);
        }

        printf("线程 %d 离开Single区域\n", omp_get_thread_num());

        // 现在所有线程继续执行
        printf("线程 %d 继续执行后续代码\n", omp_get_thread_num());
    }

    return 0;
}
```

#### nowait子句
```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("=== 使用nowait ===\n");

    #pragma omp parallel num_threads(4)
    {
        #pragma omp single nowait
        {
            printf("线程 %d 开始Single区域\n", omp_get_thread_num());
            sleep(2);
            printf("线程 %d 完成Single区域\n", omp_get_thread_num());
        }

        // 没有隐式同步，线程可以继续执行
        printf("线程 %d 在Single完成后立即执行\n", omp_get_thread_num());
    }

    printf("\n=== 不使用nowait ===\n");

    #pragma omp parallel num_threads(4)
    {
        #pragma omp single
        {
            printf("线程 %d 开始Single区域\n", omp_get_thread_num());
            sleep(2);
            printf("线程 %d 完成Single区域\n", omp_get_thread_num());
        }

        // 有隐式同步
        printf("线程 %d 等待Single完成后执行\n", omp_get_thread_num());
    }

    return 0;
}
```

## 11.5 高级特性

### 11.5.1 任务并行

#### Basic Tasks
```c
#include <omp.h>
#include <stdio.h>
#include <unistd.h>

void task_function(int task_id) {
    printf("任务 %d 由线程 %d 执行\n", task_id, omp_get_thread_num());
    sleep(1);
}

int main() {
    #pragma omp parallel
    {
        #pragma omp single
        {
            printf("主线程创建任务\n");

            for (int i = 0; i < 10; i++) {
                #pragma omp task
                task_function(i);
            }

            printf("任务创建完成，等待所有任务完成\n");
        }
    }

    printf("所有任务完成\n");
    return 0;
}
```

#### Task Dependencies
```c
#include <omp.h>
#include <stdio.h>

int main() {
    int a = 0, b = 0, c = 0;

    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp task depend(out: a)
            {
                a = 1;
                printf("任务1设置a=%d\n", a);
            }

            #pragma omp task depend(out: b)
            {
                b = 2;
                printf("任务2设置b=%d\n", b);
            }

            #pragma omp task depend(in: a, b) depend(out: c)
            {
                c = a + b;
                printf("任务3计算c=a+b=%d\n", c);
            }

            #pragma omp task depend(in: c)
            {
                printf("最终结果c=%d\n", c);
            }
        }
    }

    return 0;
}
```

#### Task Scheduling
```c
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel num_threads(4)
    {
        #pragma omp single
        {
            for (int i = 0; i < 20; i++) {
                #pragma omp task firstprivate(i)
                {
                    printf("任务%d由线程%d执行\n", i, omp_get_thread_num());
                    sleep(1);
                }
            }
        }
    }

    return 0;
}
```

### 11.5.2 线程私有存储

#### threadprivate指令
```c
#include <omp.h>
#include <stdio.h>

int counter = 0;

#pragma omp threadprivate(counter)

void increment_counter() {
    counter++;
    printf("线程 %d: counter = %d\n", omp_get_thread_num(), counter);
}

int main() {
    #pragma omp parallel num_threads(4)
    {
        increment_counter();
        increment_counter();
    }

    // 主线程的counter
    printf("主线程: counter = %d\n", counter);

    return 0;
}
```

#### 线程本地存储应用
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

// 每个线程的随机数生成器状态
#pragma omp threadprivate(rand_state)
unsigned int rand_state;

unsigned int rand_r_custom() {
    rand_state = rand_state * 1103515245 + 12345;
    return rand_state;
}

int main() {
    #pragma omp parallel num_threads(4)
    {
        // 为每个线程初始化不同的随机种子
        #pragma omp single
        {
            #pragma omp task
            {
                rand_state = 12345;
            }
            #pragma omp task
            {
                rand_state = 54321;
            }
            #pragma omp task
            {
                rand_state = 98765;
            }
            #pragma omp task
            {
                rand_state = 13579;
            }
        }

        // 每个线程生成随机数
        for (int i = 0; i < 5; i++) {
            unsigned int r = rand_r_custom();
            printf("线程 %d: 随机数 %u\n", omp_get_thread_num(), r);
        }
    }

    return 0;
}
```

### 11.5.3 SIMD指令

#### 基本SIMD
```c
#include <omp.h>
#include <stdio.h>

#define N 1000

int main() {
    double a[N], b[N], c[N];

    // 初始化数组
    for (int i = 0; i < N; i++) {
        a[i] = i * 1.0;
        b[i] = i * 2.0;
    }

    // SIMD向量化
    #pragma omp simd
    for (int i = 0; i < N; i++) {
        c[i] = a[i] + b[i];
    }

    printf("前5个结果: ");
    for (int i = 0; i < 5; i++) {
        printf("%.1f ", c[i]);
    }
    printf("\n");

    return 0;
}
```

#### SIMD with Reduction
```c
#include <omp.h>
#include <stdio.h>

#define N 1000

int main() {
    double a[N], b[N];
    double dot_product = 0.0;

    // 初始化向量
    for (int i = 0; i < N; i++) {
        a[i] = i * 1.0;
        b[i] = i * 2.0;
    }

    // SIMD点积计算
    #pragma omp simd reduction(+:dot_product)
    for (int i = 0; i < N; i++) {
        dot_product += a[i] * b[i];
    }

    printf("点积结果: %f\n", dot_product);
    return 0;
}
```

## 11.6 性能优化技巧

### 11.6.1 负载均衡

#### 动态调度优化
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define N 1000

int main() {
    int work_units[N];

    // 模拟不均匀的工作负载
    for (int i = 0; i < N; i++) {
        work_units[i] = rand() % 100 + 1;  // 1-100的工作量
    }

    clock_t start, end;
    double cpu_time_used;

    // 静态调度
    start = clock();
    #pragma omp parallel for schedule(static) reduction(+:total_work)
    for (int i = 0; i < N; i++) {
        // 模拟工作
        for (int j = 0; j < work_units[i] * 1000; j++) {
            // 空循环模拟工作
        }
    }
    end = clock();
    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("静态调度耗时: %f秒\n", cpu_time_used);

    // 动态调度
    start = clock();
    #pragma omp parallel for schedule(dynamic, 10) reduction(+:total_work)
    for (int i = 0; i < N; i++) {
        // 模拟工作
        for (int j = 0; j < work_units[i] * 1000; j++) {
            // 空循环模拟工作
        }
    }
    end = clock();
    cpu_time_used = ((double)(end - start)) / CLOCKS_PER_SEC;
    printf("动态调度耗时: %f秒\n", cpu_time_used);

    return 0;
}
```

#### 工作窃取
```c
#include <omp.h>
#include <stdio.h>

int fibonacci(int n) {
    if (n <= 1) return n;

    int x, y;

    #pragma omp task shared(x)
    x = fibonacci(n - 1);

    #pragma omp task shared(y)
    y = fibonacci(n - 2);

    #pragma omp taskwait
    return x + y;
}

int main() {
    int n = 30;

    #pragma omp parallel
    {
        #pragma omp single
        {
            printf("计算fibonacci(%d)\n", n);
            int result = fibonacci(n);
            printf("结果: %d\n", result);
        }
    }

    return 0;
}
```

### 11.6.2 内存访问优化

#### 数据对齐
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

#define N 1000000

// 对齐到缓存行边界
typedef struct {
    double data[8] __attribute__((aligned(64)));
} aligned_data;

int main() {
    aligned_data *array = (aligned_data*)aligned_alloc(64, N * sizeof(aligned_data));

    // 并行初始化
    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < 8; j++) {
            array[i].data[j] = i + j;
        }
    }

    // 并行计算
    double sum = 0.0;
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < 8; j++) {
            sum += array[i].data[j];
        }
    }

    printf("总和: %f\n", sum);

    free(array);
    return 0;
}
```

#### 缓存友好的数据访问
```c
#include <omp.h>
#include <stdio.h>

#define N 1000
#define BLOCK_SIZE 32

int main() {
    double a[N][N], b[N][N], c[N][N];

    // 初始化矩阵
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            a[i][j] = i + j;
            b[i][j] = i - j;
            c[i][j] = 0.0;
        }
    }

    // 分块矩阵乘法（缓存友好）
    #pragma omp parallel for collapse(2)
    for (int ii = 0; ii < N; ii += BLOCK_SIZE) {
        for (int jj = 0; jj < N; jj += BLOCK_SIZE) {
            for (int kk = 0; kk < N; kk += BLOCK_SIZE) {
                // 处理当前块
                for (int i = ii; i < ii + BLOCK_SIZE && i < N; i++) {
                    for (int j = jj; j < jj + BLOCK_SIZE && j < N; j++) {
                        for (int k = kk; k < kk + BLOCK_SIZE && k < N; k++) {
                            c[i][j] += a[i][k] * b[k][j];
                        }
                    }
                }
            }
        }
    }

    printf("矩阵乘法完成\n");
    printf("c[0][0] = %f\n", c[0][0]);
    printf("c[%d][%d] = %f\n", N-1, N-1, c[N-1][N-1]);

    return 0;
}
```

### 11.6.3 减少同步开销

#### 减少Barrier使用
```c
#include <omp.h>
#include <stdio.h>

#define N 1000

int main() {
    int array[N];
    int partial_sums[100];

    // 初始化数组
    for (int i = 0; i < N; i++) {
        array[i] = i % 100;
    }

    // 方法1：使用reduction（隐式同步）
    int total_sum1 = 0;
    #pragma omp parallel for reduction(+:total_sum1)
    for (int i = 0; i < N; i++) {
        total_sum1 += array[i];
    }
    printf("Reduction方法: %d\n", total_sum1);

    // 方法2：减少同步的分段求和
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();

        // 每个线程计算部分和
        int local_sum = 0;
        for (int i = thread_id; i < N; i += num_threads) {
            local_sum += array[i];
        }

        partial_sums[thread_id] = local_sum;
    }

    // 最后一次性求和（减少同步）
    int total_sum2 = 0;
    for (int i = 0; i < omp_get_max_threads(); i++) {
        total_sum2 += partial_sums[i];
    }
    printf("分段求和方法: %d\n", total_sum2);

    return 0;
}
```

## 11.7 OpenMP在生物信息学中的应用

### 11.7.1 序列比对并行化

#### 并行Smith-Waterman算法
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define MATCH_SCORE 2
#define MISMATCH_SCORE -1
#define GAP_PENALTY -1

int max(int a, int b, int c) {
    return (a > b) ? ((a > c) ? a : c) : ((b > c) ? b : c);
}

void parallel_smith_waterman(const char* seq1, const char* seq2,
                           int len1, int len2) {
    int** score_matrix = (int**)malloc((len1 + 1) * sizeof(int*));
    for (int i = 0; i <= len1; i++) {
        score_matrix[i] = (int*)malloc((len2 + 1) * sizeof(int));
    }

    // 初始化第一行和第一列
    for (int i = 0; i <= len1; i++) score_matrix[i][0] = 0;
    for (int j = 0; j <= len2; j++) score_matrix[0][j] = 0;

    // 并行计算得分矩阵
    #pragma omp parallel for collapse(2)
    for (int i = 1; i <= len1; i++) {
        for (int j = 1; j <= len2; j++) {
            int match = score_matrix[i-1][j-1] +
                       ((seq1[i-1] == seq2[j-1]) ? MATCH_SCORE : MISMATCH_SCORE);
            int delete = score_matrix[i-1][j] + GAP_PENALTY;
            int insert = score_matrix[i][j-1] + GAP_PENALTY;

            score_matrix[i][j] = max(match, delete, insert);
        }
    }

    // 找到最大得分
    int max_score = 0;
    int max_i = 0, max_j = 0;

    #pragma omp parallel for reduction(max:max_score)
    for (int i = 1; i <= len1; i++) {
        for (int j = 1; j <= len2; j++) {
            if (score_matrix[i][j] > max_score) {
                max_score = score_matrix[i][j];
                max_i = i;
                max_j = j;
            }
        }
    }

    printf("最大比对得分: %d\n", max_score);
    printf("位置: seq1[%d], seq2[%d]\n", max_i, max_j);

    // 清理内存
    for (int i = 0; i <= len1; i++) {
        free(score_matrix[i]);
    }
    free(score_matrix);
}

int main() {
    const char* seq1 = "ACGTACGTACGT";
    const char* seq2 = "ACGTAACGTAAC";

    parallel_smith_waterman(seq1, seq2, strlen(seq1), strlen(seq2));

    return 0;
}
```

### 11.7.2 基因表达数据分析

#### 并行相关性计算
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#define NUM_GENES 1000
#define NUM_SAMPLES 100

double** generate_expression_data() {
    double** data = (double**)malloc(NUM_GENES * sizeof(double*));
    for (int i = 0; i < NUM_GENES; i++) {
        data[i] = (double*)malloc(NUM_SAMPLES * sizeof(double));
        for (int j = 0; j < NUM_SAMPLES; j++) {
            data[i][j] = (double)rand() / RAND_MAX * 100.0;
        }
    }
    return data;
}

double calculate_correlation(double* gene1, double* gene2) {
    double mean1 = 0.0, mean2 = 0.0;

    // 计算均值
    for (int i = 0; i < NUM_SAMPLES; i++) {
        mean1 += gene1[i];
        mean2 += gene2[i];
    }
    mean1 /= NUM_SAMPLES;
    mean2 /= NUM_SAMPLES;

    // 计算相关系数
    double numerator = 0.0, denom1 = 0.0, denom2 = 0.0;
    for (int i = 0; i < NUM_SAMPLES; i++) {
        double diff1 = gene1[i] - mean1;
        double diff2 = gene2[i] - mean2;
        numerator += diff1 * diff2;
        denom1 += diff1 * diff1;
        denom2 += diff2 * diff2;
    }

    return numerator / (sqrt(denom1) * sqrt(denom2));
}

void parallel_correlation_analysis(double** expression_data) {
    double** correlation_matrix = (double**)malloc(NUM_GENES * sizeof(double*));
    for (int i = 0; i < NUM_GENES; i++) {
        correlation_matrix[i] = (double*)malloc(NUM_GENES * sizeof(double));
    }

    // 并行计算相关性矩阵
    printf("开始并行计算基因相关性...\n");

    #pragma omp parallel for collapse(2)
    for (int i = 0; i < NUM_GENES; i++) {
        for (int j = i; j < NUM_GENES; j++) {
            double corr = calculate_correlation(expression_data[i], expression_data[j]);
            correlation_matrix[i][j] = corr;
            correlation_matrix[j][i] = corr;  // 对称矩阵
        }
    }

    // 找到高度相关的基因对
    int highly_correlated = 0;
    #pragma omp parallel for reduction(+:highly_correlated) collapse(2)
    for (int i = 0; i < NUM_GENES; i++) {
        for (int j = i + 1; j < NUM_GENES; j++) {
            if (fabs(correlation_matrix[i][j]) > 0.8) {
                highly_correlated++;
            }
        }
    }

    printf("高度相关基因对数量 (|r| > 0.8): %d\n", highly_correlated);

    // 清理内存
    for (int i = 0; i < NUM_GENES; i++) {
        free(correlation_matrix[i]);
    }
    free(correlation_matrix);
}

int main() {
    srand(42);

    printf("生成基因表达数据...\n");
    double** expression_data = generate_expression_data();

    printf("进行并行相关性分析...\n");
    parallel_correlation_analysis(expression_data);

    // 清理内存
    for (int i = 0; i < NUM_GENES; i++) {
        free(expression_data[i]);
    }
    free(expression_data);

    return 0;
}
```

### 11.7.3 分子对接并行化

#### 并行构象搜索
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#define NUM_CONFORMATIONS 10000
#define NUM_ROTATIONS 360

typedef struct {
    double x, y, z;
} Vector3;

typedef struct {
    Vector3 position;
    double energy;
} Conformation;

// 简化的分子能量计算函数
double calculate_energy(Vector3 pos) {
    // 简化的势能函数
    return exp(-(pos.x*pos.x + pos.y*pos.y + pos.z*pos.z) / 100.0);
}

void parallel_conformation_search() {
    Conformation best_conformation = {{0, 0, 0}, INFINITY};
    Conformation* conformations = (Conformation*)malloc(NUM_CONFORMATIONS * sizeof(Conformation));

    printf("开始并行构象搜索...\n");

    // 并行生成和评估构象
    #pragma omp parallel for
    for (int i = 0; i < NUM_CONFORMATIONS; i++) {
        // 随机生成构象
        Vector3 pos;
        pos.x = (double)rand() / RAND_MAX * 20.0 - 10.0;
        pos.y = (double)rand() / RAND_MAX * 20.0 - 10.0;
        pos.z = (double)rand() / RAND_MAX * 20.0 - 10.0;

        // 计算能量
        double energy = calculate_energy(pos);

        conformations[i].position = pos;
        conformations[i].energy = energy;
    }

    // 找到最低能量构象
    #pragma omp parallel for reduction(min:best_conformation.energy)
    for (int i = 0; i < NUM_CONFORMATIONS; i++) {
        if (conformations[i].energy < best_conformation.energy) {
            best_conformation = conformations[i];
        }
    }

    printf("最佳构象:\n");
    printf("  位置: (%.2f, %.2f, %.2f)\n",
           best_conformation.position.x,
           best_conformation.position.y,
           best_conformation.position.z);
    printf("  能量: %.6f\n", best_conformation.energy);

    free(conformations);
}

int main() {
    srand(42);

    parallel_conformation_search();

    return 0;
}
```

## 11.8 调试和性能分析

### 11.8.1 OpenMP调试技巧

#### 环境变量调试
```bash
# 设置线程数量
export OMP_NUM_THREADS=4

# 设置调度策略
export OMP_SCHEDULE="dynamic,10"

# 启用嵌套并行
export OMP_NESTED=TRUE

# 设置堆栈大小
export OMP_STACKSIZE=4M

# 启用调试信息
export KMP_AFFINITY=verbose
export GOMP_DEBUG=1
```

#### 运行时检查
```c
#include <omp.h>
#include <stdio.h>

void check_openmp_environment() {
    printf("=== OpenMP环境信息 ===\n");
    printf("最大线程数: %d\n", omp_get_max_threads());
    printf("当前线程数: %d\n", omp_get_num_threads());
    printf("线程ID: %d\n", omp_get_thread_num());
    printf("是否在并行区域: %s\n", omp_in_parallel() ? "是" : "否");
    printf("嵌套并行支持: %s\n", omp_get_nested() ? "是" : "否");
    printf("动态线程支持: %s\n", omp_get_dynamic() ? "是" : "否");
}

int main() {
    check_openmp_environment();

    #pragma omp parallel
    {
        printf("线程 %d: ", omp_get_thread_num());
        check_openmp_environment();
    }

    return 0;
}
```

### 11.8.2 性能分析工具

#### 使用perf分析OpenMP程序
```bash
# 编译时添加调试信息
gcc -fopenmp -g -O2 program.c -o program

# 使用perf记录性能数据
perf record ./program

# 分析性能热点
perf report

# 分析缓存命中率
perf stat -e cache-misses,cache-references ./program

# 分析分支预测
perf stat -e branches,branch-misses ./program
```

#### Intel VTune Profiler使用
```bash
# 启用OpenMP分析
vtune -collect hotspots -result-dir=results ./program

# 分析线程和同步
vtune -collect threading -result-dir=results ./program

# 分析内存访问模式
vtune -collect memory-access -result-dir=results ./program

# 生成HTML报告
vtune -report hotspots -format=html -result-dir=results -report-output=report.html
```

### 11.8.3 性能基准测试

#### OpenMP性能测试框架
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define TEST_SIZE 10000000
#define NUM_TESTS 5

double measure_performance(int num_threads) {
    double* array = (double*)malloc(TEST_SIZE * sizeof(double));
    double sum = 0.0;
    clock_t start, end;

    // 初始化数组
    for (int i = 0; i < TEST_SIZE; i++) {
        array[i] = (double)i;
    }

    // 设置线程数
    omp_set_num_threads(num_threads);

    // 执行测试
    start = clock();
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < TEST_SIZE; i++) {
        array[i] = sqrt(array[i]);
        sum += array[i];
    }
    end = clock();

    free(array);
    return ((double)(end - start)) / CLOCKS_PER_SEC;
}

void run_performance_test() {
    printf("OpenMP性能测试\n");
    printf("================\n");

    int max_threads = omp_get_max_threads();
    printf("最大线程数: %d\n\n", max_threads);

    printf("线程数\t时间(秒)\t加速比\t效率\n");
    printf("------\t--------\t------\t-----\n");

    double serial_time = measure_performance(1);
    printf("1\t%.4f\t1.00\t100.0%%\n", serial_time);

    for (int i = 2; i <= max_threads; i++) {
        double parallel_time = measure_performance(i);
        double speedup = serial_time / parallel_time;
        double efficiency = speedup / i * 100.0;

        printf("%d\t%.4f\t%.2f\t%.1f%%\n",
               i, parallel_time, speedup, efficiency);
    }
}

int main() {
    run_performance_test();
    return 0;
}
```

## 11.9 最佳实践和常见问题

### 11.9.1 最佳实践

#### 1. 适当的任务粒度
```c
// ❌ 粒度过细：每个迭代都很轻量
#pragma omp parallel for
for (int i = 0; i < 1000; i++) {
    result[i] = i * 2;  // 太简单的操作
}

// ✅ 适当的粒度：每个迭代有足够工作量
#pragma omp parallel for
for (int i = 0; i < 1000; i++) {
    // 复杂的计算
    for (int j = 0; j < 1000; j++) {
        result[i] += data[i][j] * weights[j];
    }
}
```

#### 2. 避免数据竞争
```c
// ❌ 数据竞争
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    shared_array[i] = compute_value(i);
    global_counter++;  // 竞争条件
}

// ✅ 正确的同步
#pragma omp parallel for reduction(+:global_counter)
for (int i = 0; i < N; i++) {
    shared_array[i] = compute_value(i);
    global_counter++;
}
```

#### 3. 合理使用调度策略
```c
// 对于不均匀的工作负载，使用动态调度
#pragma omp parallel for schedule(dynamic, 10)

// 对于均匀的工作负载，使用静态调度
#pragma omp parallel for schedule(static)

// 对于递归算法，使用guided调度
#pragma omp parallel for schedule(guided)
```

### 11.9.2 常见问题和解决方案

#### 1. 死锁问题
```c
// ❌ 可能导致死锁
#pragma omp parallel
{
    #pragma omp critical(section1)
    {
        // 等待另一个临界区
        #pragma omp critical(section2)
        {
            // 代码
        }
    }
}

// ✅ 避免嵌套临界区
#pragma omp parallel
{
    #pragma omp critical(section1)
    {
        // 代码
    }
    #pragma omp critical(section2)
    {
        // 代码
    }
}
```

#### 2. 负载不均衡
```c
// ❌ 不均匀的工作负载
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    // 某些迭代需要很长时间
    if (i % 100 == 0) {
        expensive_operation();
    }
}

// ✅ 使用动态调度
#pragma omp parallel for schedule(dynamic, 10)
for (int i = 0; i < N; i++) {
    if (i % 100 == 0) {
        expensive_operation();
    }
}
```

#### 3. 内存访问模式
```c
// ❌ 不缓存友好的访问模式
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    for (int j = 0; j < M; j++) {
        result[j][i] = compute(i, j);  // 列优先访问
    }
}

// ✅ 缓存友好的访问模式
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    for (int j = 0; j < M; j++) {
        result[i][j] = compute(i, j);  // 行优先访问
    }
}
```

### 11.9.3 编译器优化

#### 编译选项
```bash
# GCC编译选项
gcc -fopenmp -O3 -march=native -ffast-math program.c -o program

# Intel编译器选项
icc -qopenmp -O3 -xHost program.c -o program

# Clang编译选项
clang -fopenmp -O3 -march=native program.c -o program
```

#### 运行时优化
```c
// 启用NUMA感知
export KMP_AFFINITY=granularity=fine,compact,1,0

// 设置线程绑定
export GOMP_CPU_AFFINITY="0-3"

// 优化内存分配
export MALLOC_ARENA_MAX=1
```

## 11.10 总结

OpenMP是一个强大而灵活的并行编程工具，特别适合共享内存系统的并行化。通过本章的学习，你应该掌握：

### 核心概念
- **Fork-Join模型**：理解主线程和并行区域的关系
- **数据共享属性**：正确使用shared、private、firstprivate、lastprivate
- **同步机制**：critical、atomic、barrier、master、single等指令

### 实用技巧
- **循环并行化**：parallel for、schedule、reduction
- **任务并行**：task、taskwait、taskgroup
- **SIMD向量化**：simd指令提高向量计算性能

### 性能优化
- **负载均衡**：选择合适的调度策略
- **内存访问**：优化数据局部性和缓存使用
- **减少同步**：最小化临界区和同步点

### 生物信息学应用
- **序列比对**：并行Smith-Waterman算法
- **基因分析**：并行相关性计算
- **分子模拟**：并行构象搜索

### 最佳实践
- **适当的任务粒度**
- **避免数据竞争**
- **合理使用调度策略**
- **缓存友好的内存访问**

通过实践这些概念和技术，你可以在生物信息学和其他科学计算领域有效地利用多核处理器的并行计算能力，显著提高程序的性能。