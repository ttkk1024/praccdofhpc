# 第六章 并行编程模型

并行编程模型是连接硬件架构与应用程序的软件抽象层。本章将详细介绍三种最主流的并行编程模型：**MPI**（消息传递接口）、**OpenMP**（共享内存多处理）和 **CUDA**（统一计算设备架构）。此外，我们还将简要讨论其他编程模型，如 OpenCL 和 Pthreads，并在最后对这些模型进行综合比较。

## 6.1 消息传递接口 (MPI)

### 6.1.1 核心概念

**MPI (Message Passing Interface)** 是分布式内存系统的标准编程模型。它定义了一组库函数，允许运行在不同节点上的进程通过消息传递进行通信。

> **核心特征**：
> *   **进程级并行**：每个进程拥有独立的地址空间。
> *   **显式通信**：程序员必须显式地发送和接收数据。
> *   **SPMD模式**：通常采用"单程序多数据"模式，所有进程运行相同的代码，但根据进程ID（Rank）执行不同的路径。

### 6.1.2 MPI基础编程

#### 1. 环境初始化与管理

```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // 1. 初始化MPI环境
    MPI_Init(&argc, &argv);

    int rank, size;
    // 2. 获取当前进程的ID (Rank)
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    // 3. 获取进程总数 (Size)
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    printf("Hello form process %d of %d\n", rank, size);

    // 4. 清理并退出MPI环境
    MPI_Finalize();
    return 0;
}
```

#### 2. 点对点通信 (Point-to-Point)

最基本的通信方式是一对一的消息传递。

*   **阻塞通信** (`MPI_Send` / `MPI_Recv`)：调用直到操作完成（缓冲区可用或数据已接收）才返回。
*   **非阻塞通信** (`MPI_Isend` / `MPI_Irecv`)：立即返回，允许计算与通信重叠，需配合 `MPI_Wait` 使用。

```c
void p2p_example(int rank) {
    int token;
    if (rank == 0) {
        token = 42;
        // 发送给进程1，标签为0
        MPI_Send(&token, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        printf("Rank 0 sent token %d\n", token);
    } else if (rank == 1) {
        // 从进程0接收，标签为0
        MPI_Recv(&token, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Rank 1 received token %d\n", token);
    }
}
```

#### 3. 集体通信 (Collective Communication)

涉及通信域内所有进程的操作。所有进程必须调用同一个集体通信函数。

| 函数 | 描述 | 图示 |
| :--- | :--- | :--- |
| **MPI_Bcast** | 广播：根进程将数据发送给所有其他进程 | Root -> All |
| **MPI_Scatter** | 散发：根进程将数组分块发送给不同进程 | Root -> p0, p1, ... |
| **MPI_Gather** | 收集：根进程收集所有进程的数据块组合成数组 | p0, p1, ... -> Root |
| **MPI_Reduce** | 归约：对所有进程的数据进行运算（如求和、最大值） | Op(p0, p1, ...) -> Root |
| **MPI_Allreduce**| 全局归约：归约结果发送给所有进程 | Op -> All |

```c
void collective_example(int rank, int size) {
    int local_val = rank + 1;
    int global_sum = 0;

    // 所有进程计算局部变量 local_val 的总和，结果存入 global_sum
    // 结果分发给所有进程
    MPI_Allreduce(&local_val, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Rank %d: Global sum is %d\n", rank, global_sum);
}
```

---

## 6.2 OpenMP

### 6.2.1 核心概念

**OpenMP (Open Multi-Processing)** 是用于共享内存并行系统的应用程序接口 (API)。它基于编译器指令（Compiler Directives），通过多线程来实现并行。

> **核心特征**：
> *   **线程级并行**：所有线程共享同一进程的地址空间。
> *   **Fork-Join模型**：主线程在需要并行时派生（Fork）一组工作线程，并行块结束后合并（Join）。
> *   **增量式并行化**：可以逐步地在串行代码的关键循环上添加指令进行并行化。

### 6.2.2 OpenMP基础编程

#### 1. 并行区域与循环

使用 `#pragma omp` 指令。

```c
#include <omp.h>
#include <stdio.h>

void parallel_loop() {
    int n = 1000;
    int a[1000], b[1000], c[1000];

    // 初始化数组
    for (int i = 0; i < n; i++) { a[i] = i; b[i] = i * 2; }

    // 并行化 for 循环
    // schedule(static) 表示将迭代平均静态分配给线程
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}
```

#### 2. 数据环境管理

控制变量在线程间的共享属性是OpenMP编程的关键。

| 子句 | 说明 |
| :--- | :--- |
| `shared(x)` | 变量 `x` 在所有线程间共享（默认）。 |
| `private(x)` | 每个线程拥有 `x` 的独立未初始化副本。 |
| `firstprivate(x)` | 同 `private`，但副本初始化为进入并行区时的值。 |
| `reduction(op:x)` | 对变量 `x` 进行归约操作，避免数据竞争。 |

```c
void reduction_example() {
    int sum = 0;
    
    // 使用 reduction 子句安全地并行求和
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < 1000; i++) {
        sum += i;
    }
    
    printf("Sum: %d\n", sum);
}
```

#### 3. 同步机制

在共享内存环境中，必须防止多个线程同时修改同一数据（竞态条件）。

*   **`#pragma omp critical`**：定义临界区，同一时刻只允许一个线程执行。
*   **`#pragma omp atomic`**：轻量级原子操作，通常用于更新计数器。
*   **`#pragma omp barrier`**：所有线程必须到达此点后才能继续。

```c
void synchronization_example() {
    int count = 0;

    #pragma omp parallel
    {
        // ... 执行一些工作 ...
        
        // 原子更新，比 critical 更快
        #pragma omp atomic
        count++;
    }
}
```

---

## 6.3 CUDA (Compute Unified Device Architecture)

### 6.3.1 核心概念

**CUDA** 是 NVIDIA 推出的异构计算平台。它利用 GPU 强大的并行计算能力（成千上万个核心）来加速计算密集型任务。

> **核心特征**：
> *   **SIMT (单指令多线程)**：大量线程执行相同的指令流，但处理不同的数据。
> *   **主机-设备模型**：CPU（主机）负责逻辑控制和数据传输，GPU（设备）负责并行计算。
> *   **层次化线程结构**：Grid -> Block -> Thread。

### 6.3.2 CUDA基础编程

#### 1. 编程流程

1.  分配主机（Host）和设备（Device）内存。
2.  将数据从 Host 拷贝到 Device。
3.  调用 Kernel 函数在 GPU 上执行。
4.  将结果从 Device 拷贝回 Host。
5.  释放内存。

#### 2. Kernel函数与线程索引

Kernel 是在 GPU 上运行的函数，用 `__global__` 修饰。

```cuda
// Kernel定义：向量加法
__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements) {
    // 计算全局唯一的线程索引
    int i = blockDim.x * blockIdx.x + threadIdx.x;

    if (i < numElements) {
        C[i] = A[i] + B[i];
    }
}
```

#### 3. 完整示例

```cuda
#include <cuda_runtime.h>
#include <stdio.h>

int main() {
    int n = 50000;
    size_t size = n * sizeof(float);

    // 1. 分配主机内存
    float *h_A = (float *)malloc(size);
    float *h_B = (float *)malloc(size);
    float *h_C = (float *)malloc(size);

    // 初始化...
    for (int i = 0; i < n; ++i) { h_A[i] = rand()/(float)RAND_MAX; h_B[i] = rand()/(float)RAND_MAX; }

    // 2. 分配设备内存
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    // 3. 数据拷贝 Host -> Device
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // 4. 启动 Kernel
    // 256个线程一个Block，计算需要的Block数量
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);

    // 5. 数据拷贝 Device -> Host
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // 验证结果...

    // 6. 释放内存
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
    free(h_A); free(h_B); free(h_C);

    return 0;
}
```

### 6.3.3 GPU内存层次与优化

理解内存层次是CUDA优化的关键。

| 内存类型 | 作用域 | 速度 | 说明 |
| :--- | :--- | :--- | :--- |
| **寄存器 (Registers)** | 线程私有 | 极快 | 数量有限，用完会溢出到本地内存。 |
| **共享内存 (Shared Memory)**| Block内共享 | 很快 | 用户可控的高速缓存，用于Block内线程通信。 |
| **全局内存 (Global Memory)** | 所有线程共享 | 较慢 | 容量大，但延迟高。需通过"合并访问" (Coalesced Access) 优化。 |
| **常量/纹理内存** | 只读 | 快(有缓存) | 适合特定访问模式。 |

**优化策略**：
*   **合并访问**：确保 warp 中的线程访问连续的内存地址。
*   **利用共享内存**：将频繁访问的全局数据加载到共享内存中复用。
*   **减少分支发散**：避免 warp 内的线程执行不同的 `if-else` 分支。

---

## 6.4 其他并行编程模型

### 6.4.1 OpenCL (Open Computing Language)
*   **定位**：开放标准的异构计算框架，类似 CUDA 但支持非 NVIDIA 硬件（AMD GPU, Intel CPU/GPU, FPGA）。
*   **特点**：代码可移植性强，但样板代码（Boilerplate）较多，编程略显繁琐。

### 6.4.2 Pthreads (POSIX Threads)
*   **定位**：Unix/Linux 系统底层的线程标准。
*   **特点**：提供细粒度的线程控制（创建、挂起、同步）。OpenMP 的底层实现通常基于 Pthreads。直接使用 Pthreads 灵活性最高，但开发难度大，容易出错。

### 6.4.3 Spark / Hadoop (MapReduce)
*   **定位**：大数据处理框架。
*   **特点**：处理海量数据，关注数据容错和分布式存储，而非高性能计算（HPC）强调的浮点运算速度。

---

## 6.5 模型对比与总结

| 特性 | MPI | OpenMP | CUDA |
| :--- | :--- | :--- | :--- |
| **适用架构** | 分布式内存 (Cluster) | 共享内存 (Multicore CPU) | 异构计算 (CPU + GPU) |
| **并行粒度** | 粗粒度 (进程) | 中粒度 (线程) | 细粒度 (SIMT 线程) |
| **地址空间** | 隔离 (需显式通信) | 共享 (需同步保护) | 独立 (需显式拷贝) |
| **编程难度** | 困难 (需管理通信) | 简单 (编译器指令) | 中等 (需管理内存层次) |
| **可扩展性** | 极高 (数万节点) | 受限于单机核心数 | 受限于单卡/多卡显存与算力 |

### 混合编程 (Hybrid Programming)
在现代超算应用中，**MPI + X** 是主流模式：
*   **MPI + OpenMP**：节点间用 MPI 通信，节点内用 OpenMP 利用多核。
*   **MPI + CUDA**：节点间用 MPI 通信，节点内用 CUDA 加速计算。

## 练习题

1.  **代码实现**：使用 MPI 编写一个程序，计算圆周率 $\pi$（利用蒙特卡洛方法或积分法），并测试不同进程数下的加速比。
2.  **OpenMP优化**：给定一个存在数据依赖的三重循环，尝试使用 OpenMP 进行并行化，并解决可能的数据竞争问题。
3.  **CUDA实践**：编写一个 CUDA 矩阵乘法 Kernel，并尝试使用 Shared Memory 进行分块优化（Tiled Matrix Multiplication）。

```