# 第六章：并行编程模型

## 目录
- [6.1 消息传递接口（MPI）](#61-消息传递接口mpi)
  - [6.1.1 MPI基础概念](#611-mpi基础概念)
  - [6.1.2 点对点通信](#612-点对点通信)
  - [6.1.3 集体通信](#613-集体通信)
  - [6.1.4 进程组管理](#614-进程组管理)
  - [6.1.5 MPI高级特性](#615-mpi高级特性)
- [6.2 OpenMP](#62-openmp)
  - [6.2.1 OpenMP基础](#621-openmp基础)
  - [6.2.2 并行区域](#622-并行区域)
  - [6.2.3 数据共享管理](#623-数据共享管理)
  - [6.2.4 同步机制](#624-同步机制)
  - [6.2.5 OpenMP优化](#625-openmp优化)
- [6.3 CUDA](#63-cuda)
  - [6.3.1 CUDA架构](#631-cuda架构)
  - [6.3.2 线程层次模型](#632-线程层次模型)
  - [6.3.3 内存模型](#633-内存模型)
  - [6.3.4 CUDA编程](#634-cuda编程)
  - [63.5 GPU优化策略](#635-gpu优化策略)
- [6.4 其他并行编程模型](#64-其他并行编程模型)
  - [6.4.1 OpenCL](#641-opencl)
  - [6.4.2 Pthreads](#642-pthreads)
  - [6.4.3 TBB](#643-tbb)
  - [6.4.4 分布式计算框架](#644-分布式计算框架)
- [6.5 编程模型比较](#65-编程模型比较)
- [6.6 混合编程模型](#66-混合编程模型)

## 6.1 消息传递接口（MPI）

### 6.1.1 MPI基础概念

#### 6.1.1.1 MPI简介
**Message Passing Interface (MPI)** 是一个标准的消息传递库，用于编写并行程序。它提供了一套标准化的函数接口，支持分布式内存系统的并行计算。

**核心特性**：
- **标准化**：跨平台、跨编译器的标准接口
- **可移植性**：同一代码可在不同MPI实现上运行
- **高性能**：针对高性能计算优化
- **灵活性**：支持点对点和集体通信

#### 6.1.1.2 MPI基本组件

**进程（Process）**：
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int rank, size;

    // 初始化MPI环境
    MPI_Init(&argc, &argv);

    // 获取进程排名
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    // 获取进程总数
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    printf("Hello from process %d of %d\n", rank, size);

    // 结束MPI环境
    MPI_Finalize();
    return 0;
}
```

**通信器（Communicator）**：
```c
MPI_Comm comm;           // 通信器句柄
MPI_Comm_dup(MPI_COMM_WORLD, &comm);  // 复制通信器
MPI_Comm_split(comm, color, key, &new_comm);  // 分割通信器
MPI_Comm_free(&comm);    // 释放通信器
```

**数据类型（Datatype）**：
```c
// 预定义数据类型
MPI_INT, MPI_FLOAT, MPI_DOUBLE, MPI_CHAR

// 创建自定义数据类型
MPI_Datatype create_vector_type(int count, int blocklength, int stride) {
    MPI_Datatype vector_type;
    MPI_Type_vector(count, blocklength, stride, MPI_DOUBLE, &vector_type);
    MPI_Type_commit(&vector_type);
    return vector_type;
}
```

#### 6.1.1.3 MPI编译和执行
```bash
# 编译
mpicc -o mpi_program mpi_program.c

# 执行（4个进程）
mpirun -np 4 ./mpi_program

# 指定主机文件
mpirun -np 8 -hostfile hosts.txt ./mpi_program
```

### 6.1.2 点对点通信

#### 6.1.2.1 阻塞通信

**发送操作**：
```c
// 基本发送函数
int MPI_Send(
    void* buf,              // 发送缓冲区
    int count,              // 发送元素数量
    MPI_Datatype datatype,  // 数据类型
    int dest,               // 目标进程排名
    int tag,                // 消息标签
    MPI_Comm comm           // 通信器
);
```

**接收操作**：
```c
// 基本接收函数
int MPI_Recv(
    void* buf,              // 接收缓冲区
    int count,              // 接收元素最大数量
    MPI_Datatype datatype,  // 数据类型
    int source,             // 消息源进程排名
    int tag,                // 消息标签
    MPI_Comm comm,          // 通信器
    MPI_Status* status      // 状态信息
);
```

**点对点通信示例**：
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int rank, size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (size < 2) {
        printf("This program requires at least 2 processes\n");
        MPI_Finalize();
        return 1;
    }

    if (rank == 0) {
        // 进程0发送数据给进程1
        int data = 42;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        printf("Process 0 sent data: %d\n", data);
    } else if (rank == 1) {
        // 进程1接收数据
        int received_data;
        MPI_Status status;
        MPI_Recv(&received_data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
        printf("Process 1 received data: %d\n", received_data);
    }

    MPI_Finalize();
    return 0;
}
```

#### 6.1.2.2 非阻塞通信

**非阻塞发送**：
```c
int MPI_Isend(
    void* buf,              // 发送缓冲区
    int count,              // 发送元素数量
    MPI_Datatype datatype,  // 数据类型
    int dest,               // 目标进程排名
    int tag,                // 消息标签
    MPI_Comm comm,          // 通信器
    MPI_Request* request    // 请求句柄
);
```

**非阻塞接收**：
```c
int MPI_Irecv(
    void* buf,              // 接收缓冲区
    int count,              // 接收元素最大数量
    MPI_Datatype datatype,  // 数据类型
    int source,             // 消息源进程排名
    int tag,                // 消息标签
    MPI_Comm comm,          // 通信器
    MPI_Request* request    // 请求句柄
);
```

**请求处理**：
```c
MPI_Request request;
MPI_Status status;

// 发起非阻塞发送
MPI_Isend(buffer, count, MPI_INT, dest, tag, comm, &request);

// 执行其他计算工作
do_computation();

// 等待发送完成
MPI_Wait(&request, &status);

// 或者轮询检查
int flag;
MPI_Test(&request, &flag, &status);
if (flag) {
    printf("Send completed\n");
}
```

#### 6.1.2.3 通信模式

**标准模式**：
```c
// 发送方可能阻塞直到接收方准备好
MPI_Send(buffer, count, MPI_INT, dest, tag, comm);
```

**缓存模式**：
```c
// 先缓冲数据，然后发送
MPI_Bsend(buffer, count, MPI_INT, dest, tag, comm);
```

**同步模式**：
```c
// 确保接收方已开始接收才返回
MPI_Ssend(buffer, count, MPI_INT, dest, tag, comm);
```

**就绪模式**：
```c
// 假设接收方已准备好，用于性能优化
MPI_Rsend(buffer, count, MPI_INT, dest, tag, comm);
```

### 6.1.3 集体通信

#### 6.1.3.1 广播（Broadcast）

**MPI_Bcast**：
```c
int MPI_Bcast(
    void* buffer,           // 缓冲区（发送/接收）
    int count,              // 元素数量
    MPI_Datatype datatype,  // 数据类型
    int root,               // 根进程排名
    MPI_Comm comm           // 通信器
);
```

**广播示例**：
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int rank, size;
    int data;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        data = 100;  // 根进程设置数据
        printf("Root process (0) broadcasting data: %d\n", data);
    }

    // 广播数据
    MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received data: %d\n", rank, data);

    MPI_Finalize();
    return 0;
}
```

#### 6.1.3.2 散发（Scatter）

**MPI_Scatter**：
```c
int MPI_Scatter(
    void* sendbuf,          // 发送缓冲区（仅根进程使用）
    int sendcount,          // 每个进程接收的元素数量
    MPI_Datatype sendtype,  // 发送数据类型
    void* recvbuf,          // 接收缓冲区
    int recvcount,          // 接收元素数量
    MPI_Datatype recvtype,  // 接收数据类型
    int root,               // 根进程排名
    MPI_Comm comm           // 通信器
);
```

**散发示例**：
```c
#include <mpi.h>
#include <stdio.h>

#define ARRAY_SIZE 12

int main(int argc, char** argv) {
    int rank, size;
    int send_data[ARRAY_SIZE];
    int recv_data[ARRAY_SIZE / 4];

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        // 根进程初始化数据
        for (int i = 0; i < ARRAY_SIZE; i++) {
            send_data[i] = i;
        }
        printf("Root process sending data: ");
        for (int i = 0; i < ARRAY_SIZE; i++) {
            printf("%d ", send_data[i]);
        }
        printf("\n");
    }

    // 散发数据
    MPI_Scatter(send_data, 3, MPI_INT, recv_data, 3, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received: ", rank);
    for (int i = 0; i < 3; i++) {
        printf("%d ", recv_data[i]);
    }
    printf("\n");

    MPI_Finalize();
    return 0;
}
```

#### 6.1.3.3 收集（Gather）

**MPI_Gather**：
```c
int MPI_Gather(
    void* sendbuf,          // 发送缓冲区
    int sendcount,          // 发送元素数量
    MPI_Datatype sendtype,  // 发送数据类型
    void* recvbuf,          // 接收缓冲区（仅根进程使用）
    int recvcount,          // 每个进程发送的元素数量
    MPI_Datatype recvtype,  // 接收数据类型
    int root,               // 根进程排名
    MPI_Comm comm           // 通信器
);
```

**收集示例**：
```c
#include <mpi.h>
#include <stdio.h>

#define ARRAY_SIZE 3

int main(int argc, char** argv) {
    int rank, size;
    int send_data[ARRAY_SIZE];
    int recv_data[ARRAY_SIZE * 4];

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 每个进程设置发送数据
    for (int i = 0; i < ARRAY_SIZE; i++) {
        send_data[i] = rank * 10 + i;
    }

    printf("Process %d sending: %d %d %d\n", rank,
           send_data[0], send_data[1], send_data[2]);

    // 收集数据
    MPI_Gather(send_data, ARRAY_SIZE, MPI_INT,
               recv_data, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Root process collected data: ");
        for (int i = 0; i < ARRAY_SIZE * size; i++) {
            printf("%d ", recv_data[i]);
        }
        printf("\n");
    }

    MPI_Finalize();
    return 0;
}
```

#### 6.1.3.4 全局归约（Allreduce）

**MPI_Allreduce**：
```c
int MPI_Allreduce(
    void* sendbuf,          // 发送缓冲区
    void* recvbuf,          // 接收缓冲区
    int count,              // 元素数量
    MPI_Datatype datatype,  // 数据类型
    MPI_Op op,              // 归约操作
    MPI_Comm comm           // 通信器
);
```

**归约操作**：
```c
// 常用归约操作
MPI_SUM     // 求和
MPI_MAX     // 最大值
MPI_MIN     // 最小值
MPI_PROD    // 乘积
MPI_LAND    // 逻辑与
MPI_BAND    // 位与
MPI_LOR     // 逻辑或
MPI_BOR     // 位或
```

**全局归约示例**：
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int rank, size;
    int local_sum, global_sum;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 每个进程计算局部和
    local_sum = rank * 10;
    printf("Process %d local sum: %d\n", rank, local_sum);

    // 全局求和
    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d global sum: %d\n", rank, global_sum);

    MPI_Finalize();
    return 0;
}
```

#### 6.1.3.5 全局通信模式

**MPI_Allgather**：
```c
// 所有进程收集所有其他进程的数据
MPI_Allgather(sendbuf, sendcount, sendtype,
              recvbuf, recvcount, recvtype, comm);
```

**MPI_Alltoall**：
```c
// 全对全通信
MPI_Alltoall(sendbuf, sendcount, sendtype,
             recvbuf, recvcount, recvtype, comm);
```

**MPI_Reduce_scatter**：
```c
// 归约后分散
MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, datatype, op, comm);
```

### 6.1.4 进程组管理

#### 6.1.4.1 进程组创建

**进程组（Group）**：
```c
MPI_Group group, new_group;
int ranks[] = {0, 2, 4};  // 选择的进程排名
int num_ranks = 3;

// 从通信器获取进程组
MPI_Comm_group(MPI_COMM_WORLD, &group);

// 创建新的进程组
MPI_Group_incl(group, num_ranks, ranks, &new_group);

// 从进程组创建通信器
MPI_Comm new_comm;
MPI_Comm_create(MPI_COMM_WORLD, new_group, &new_comm);

// 释放资源
MPI_Group_free(&group);
MPI_Group_free(&new_group);
```

**进程组操作**：
```c
// 进程组差集
MPI_Group_difference(group1, group2, &new_group);

// 进程组合并
MPI_Group_union(group1, group2, &new_group);

// 进程组交集
MPI_Group_intersection(group1, group2, &new_group);
```

#### 6.1.4.2 拓扑管理

**Cartesian拓扑**：
```c
int dims[2] = {0, 0};  // 维度大小，0表示自动计算
int periods[2] = {0, 0};  // 周期性
int reorder = 0;      // 是否允许重排序

// 计算最优维度
MPI_Dims_create(8, 2, dims);

// 创建Cartesian拓扑通信器
MPI_Comm cart_comm;
MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cart_comm);

// 获取进程在拓扑中的坐标
int coords[2];
MPI_Cart_coords(cart_comm, rank, 2, coords);

// 获取邻居进程
int left, right, up, down;
MPI_Cart_shift(cart_comm, 0, 1, &left, &right);  // x方向
MPI_Cart_shift(cart_comm, 1, 1, &up, &down);     // y方向
```

**Graph拓扑**：
```c
int index[] = {2, 4, 6};  // 每个进程的邻居数量
int edges[] = {1, 2, 0, 2, 0, 1};  // 邻居列表

// 创建Graph拓扑通信器
MPI_Comm graph_comm;
MPI_Graph_create(MPI_COMM_WORLD, 3, index, edges, 0, &graph_comm);
```

#### 6.1.4.3 动态进程管理

**进程创建**：
```c
MPI_Comm intercomm, intracomm;
char* command = "./worker_program";
char* argv[] = {NULL};

// 动态创建新进程
MPI_Comm_spawn(command, argv, 1, MPI_INFO_NULL, 0, MPI_COMM_SELF,
              &intercomm, MPI_ERRCODES_IGNORE);

// 获取新创建的进程间通信器
MPI_Intercomm_merge(intercomm, 0, &intracomm);
```

### 6.1.5 MPI高级特性

#### 6.1.5.1 派生数据类型

**结构体数据类型**：
```c
typedef struct {
    int id;
    double value;
    char name[20];
} Data;

// 创建派生数据类型
int block_lengths[] = {1, 1, 20};
MPI_Datatype types[] = {MPI_INT, MPI_DOUBLE, MPI_CHAR};
MPI_Aint displacements[3];

Data data;
MPI_Get_address(&data.id, &displacements[0]);
MPI_Get_address(&data.value, &displacements[1]);
MPI_Get_address(data.name, &displacements[2]);

// 相对位移
MPI_Aint base;
MPI_Get_address(&data, &base);
for (int i = 0; i < 3; i++) {
    displacements[i] -= base;
}

MPI_Datatype data_type;
MPI_Type_create_struct(3, block_lengths, displacements, types, &data_type);
MPI_Type_commit(&data_type);

// 使用派生类型
MPI_Send(&data, 1, data_type, dest, tag, comm);
```

**向量数据类型**：
```c
// 创建向量类型：每blocklength个元素取一个，共count个
MPI_Datatype vector_type;
MPI_Type_vector(count, blocklength, stride, MPI_DOUBLE, &vector_type);
MPI_Type_commit(&vector_type);

// 使用向量类型发送矩阵的列
double matrix[100][100];
MPI_Send(&matrix[0][col], 1, vector_type, dest, tag, comm);
```

#### 6.1.5.2 窗口和RMA

**单边通信**：
```c
MPI_Win win;
double* window_buffer;
int window_size = 1000;

// 创建窗口
MPI_Win_create(window_buffer, window_size * sizeof(double),
              sizeof(double), MPI_INFO_NULL, MPI_COMM_WORLD, &win);

// 同步
MPI_Win_fence(0, win);

// RMA操作
if (rank == 0) {
    // 进程0向进程1写入数据
    double data = 3.14;
    MPI_Put(&data, 1, MPI_DOUBLE, 1, 0, 1, MPI_DOUBLE, win);
}

// 同步
MPI_Win_fence(0, win);

// 释放窗口
MPI_Win_free(&win);
```

#### 6.1.5.3 性能调优

**通信器缓存**：
```c
// 避免重复创建通信器
static MPI_Comm cached_comm = MPI_COMM_NULL;

if (cached_comm == MPI_COMM_NULL) {
    // 创建并缓存通信器
    MPI_Comm_dup(MPI_COMM_WORLD, &cached_comm);
}
```

**非阻塞通信优化**：
```c
// 重叠计算和通信
MPI_Request requests[2];
double* send_buffer = malloc(buffer_size);
double* recv_buffer = malloc(buffer_size);

// 发起非阻塞通信
MPI_Isend(send_buffer, size, MPI_DOUBLE, dest, 0, comm, &requests[0]);
MPI_Irecv(recv_buffer, size, MPI_DOUBLE, source, 0, comm, &requests[1]);

// 执行计算
compute_heavy_work();

// 等待通信完成
MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);
```

## 6.2 OpenMP

### 6.2.1 OpenMP基础

#### 6.2.1.1 OpenMP简介
**OpenMP (Open Multi-Processing)** 是一个支持共享内存并行编程的API标准，主要用于多核处理器的并行计算。

**核心特性**：
- **编译指令**：使用#pragma指令控制并行
- **共享内存**：所有线程共享同一内存空间
- **简单易用**：容易将串行代码转换为并行
- **可移植性**：支持多种编译器和平台

#### 6.2.1.2 OpenMP环境设置

**编译**：
```bash
# GCC编译
gcc -fopenmp -o openmp_program openmp_program.c

# Intel编译器
icc -qopenmp -o openmp_program openmp_program.c

# Visual Studio
# 在项目属性中启用OpenMP支持
```

**运行时环境**：
```bash
# 设置线程数
export OMP_NUM_THREADS=4

# 设置调度策略
export OMP_SCHEDULE="dynamic,10"

# 设置嵌套并行
export OMP_NESTED=TRUE
```

#### 6.2.1.3 基本指令

**并行区域**：
```c
#include <omp.h>
#include <stdio.h>

int main() {
    printf("Serial code before parallel region\n");

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("Hello from thread %d of %d\n", thread_id, num_threads);
    }

    printf("Serial code after parallel region\n");
    return 0;
}
```

**线程信息**：
```c
// 获取线程ID
int thread_id = omp_get_thread_num();

// 获取线程总数
int num_threads = omp_get_num_threads();

// 获取最大线程数
int max_threads = omp_get_max_threads();

// 获取处理器数量
int num_procs = omp_get_num_procs();

// 检查是否在并行区域
int in_parallel = omp_in_parallel();
```

### 6.2.2 并行区域

#### 6.2.2.1 并行for循环

**基本语法**：
```c
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // 循环体
    array[i] = compute(i);
}
```

**调度策略**：
```c
// 静态调度
#pragma omp parallel for schedule(static)
for (int i = 0; i < n; i++) {
    // 每个线程处理固定数量的迭代
}

// 动态调度
#pragma omp parallel for schedule(dynamic, chunk_size)
for (int i = 0; i < n; i++) {
    // 动态分配迭代给空闲线程
}

// 运行时调度
#pragma omp parallel for schedule(runtime)
for (int i = 0; i < n; i++) {
    // 根据OMP_SCHEDULE环境变量决定
}

// guided调度
#pragma omp parallel for schedule(guided, chunk_size)
for (int i = 0; i < n; i++) {
    // 初始大块，逐渐减小
}
```

**并行for示例**：
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

#define N 1000000

int main() {
    double* array = malloc(N * sizeof(double));

    // 初始化数组
    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        array[i] = (double)rand() / RAND_MAX;
    }

    // 计算数组和
    double sum = 0.0;
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < N; i++) {
        sum += array[i];
    }

    printf("Array sum: %f\n", sum);
    free(array);
    return 0;
}
```

#### 6.2.2.2 sections指令

**sections分区**：
```c
#pragma omp parallel sections
{
    #pragma omp section
    {
        // section 1
        printf("Section 1 executed by thread %d\n", omp_get_thread_num());
    }

    #pragma omp section
    {
        // section 2
        printf("Section 2 executed by thread %d\n", omp_get_thread_num());
    }

    #pragma omp section
    {
        // section 3
        printf("Section 3 executed by thread %d\n", omp_get_thread_num());
    }
}
```

**动态sections**：
```c
#pragma omp parallel
#pragma omp sections nowait
{
    #pragma omp section
    {
        // 任务1
        heavy_computation_1();
    }

    #pragma omp section
    {
        // 任务2
        heavy_computation_2();
    }
}
// 不等待sections完成
light_computation();
```

#### 6.2.2.3 单线程执行

**single指令**：
```c
#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < 1000; i++) {
        work(i);
    }

    #pragma omp single
    {
        // 只有一个线程执行此代码
        printf("Single thread execution\n");
    }

    // 其他线程继续执行
    cleanup();
}
```

**master指令**：
```c
#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < 1000; i++) {
        work(i);
    }

    #pragma omp master
    {
        // 只有主线程执行此代码
        printf("Master thread execution\n");
    }
}
// 主线程不等待其他线程
```

### 6.2.3 数据共享管理

#### 6.2.3.1 数据共享属性

**共享和私有变量**：
```c
int shared_var = 0;  // 共享变量

#pragma omp parallel private(local_var) shared(shared_var)
{
    int local_var = 0;  // 每个线程的私有变量

    // 修改私有变量
    local_var = omp_get_thread_num();

    // 修改共享变量（需要同步）
    #pragma omp critical
    {
        shared_var += local_var;
    }
}
```

**数据属性子句**：
```c
// private: 每个线程创建私有副本
#pragma omp parallel for private(temp)
for (int i = 0; i < n; i++) {
    temp = array[i] * 2;
    result[i] = temp;
}

// firstprivate: 私有副本初始化为原始值
#pragma omp parallel for firstprivate(counter)
for (int i = 0; i < n; i++) {
    counter += i;  // 每个线程有自己的counter副本
}

// lastprivate: 最后一个迭代的值赋给原始变量
#pragma omp parallel for lastprivate(result)
for (int i = 0; i < n; i++) {
    result = i * i;
}
// result的值是最后一个迭代的值

// reduction: 归约操作
#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += array[i];
}
```

#### 6.2.3.2 归约操作

**归约子句**：
```c
// 求和
#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += data[i];
}

// 最大值
#pragma omp parallel for reduction(max:max_val)
for (int i = 0; i < n; i++) {
    if (data[i] > max_val) {
        max_val = data[i];
    }
}

// 最小值
#pragma omp parallel for reduction(min:min_val)
for (int i = 0; i < n; i++) {
    if (data[i] < min_val) {
        min_val = data[i];
    }
}

// 逻辑与
#pragma omp parallel for reduction(&&:all_true)
for (int i = 0; i < n; i++) {
    all_true = all_true && condition[i];
}
```

**自定义归约**：
```c
// OpenMP 4.0+ 支持自定义归约
#pragma omp declare reduction(vec_sum : std::vector<int> : \
    omp_out.insert(omp_out.end(), omp_in.begin(), omp_in.end()))

#pragma omp parallel for reduction(vec_sum:result)
for (int i = 0; i < n; i++) {
    result.push_back(data[i]);
}
```

### 6.2.4 同步机制

#### 6.2.4.1 屏障同步

**barrier指令**：
```c
#pragma omp parallel
{
    // 所有线程执行第一阶段
    phase1();

    // 所有线程在此处同步
    #pragma omp barrier

    // 所有线程执行第二阶段
    phase2();
}
```

**隐式屏障**：
```c
// for循环后有隐式屏障
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    work(i);
}
// 隐式屏障

// sections后有隐式屏障
#pragma omp parallel sections
{
    #pragma omp section
    work1();
    #pragma omp section
    work2();
}
// 隐式屏障
```

**nowait子句**：
```c
#pragma omp parallel for nowait
for (int i = 0; i < n; i++) {
    work(i);
}
// 没有隐式屏障，线程可以继续执行
cleanup();
```

#### 6.2.4.2 临界区

**critical指令**：
```c
#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < n; i++) {
        double result = compute(i);

        // 保护共享资源
        #pragma omp critical
        {
            printf("Thread %d: result = %f\n", omp_get_thread_num(), result);
            global_sum += result;
        }
    }
}
```

**named critical**：
```c
#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < n; i++) {
        // 保护输出
        #pragma omp critical(output)
        {
            printf("Output from thread %d\n", omp_get_thread_num());
        }

        // 保护文件写入
        #pragma omp critical(file_write)
        {
            write_to_file(result);
        }
    }
}
```

#### 6.2.4.3 原子操作

**atomic指令**：
```c
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    double val = compute(i);

    // 原子加法
    #pragma omp atomic
    global_sum += val;

    // 原子减法
    #pragma omp atomic
    counter--;

    // 原子乘法
    #pragma omp atomic
    product *= val;

    // 原子最小值
    #pragma omp atomic
    if (val < min_val) min_val = val;

    // 原子最大值
    #pragma omp atomic
    if (val > max_val) max_val = val;
}
```

#### 6.2.4.4 锁机制

**简单锁**：
```c
#include <omp.h>

omp_lock_t lock;

// 初始化锁
omp_init_lock(&lock);

#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < n; i++) {
        // 获取锁
        omp_set_lock(&lock);

        // 临界区代码
        printf("Thread %d processing %d\n", omp_get_thread_num(), i);

        // 释放锁
        omp_unset_lock(&lock);
    }
}

// 销毁锁
omp_destroy_lock(&lock);
```

**嵌套锁**：
```c
omp_nest_lock_t nest_lock;

// 初始化嵌套锁
omp_init_nest_lock(&nest_lock);

#pragma omp parallel
{
    #pragma omp for
    for (int i = 0; i < n; i++) {
        omp_set_nest_lock(&nest_lock);
        nested_function();
        omp_unset_nest_lock(&nest_lock);
    }
}

// 销毁嵌套锁
omp_destroy_nest_lock(&nest_lock);
```

### 6.2.5 OpenMP优化

#### 6.2.5.1 调度优化

**静态调度**：
```c
// 适用于迭代时间均匀的情况
#pragma omp parallel for schedule(static, chunk_size)
for (int i = 0; i < n; i++) {
    // 均匀负载
    work(i);
}
```

**动态调度**：
```c
// 适用于迭代时间不均匀的情况
#pragma omp parallel for schedule(dynamic, 1)
for (int i = 0; i < n; i++) {
    // 不均匀负载
    variable_work(i);
}
```

**guided调度**：
```c
// 适用于开始时迭代数多，后来迭代数少的情况
#pragma omp parallel for schedule(guided, 10)
for (int i = 0; i < n; i++) {
    work(i);
}
```

#### 6.2.5.2 数据局部性优化

**数据对齐**：
```c
// 确保数据对齐以提高缓存性能
#define CACHE_LINE_SIZE 64
#define ALIGNED_ALLOC(size) \
    aligned_alloc(CACHE_LINE_SIZE, size)

#pragma omp parallel
{
    // 每个线程使用对齐的数据
    double* local_data = ALIGNED_ALLOC(n * sizeof(double));

    #pragma omp for
    for (int i = 0; i < n; i++) {
        local_data[i] = compute(i);
    }

    free(local_data);
}
```

**减少false sharing**：
```c
// 使用填充避免false sharing
struct ThreadData {
    double sum;
    char padding[CACHE_LINE_SIZE - sizeof(double)];
};

#pragma omp parallel
{
    struct ThreadData local_data = {0.0};

    #pragma omp for
    for (int i = 0; i < n; i++) {
        local_data.sum += data[i];
    }

    // 最后归约到全局变量
    #pragma omp atomic
    global_sum += local_data.sum;
}
```

#### 6.2.5.3 嵌套并行

**嵌套并行控制**：
```c
// 启用嵌套并行
omp_set_nested(1);

#pragma omp parallel
{
    printf("Outer thread %d\n", omp_get_thread_num());

    #pragma omp parallel for
    for (int i = 0; i < 10; i++) {
        printf("  Inner thread %d, iteration %d\n",
               omp_get_thread_num(), i);
    }
}
```

**线程绑定**：
```c
// 设置线程绑定策略
export OMP_PROC_BIND=TRUE
export OMP_PLACES=cores

// 或者在代码中设置
omp_set_nested(1);
omp_set_dynamic(0);  // 禁用动态线程数调整
```

## 6.3 CUDA

### 6.3.1 CUDA架构

#### 6.3.1.1 GPU架构基础

**CUDA核心架构**：
- **Streaming Multiprocessor (SM)**：GPU的核心计算单元
- **CUDA Core**：基本的算术逻辑单元
- **Warp**：32个线程的执行单元
- **Memory Hierarchy**：多级内存结构

**GPU vs CPU对比**：
```
CPU: Few powerful cores, high clock speed, large cache
     - 适合串行任务和复杂控制流
     - 高延迟，高吞吐量

GPU: Many simpler cores, lower clock speed, small cache
     - 适合大规模并行计算
     - 低延迟，极高吞吐量
```

#### 6.3.1.2 CUDA编程模型

**主机-设备架构**：
```cuda
// 主机代码（CPU）
int main() {
    // 分配主机内存
    float* h_a = new float[N];

    // 分配设备内存
    float* d_a;
    cudaMalloc(&d_a, N * sizeof(float));

    // 数据传输：主机到设备
    cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice);

    // 启动kernel
    kernel_function<<<blocks, threads>>>(d_a);

    // 数据传输：设备到主机
    cudaMemcpy(h_a, d_a, N * sizeof(float), cudaMemcpyDeviceToHost);

    // 释放内存
    cudaFree(d_a);
    delete[] h_a;
}
```

**内存管理**：
```cuda
// 设备内存分配
float* d_data;
cudaMalloc((void**)&d_data, size * sizeof(float));

// 设备内存释放
cudaFree(d_data);

// 内存拷贝
cudaMemcpy(dst, src, size, cudaMemcpyKind);
// cudaMemcpyHostToDevice
// cudaMemcpyDeviceToHost
// cudaMemcpyDeviceToDevice
// cudaMemcpyHostToHost
```

### 6.3.2 线程层次模型

#### 6.3.2.1 线程组织

**线程层次结构**：
```
Grid
├── Block (1D, 2D, or 3D)
    ├── Thread (1D, 2D, or 3D)
    ├── Thread
    └── ...
├── Block
└── ...
```

**线程索引**：
```cuda
__global__ void example_kernel() {
    // 线程在block内的索引
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int tz = threadIdx.z;

    // block在grid内的索引
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int bz = blockIdx.z;

    // block的维度
    int bd_x = blockDim.x;
    int bd_y = blockDim.y;
    int bd_z = blockDim.z;

    // grid的维度
    int gd_x = gridDim.x;
    int gd_y = gridDim.y;
    int gd_z = gridDim.z;

    // 全局线程ID
    int global_id = blockIdx.x * blockDim.x + threadIdx.x;
}
```

#### 6.3.2.2 Kernel启动

**Kernel配置**：
```cuda
// 1D配置
kernel<<<num_blocks, threads_per_block>>>();

// 2D配置
dim3 block_size(16, 16);
dim3 grid_size((width + 15) / 16, (height + 15) / 16);
kernel<<<grid_size, block_size>>>();

// 3D配置
dim3 block_size(8, 8, 8);
dim3 grid_size((width + 7) / 8, (height + 7) / 8, (depth + 7) / 8);
kernel<<<grid_size, block_size>>>();
```

**Kernel示例**：
```cuda
__global__ void vector_add(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// 主机代码调用
int threads_per_block = 256;
int num_blocks = (n + threads_per_block - 1) / threads_per_block;
vector_add<<<num_blocks, threads_per_block>>>(d_a, d_b, d_c, n);
```

### 6.3.3 内存模型

#### 6.3.3.1 内存类型

**寄存器内存**：
```cuda
__global__ void kernel() {
    // 局部变量通常存储在寄存器中
    int local_var = threadIdx.x;
    float temp = 1.0f;
}
```

**共享内存**：
```cuda
__global__ void shared_memory_kernel() {
    // 声明共享内存
    __shared__ float shared_data[256];

    int tid = threadIdx.x;

    // 每个线程写入共享内存
    shared_data[tid] = tid * 1.0f;

    // 同步所有线程
    __syncthreads();

    // 读取共享内存
    if (tid > 0) {
        float prev = shared_data[tid - 1];
        // 使用前一个线程的数据
    }
}
```

**全局内存**：
```cuda
__global__ void global_memory_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 全局内存访问
        data[idx] = data[idx] * 2.0f;
    }
}
```

**常量内存**：
```cuda
// 声明常量内存
__constant__ float const_data[256];

__global__ void constant_memory_kernel() {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 读取常量内存
    float value = const_data[idx % 256];
    // 常量内存缓存，适合只读数据
}
```

**纹理内存**：
```cuda
// CUDA 11+ 使用纹理对象
cudaTextureObject_t tex_obj;

// 创建纹理对象
cudaResourceDesc res_desc = {};
res_desc.resType = cudaResourceTypeLinear;
res_desc.res.linear.devPtr = d_data;
res_desc.res.linear.sizeInBytes = n * sizeof(float);
res_desc.res.linear.desc = cudaCreateChannelDesc<float>();

cudaTextureDesc tex_desc = {};
tex_desc.readMode = cudaReadModeElementType;

cudaCreateTextureObject(&tex_obj, &res_desc, &tex_desc, NULL);

__global__ void texture_kernel() {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 通过纹理缓存读取
    float value = tex1Dfetch<float>(tex_obj, idx);
}
```

#### 6.3.3.2 内存访问模式

**合并访问**：
```cuda
__global__ void coalesced_access(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 合并访问：连续的线程访问连续的内存
    if (idx < N) {
        float value = data[idx];  // 好：合并访问
    }
}

__global__ void uncoalesced_access(float* data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 非合并访问：连续的线程访问不连续的内存
    if (idx < N) {
        float value = data[idx * stride];  // 坏：非合并访问
    }
}
```

**内存对齐**：
```cuda
// 使用对齐的内存分配
float* aligned_data;
cudaMalloc((void**)&aligned_data, aligned_size);

// 使用对齐的结构体
struct __align__(16) AlignedStruct {
    float4 data;  // 16字节对齐
};
```

### 6.3.4 CUDA编程

#### 6.3.4.1 基本CUDA程序

**完整示例**：
```cuda
#include <cuda_runtime.h>
#include <stdio.h>

#define N 1024

// Kernel函数
__global__ void vector_add(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// 错误检查宏
#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            printf("CUDA error at %s:%d - %s\n", __FILE__, __LINE__, \
                   cudaGetErrorString(error)); \
            exit(1); \
        } \
    } while(0)

int main() {
    // 1. 分配主机内存
    float *h_a, *h_b, *h_c;
    h_a = (float*)malloc(N * sizeof(float));
    h_b = (float*)malloc(N * sizeof(float));
    h_c = (float*)malloc(N * sizeof(float));

    // 2. 初始化主机数据
    for (int i = 0; i < N; i++) {
        h_a[i] = i;
        h_b[i] = i * 2;
    }

    // 3. 分配设备内存
    float *d_a, *d_b, *d_c;
    CUDA_CHECK(cudaMalloc((void**)&d_a, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc((void**)&d_b, N * sizeof(float)));
    CUDA_CHECK(cudaMalloc((void**)&d_c, N * sizeof(float)));

    // 4. 数据传输：主机到设备
    CUDA_CHECK(cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_b, h_b, N * sizeof(float), cudaMemcpyHostToDevice));

    // 5. 配置kernel执行参数
    int threads_per_block = 256;
    int num_blocks = (N + threads_per_block - 1) / threads_per_block;

    // 6. 启动kernel
    vector_add<<<num_blocks, threads_per_block>>>(d_a, d_b, d_c, N);

    // 7. 检查kernel执行错误
    CUDA_CHECK(cudaGetLastError());

    // 8. 等待kernel完成
    CUDA_CHECK(cudaDeviceSynchronize());

    // 9. 数据传输：设备到主机
    CUDA_CHECK(cudaMemcpy(h_c, d_c, N * sizeof(float), cudaMemcpyDeviceToHost));

    // 10. 验证结果
    for (int i = 0; i < N; i++) {
        if (h_c[i] != h_a[i] + h_b[i]) {
            printf("Error at index %d: %f != %f + %f\n",
                   i, h_c[i], h_a[i], h_b[i]);
            break;
        }
    }

    // 11. 释放内存
    free(h_a); free(h_b); free(h_c);
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    printf("Vector addition completed successfully!\n");
    return 0;
}
```

#### 6.3.4.2 编译和运行

**编译**：
```bash
# 使用nvcc编译
nvcc -o cuda_program cuda_program.cu

# 优化编译
nvcc -O3 -arch=sm_75 -o cuda_program cuda_program.cu

# 调试编译
nvcc -G -g -o cuda_program cuda_program.cu
```

**运行时检查**：
```cuda
// 检查设备信息
int device_count;
cudaGetDeviceCount(&device_count);

for (int i = 0; i < device_count; i++) {
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, i);
    printf("Device %d: %s\n", i, prop.name);
    printf("  Compute capability: %d.%d\n", prop.major, prop.minor);
    printf("  Multiprocessors: %d\n", prop.multiProcessorCount);
    printf("  Memory: %zu MB\n", prop.totalGlobalMem / (1024 * 1024));
}
```

### 6.3.5 GPU优化策略

#### 6.3.5.1 内存优化

**内存带宽最大化**：
```cuda
__global__ void optimized_copy(float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 使用向量类型提高带宽
    if (idx * 4 < n) {
        float4 vec = ((float4*)input)[idx];
        ((float4*)output)[idx] = vec;
    }
}
```

**共享内存优化**：
```cuda
__global__ void shared_memory_matmul(float* A, float* B, float* C,
                                    int width) {
    __shared__ float As[16][16];
    __shared__ float Bs[16][16];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * 16 + ty;
    int col = blockIdx.x * 16 + tx;

    float sum = 0.0f;

    // 分块计算
    for (int k = 0; k < (width + 15) / 16; k++) {
        // 加载数据到共享内存
        if (row < width && k * 16 + tx < width) {
            As[ty][tx] = A[row * width + k * 16 + tx];
        } else {
            As[ty][tx] = 0.0f;
        }

        if (col < width && k * 16 + ty < width) {
            Bs[ty][tx] = B[(k * 16 + ty) * width + col];
        } else {
            Bs[ty][tx] = 0.0f;
        }

        __syncthreads();

        // 计算部分结果
        for (int i = 0; i < 16; i++) {
            sum += As[ty][i] * Bs[i][tx];
        }

        __syncthreads();
    }

    if (row < width && col < width) {
        C[row * width + col] = sum;
    }
}
```

#### 6.3.5.2 计算优化

**指令级并行**：
```cuda
__global__ void pipelined_computation(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        float x = data[idx];

        // 通过指令重排提高并行度
        float a = x * 2.0f;
        float b = x + 1.0f;
        float c = x - 1.0f;

        // 同时执行多个独立操作
        float result = a * b + c * x;
        data[idx] = result;
    }
}
```

**分支发散优化**：
```cuda
__global__ void divergent_optimized(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        float value = data[idx];

        // 减少分支发散
        int thread_id_in_warp = threadIdx.x % 32;

        if (thread_id_in_warp < 16) {
            // warp的前半部分执行这个分支
            value = value * 2.0f;
        } else {
            // warp的后半部分执行这个分支
            value = value / 2.0f;
        }

        data[idx] = value;
    }
}
```

#### 6.3.5.3 性能分析

**CUDA Profiler**：
```bash
# 使用nvprof分析
nvprof ./cuda_program

# 生成可视化报告
nvprof --print-gpu-trace ./cuda_program
nvprof --analysis-metrics ./cuda_program

# 使用Nsight Systems
nsys profile ./cuda_program
```

**内联PTX汇编**：
```cuda
__device__ float fast_sqrt(float x) {
    float result;
    asm("sqrt.approx.ftz.f32 %0, %1;" : "=f"(result) : "f"(x));
    return result;
}
```

## 6.4 其他并行编程模型

### 6.4.1 OpenCL

#### 6.4.1.1 OpenCL基础

**OpenCL简介**：
- **跨平台**：支持CPU、GPU、FPGA等多种设备
- **异构计算**：统一的编程模型
- **标准API**：Khronos Group维护的标准

**基本组件**：
```c
#include <CL/cl.h>

// 平台和设备
cl_platform_id platform;
cl_device_id device;

// 上下文和命令队列
cl_context context;
cl_command_queue queue;

// 程序和kernel
cl_program program;
cl_kernel kernel;
```

#### 6.4.1.2 OpenCL程序结构

**主机代码**：
```c
#include <CL/cl.h>
#include <stdio.h>

#define MAX_DEVICES 10
#define MAX_SOURCE_SIZE 1000000

int main() {
    cl_platform_id platform_id;
    cl_device_id device_id[MAX_DEVICES];
    cl_uint num_devices;
    cl_uint num_platforms;

    // 获取平台
    clGetPlatformIDs(1, &platform_id, &num_platforms);

    // 获取设备
    clGetDeviceIDs(platform_id, CL_DEVICE_TYPE_GPU,
                   MAX_DEVICES, device_id, &num_devices);

    // 创建上下文
    cl_context context = clCreateContext(NULL, 1,
                                         &device_id[0], NULL, NULL, NULL);

    // 创建命令队列
    cl_command_queue queue = clCreateCommandQueue(context,
                                                 device_id[0], 0, NULL);

    // 读取kernel源码
    FILE* fp = fopen("kernel.cl", "r");
    char* source_str = (char*)malloc(MAX_SOURCE_SIZE);
    size_t source_size = fread(source_str, 1, MAX_SOURCE_SIZE, fp);
    fclose(fp);

    // 创建程序
    cl_program program = clCreateProgramWithSource(context, 1,
                                                  (const char**)&source_str,
                                                  (const size_t*)&source_size, NULL);

    // 编译程序
    clBuildProgram(program, 1, &device_id[0], NULL, NULL, NULL);

    // 创建kernel
    cl_kernel kernel = clCreateKernel(program, "vector_add", NULL);

    // 分配内存
    cl_mem d_a = clCreateBuffer(context, CL_MEM_READ_ONLY,
                                N * sizeof(float), NULL, NULL);
    cl_mem d_b = clCreateBuffer(context, CL_MEM_READ_ONLY,
                                N * sizeof(float), NULL, NULL);
    cl_mem d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY,
                                N * sizeof(float), NULL, NULL);

    // 设置kernel参数
    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);
    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);
    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);
    clSetKernelArg(kernel, 3, sizeof(int), &N);

    // 执行kernel
    size_t global_size = N;
    size_t local_size = 256;
    clEnqueueNDRangeKernel(queue, kernel, 1, NULL,
                           &global_size, &local_size, 0, NULL, NULL);

    // 清理资源
    clFlush(queue);
    clFinish(queue);
    clReleaseKernel(kernel);
    clReleaseProgram(program);
    clReleaseMemObject(d_a);
    clReleaseMemObject(d_b);
    clReleaseMemObject(d_c);
    clReleaseCommandQueue(queue);
    clReleaseContext(context);

    return 0;
}
```

**Kernel代码（kernel.cl）**：
```opencl
__kernel void vector_add(__global float* a,
                         __global float* b,
                         __global float* c,
                         int n) {
    int idx = get_global_id(0);

    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
```

### 6.4.2 Pthreads

#### 6.4.2.1 Pthreads基础

**POSIX线程简介**：
- **标准API**：POSIX标准定义的线程接口
- **轻量级**：比进程更轻量的并发单元
- **共享内存**：线程间共享进程地址空间

**基本函数**：
```c
#include <pthread.h>
#include <stdio.h>

// 线程函数
void* thread_function(void* arg) {
    int thread_id = *(int*)arg;
    printf("Hello from thread %d\n", thread_id);
    return NULL;
}

int main() {
    pthread_t threads[4];
    int thread_ids[4];

    // 创建线程
    for (int i = 0; i < 4; i++) {
        thread_ids[i] = i;
        pthread_create(&threads[i], NULL, thread_function, &thread_ids[i]);
    }

    // 等待线程完成
    for (int i = 0; i < 4; i++) {
        pthread_join(threads[i], NULL);
    }

    return 0;
}
```

#### 6.4.2.2 线程同步

**互斥锁**：
```c
#include <pthread.h>
#include <stdio.h>

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
int shared_counter = 0;

void* worker(void* arg) {
    for (int i = 0; i < 100000; i++) {
        // 加锁
        pthread_mutex_lock(&mutex);
        shared_counter++;
        pthread_mutex_unlock(&mutex);
    }
    return NULL;
}

int main() {
    pthread_t threads[4];

    // 创建线程
    for (int i = 0; i < 4; i++) {
        pthread_create(&threads[i], NULL, worker, NULL);
    }

    // 等待线程完成
    for (int i = 0; i < 4; i++) {
        pthread_join(threads[i], NULL);
    }

    printf("Final counter value: %d\n", shared_counter);
    return 0;
}
```

**条件变量**：
```c
#include <pthread.h>
#include <stdio.h>
#include <unistd.h>

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
int data_ready = 0;

void* producer(void* arg) {
    sleep(2);
    pthread_mutex_lock(&mutex);
    data_ready = 1;
    printf("Producer: Data is ready\n");
    pthread_cond_signal(&cond);
    pthread_mutex_unlock(&mutex);
    return NULL;
}

void* consumer(void* arg) {
    pthread_mutex_lock(&mutex);
    while (!data_ready) {
        printf("Consumer: Waiting for data...\n");
        pthread_cond_wait(&cond, &mutex);
    }
    printf("Consumer: Got the data\n");
    pthread_mutex_unlock(&mutex);
    return NULL;
}

int main() {
    pthread_t prod_thread, cons_thread;

    pthread_create(&cons_thread, NULL, consumer, NULL);
    pthread_create(&prod_thread, NULL, producer, NULL);

    pthread_join(prod_thread, NULL);
    pthread_join(cons_thread, NULL);

    return 0;
}
```

### 6.4.3 TBB

#### 6.4.3.1 Intel TBB简介

**Threading Building Blocks**：
- **C++库**：基于C++的并行编程库
- **任务并行**：基于任务的并行模型
- **算法模板**：丰富的并行算法模板

**基本使用**：
```cpp
#include <tbb/parallel_for.h>
#include <tbb/blocked_range.h>
#include <iostream>

class VectorAdd {
    float* a;
    float* b;
    float* c;
public:
    VectorAdd(float* _a, float* _b, float* _c) : a(_a), b(_b), c(_c) {}

    void operator()(const tbb::blocked_range<int>& range) const {
        for (int i = range.begin(); i != range.end(); ++i) {
            c[i] = a[i] + b[i];
        }
    }
};

int main() {
    const int N = 1000000;
    float* a = new float[N];
    float* b = new float[N];
    float* c = new float[N];

    // 初始化数据
    for (int i = 0; i < N; i++) {
        a[i] = i;
        b[i] = i * 2;
    }

    // 并行执行
    tbb::parallel_for(tbb::blocked_range<int>(0, N), VectorAdd(a, b, c));

    // 验证结果
    std::cout << "Result[0] = " << c[0] << std::endl;
    std::cout << "Result[100] = " << c[100] << std::endl;

    delete[] a; delete[] b; delete[] c;
    return 0;
}
```

#### 6.4.3.2 TBB高级特性

**并行算法**：
```cpp
#include <tbb/parallel_reduce.h>
#include <tbb/parallel_scan.h>
#include <tbb/parallel_sort.h>

// 并行归约
class SumReducer {
    float sum;
public:
    SumReducer() : sum(0) {}
    SumReducer(SumReducer&, tbb::split) : sum(0) {}

    void operator()(const tbb::blocked_range<float*>& range) {
        for (float* p = range.begin(); p != range.end(); ++p) {
            sum += *p;
        }
    }

    void join(const SumReducer& r) { sum += r.sum; }
    float get_sum() const { return sum; }
};

// 并行排序
std::vector<int> data = {5, 2, 8, 1, 9, 3};
tbb::parallel_sort(data.begin(), data.end());

// 并行扫描
tbb::parallel_scan(tbb::blocked_range<size_t>(0, data.size()),
                   ScanBody(data));
```

**任务调度**：
```cpp
#include <tbb/task.h>
#include <tbb/task_scheduler_init.h>

class MyTask : public tbb::task {
public:
    tbb::task* execute() {
        // 执行任务
        compute_heavy_work();
        return NULL;
    }
};

int main() {
    tbb::task_scheduler_init init;

    // 创建任务
    tbb::task_list list;
    for (int i = 0; i < 100; i++) {
        list.push_back(*new(tbb::task::allocate_root()) MyTask());
    }

    // 执行任务
    tbb::task::spawn_root_and_wait(list);

    return 0;
}
```

### 6.4.4 分布式计算框架

#### 6.4.4.1 Spark

**Apache Spark简介**：
- **大数据处理**：分布式数据处理框架
- **内存计算**：基于内存的计算模型
- **多种API**：支持Scala、Java、Python、R

**基本使用（Python）**：
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as spark_sum

# 创建Spark会话
spark = SparkSession.builder \
    .appName("ParallelExample") \
    .getOrCreate()

# 创建DataFrame
data = [(1, "Alice", 25), (2, "Bob", 30), (3, "Charlie", 35)]
df = spark.createDataFrame(data, ["id", "name", "age"])

# 并行操作
result = df.filter(col("age") > 25) \
           .groupBy("age") \
           .agg(spark_sum("id").alias("total_id")) \
           .collect()

for row in result:
    print(f"Age: {row.age}, Total ID: {row.total_id}")

spark.stop()
```

**RDD操作**：
```python
from pyspark import SparkContext

sc = SparkContext("local", "ParallelExample")

# 创建RDD
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(data)

# 转换操作
squared = rdd.map(lambda x: x * x)
filtered = squared.filter(lambda x: x > 20)

# 行动操作
result = filtered.reduce(lambda a, b: a + b)
print(f"Sum of squares > 20: {result}")

sc.stop()
```

#### 6.4.4.2 Hadoop MapReduce

**MapReduce模型**：
```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

    public static class TokenizerMapper
            extends Mapper<Object, Text, Text, IntWritable> {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer
            extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values,
                         Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

## 6.5 编程模型比较

### 6.5.1 模型特性对比

| 特性 | MPI | OpenMP | CUDA | OpenCL | Pthreads |
|------|-----|--------|------|--------|----------|
| **内存模型** | 分布式 | 共享 | 分层 | 分层 | 共享 |
| **适用场景** | 集群 | 多核CPU | GPU | 异构 | 多核CPU |
| **编程复杂度** | 高 | 低 | 中 | 高 | 中 |
| **性能** | 高 | 中 | 最高 | 高 | 中 |
| **可移植性** | 高 | 高 | 低 | 高 | 中 |

### 6.5.2 选择指南

**MPI适用场景**：
- 分布式内存系统
- 大规模集群计算
- 进程间通信密集型应用
- 需要跨节点扩展

**OpenMP适用场景**：
- 共享内存多核系统
- 代码简单易并行化
- 快速原型开发
- CPU密集型计算

**CUDA适用场景**：
- GPU加速计算
- 大规模数据并行
- 图形和科学计算
- 高性能需求

**OpenCL适用场景**：
- 跨平台异构计算
- 多设备支持
- 标准化需求
- 灵活的设备选择

**Pthreads适用场景**：
- 低层次线程控制
- 系统级编程
- 精确的同步控制
- 轻量级并发

## 6.6 混合编程模型

### 6.6.1 MPI + OpenMP

**混合并行模型**：
```c
#include <mpi.h>
#include <omp.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int world_rank, world_size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    printf("MPI process %d of %d\n", world_rank, world_size);

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("  MPI %d: OpenMP thread %d of %d\n",
               world_rank, thread_id, num_threads);
    }

    MPI_Finalize();
    return 0;
}
```

**编译和运行**：
```bash
# 编译
mpicc -fopenmp -o hybrid_program hybrid_program.c

# 运行（4个MPI进程，每个进程2个OpenMP线程）
export OMP_NUM_THREADS=2
mpirun -np 4 ./hybrid_program
```

### 6.6.2 MPI + CUDA

**GPU加速的MPI程序**：
```c
#include <mpi.h>
#include <cuda_runtime.h>
#include <stdio.h>

#define N 1000

__global__ void gpu_add(float* a, float* b, float* c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

int main(int argc, char** argv) {
    int rank, size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 每个进程分配GPU内存
    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, N * sizeof(float));
    cudaMalloc(&d_b, N * sizeof(float));
    cudaMalloc(&d_c, N * sizeof(float));

    // 设置GPU设备（每个进程使用不同GPU）
    cudaSetDevice(rank % 4);

    // 执行GPU计算
    int threads_per_block = 256;
    int num_blocks = (N + threads_per_block - 1) / threads_per_block;
    gpu_add<<<num_blocks, threads_per_block>>>(d_a, d_b, d_c, N);
    cudaDeviceSynchronize();

    // MPI通信
    float local_result, global_result;
    // ... 数据传输和通信 ...

    // 清理GPU内存
    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);

    MPI_Finalize();
    return 0;
}
```

### 6.6.3 OpenMP + CUDA

**CPU-GPU混合并行**：
```cuda
#include <omp.h>
#include <cuda_runtime.h>

#define N 1000000

__global__ void gpu_kernel(float* data, int start, int end) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x + start;
    if (idx < end) {
        data[idx] = data[idx] * 2.0f;
    }
}

int main() {
    float* h_data = new float[N];
    float* d_data;

    // 初始化数据
    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        h_data[i] = i;
    }

    // 分配GPU内存
    cudaMalloc(&d_data, N * sizeof(float));

    // CPU-GPU混合并行
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();

        // 每个线程处理数据的不同部分
        int chunk_size = N / num_threads;
        int start = thread_id * chunk_size;
        int end = (thread_id == num_threads - 1) ? N : (thread_id + 1) * chunk_size;

        // 传输数据到GPU
        cudaMemcpy(d_data + start, h_data + start,
                   (end - start) * sizeof(float), cudaMemcpyHostToDevice);

        // 在GPU上执行kernel
        int threads_per_block = 256;
        int num_blocks = (end - start + threads_per_block - 1) / threads_per_block;
        gpu_kernel<<<num_blocks, threads_per_block>>>(d_data, start, end);

        // 从GPU传输结果回来
        cudaMemcpy(h_data + start, d_data + start,
                   (end - start) * sizeof(float), cudaMemcpyDeviceToHost);
    }

    delete[] h_data;
    cudaFree(d_data);
    return 0;
}
```

## 本章小结

本章详细介绍了主要的并行编程模型，包括：

### 核心编程模型
1. **MPI**：分布式内存消息传递，适用于集群计算
2. **OpenMP**：共享内存编译指令，适用于多核CPU
3. **CUDA**：GPU并行编程，适用于大规模数据并行

### 其他重要模型
4. **OpenCL**：跨平台异构计算
5. **Pthreads**：POSIX线程标准
6. **TBB**：Intel线程构建模块
7. **分布式框架**：Spark、Hadoop MapReduce

### 关键要点
- **模型选择**：根据硬件架构和应用需求选择合适的模型
- **性能优化**：理解各模型的优化策略和最佳实践
- **混合编程**：结合多种模型发挥不同硬件的优势
- **可移植性**：考虑代码在不同平台的可移植性

### 实际应用建议
- **小规模并行**：优先考虑OpenMP，简单易用
- **大规模集群**：使用MPI，支持分布式计算
- **GPU加速**：选择CUDA或OpenCL
- **跨平台需求**：考虑OpenCL和标准库
- **混合架构**：结合多种模型实现最优性能

通过掌握这些并行编程模型，开发者可以根据具体的应用场景和硬件环境选择最适合的工具，实现高效的并行计算解决方案。