# 第三章：并行算法设计

并行算法设计是高性能计算的核心。本章将深入探讨并行算法的设计原则、常见模式、性能分析方法以及具体的优化策略。通过理解这些基本概念，我们能够设计出高效、可扩展的并行程序，充分利用现代硬件的计算能力。

## 目录
- [3.1 并行算法设计原则](#31-并行算法设计原则)
  - [3.1.1 分解 (Decomposition)](#311-分解-decomposition)
  - [3.1.2 通信 (Communication)](#312-通信-communication)
  - [3.1.3 同步 (Synchronization)](#313-同步-synchronization)
  - [3.1.4 映射 (Mapping)](#314-映射-mapping)
- [3.2 常见并行算法模式](#32-常见并行算法模式)
  - [3.2.1 分治法 (Divide and Conquer)](#321-分治法-divide-and-conquer)
  - [3.2.2 流水线 (Pipeline)](#322-流水线-pipeline)
  - [3.2.3 主从模式 (Master-Slave)](#323-主从模式-master-slave)
  - [3.2.4 工作窃取 (Work Stealing)](#324-工作窃取-work-stealing)
- [3.3 并行算法性能分析](#33-并行算法性能分析)
  - [3.3.1 性能度量指标](#331-性能度量指标)
  - [3.3.2 理论模型](#332-理论模型)
  - [3.3.3 性能分析工具](#333-性能分析工具)
  - [3.3.4 性能瓶颈识别](#334-性能瓶颈识别)
- [3.4 并行算法设计实例](#34-并行算法设计实例)
  - [3.4.1 并行排序算法](#341-并行排序算法)
  - [3.4.2 并行搜索算法](#342-并行搜索算法)
  - [3.4.3 并行数值计算](#343-并行数值计算)
  - [3.4.4 并行图算法](#344-并行图算法)
- [3.5 并行算法优化策略](#35-并行算法优化策略)
  - [3.5.1 负载均衡优化](#351-负载均衡优化)
  - [3.5.2 通信优化](#352-通信优化)
  - [3.5.3 内存访问优化](#353-内存访问优化)
  - [3.5.4 算法级优化](#354-算法级优化)

---

## 3.1 并行算法设计原则

设计一个高效的并行算法通常遵循四个核心步骤，即 **PCAM** 设计方法学：分解 (Partitioning/Decomposition)、通信 (Communication)、聚合/同步 (Agglomeration/Synchronization) 和映射 (Mapping)。

### 3.1.1 分解 (Decomposition)

**概念**：将复杂的计算问题分解为可以并发执行的较小任务或子问题。这是并行化的第一步，也是最关键的一步。

**常见策略**：
- **数据分解 (Data Decomposition)**：
  - 适用于数据量大、计算规则统一的场景（如矩阵运算、图像处理）。
  - 方法：将数据集划分为多个块（Block）或条带（Strip），分配给不同处理器。
- **功能分解 (Functional Decomposition)**：
  - 适用于任务包含多个不同功能模块的场景（如复杂的模拟系统、气候模型）。
  - 方法：将计算过程中的不同功能步骤分配给不同的处理器（如流水线）。
- **递归分解 (Recursive Decomposition)**：
  - 适用于分治算法（如快速排序）。
  - 方法：递归地将问题分解为子问题，直到子问题规模足够小可以串行解决。

**设计原则**：
- **独立性**：子问题之间应尽可能减少依赖。
- **粒度适中**：
  - **过细**：任务切换和通信开销过大。
  - **过粗**：并发度不足，难以平衡负载。

```python
# 数据分解示例：并行矩阵乘法（行分解）
def parallel_matrix_multiply(A, B, num_processors):
    rows_per_processor = len(A) // num_processors
    results = []

    # 模拟任务分发
    for i in range(num_processors):
        start_row = i * rows_per_processor
        # 处理剩余行
        end_row = (i + 1) * rows_per_processor if i < num_processors - 1 else len(A)
        
        # 每个"处理器"只计算分配到的行
        # 在实际MPI中，这里会分发数据到不同节点
        processor_result = multiply_rows(A[start_row:end_row], B)
        results.append(processor_result)

    return combine_results(results)
```

### 3.1.2 通信 (Communication)

**概念**：确定子任务执行过程中需要交换的数据和控制信息。并行任务通常不是完全隔离的，它们需要协作。

**通信模式**：
- **点对点通信 (Point-to-Point)**：
  - 两个特定的处理器之间直接传输数据（如 `MPI_Send`, `MPI_Recv`）。
  - 灵活性高，但管理复杂，容易死锁。
- **集体通信 (Collective)**：
  - 涉及通信域内所有处理器的操作（如广播 `Broadcast`、规约 `Reduce`、全交换 `All-to-All`）。
  - 通常经过高度优化，比手动实现点对点更高效。
- **共享内存 (Shared Memory)**：
  - 通过读写同一块内存地址进行隐式通信（如 OpenMP, Pthreads）。
  - 需注意竞态条件（Race Condition）。

**优化原则**：
- **最小化通信量**：优先进行本地计算。
- **通信与计算重叠**：利用非阻塞通信（Non-blocking communication）隐藏通信延迟。
- **批量传输**：将多次小数据传输合并为一次大数据传输，减少启动开销（Latency）。

```c
// MPI 点对点通信示例
// 进程 0 发送数据给进程 1
if (rank == 0) {
    MPI_Send(data, count, MPI_INT, 1, tag, MPI_COMM_WORLD);
} else if (rank == 1) {
    MPI_Recv(buffer, count, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);
}
```

### 3.1.3 同步 (Synchronization)

**概念**：协调多个并行任务的执行顺序，确保数据一致性和程序正确性。

**主要机制**：
- **屏障 (Barrier)**：所有参与的线程或进程必须到达此点后，才能继续执行。用于阶段性同步。
- **互斥锁 (Mutex/Lock)**：确保同一时间只有一个线程访问共享资源（临界区）。
- **信号量 (Semaphore)**：控制对有限数量资源的访问。
- **条件变量 (Condition Variable)**：允许线程在特定条件满足前挂起。

**设计陷阱**：
- **死锁 (Deadlock)**：多个线程互相等待对方释放资源。
- **活锁 (Livelock)**：线程不断改变状态以响应对方，但无法取得进展。
- **串行化瓶颈**：过多的同步会导致程序退化为串行执行。

```c
// OpenMP 屏障同步示例
#pragma omp parallel
{
    setup_data();
    // 强制所有线程完成setup后才能开始计算
    #pragma omp barrier 
    process_data();
}
```

### 3.1.4 映射 (Mapping)

**概念**：将分解后的任务分配给具体的物理处理单元（CPU 核心、GPU 线程、集群节点）。

**策略**：
- **静态映射 (Static Mapping)**：
  - 在编译时或程序启动时确定分配方案。
  - 适用于任务量已知且均匀的场景。
  - 开销小，但适应性差。
- **动态映射 (Dynamic Mapping)**：
  - 在运行时根据系统负载动态分配任务（如工作窃取）。
  - 适用于任务不均匀或计算环境动态变化的场景。
  - 开销较大，但负载均衡效果好。

**原则**：
- **负载均衡**：确保所有处理器都处于忙碌状态，避免"木桶效应"。
- **数据局部性**：将计算任务分配到数据所在的节点（Data Locality），减少远程内存访问。

---

## 3.2 常见并行算法模式

### 3.2.1 分治法 (Divide and Conquer)

**基本思想**：
将大问题递归地分解为若干个独立的子问题，并行求解子问题，最后合并结果。

**适用场景**：
- 问题具有递归结构。
- 子问题相互独立。
- 排序（归并、快速排序）、树/图算法、FFT。

**代码示例：并行归并排序**

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_merge_sort(arr, depth=0, max_depth=3):
    # 基础情况：数组过小或达到递归深度限制，转为串行排序
    if len(arr) <= 1 or depth >= max_depth:
        return sorted(arr)

    mid = len(arr) // 2
    left_part = arr[:mid]
    right_part = arr[mid:]

    # 并行递归
    with ThreadPoolExecutor(max_workers=2) as executor:
        left_future = executor.submit(parallel_merge_sort, left_part, depth + 1, max_depth)
        right_future = executor.submit(parallel_merge_sort, right_part, depth + 1, max_depth)
        
        left_sorted = left_future.result()
        right_sorted = right_future.result()

    return merge(left_sorted, right_sorted)

def merge(left, right):
    # 标准归并操作
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result
```

### 3.2.2 流水线 (Pipeline)

**基本思想**：
将计算过程分解为若干个连续的阶段（Stage），数据流像工厂流水线一样依次经过各个阶段。不同阶段可以并行处理不同的数据项。

**结构**：
```
数据流 -> [阶段1] -> [阶段2] -> [阶段3] -> 结果
            ^         ^         ^
         处理器A    处理器B   处理器C
```

**特点**：
- **吞吐量 (Throughput)**：大幅提升。
- **延迟 (Latency)**：单项数据的处理时间可能因缓冲而略微增加。
- **适用性**：视频解码、图像处理、编译器前端。

**代码示例：图像处理流水线**

```python
import multiprocessing as mp
from queue import Empty

def stage_1_preprocess(input_q, output_q):
    while True:
        item = input_q.get()
        if item is None:
            output_q.put(None)
            break
        # 模拟预处理
        processed = f"Preprocessed({item})"
        output_q.put(processed)

def stage_2_process(input_q, output_q):
    while True:
        item = input_q.get()
        if item is None:
            output_q.put(None)
            break
        # 模拟核心处理
        processed = f"Analyzed({item})"
        output_q.put(processed)

def stage_3_save(input_q):
    while True:
        item = input_q.get()
        if item is None:
            break
        print(f"Saved: {item}")

# 编排流水线
def run_pipeline(data_items):
    q1 = mp.Queue()
    q2 = mp.Queue()
    
    p1 = mp.Process(target=stage_1_preprocess, args=(q1, q2))
    p2 = mp.Process(target=stage_2_process, args=(q2, q2)) # 注意：这里简化了，实际应连接到下一级
    # 修正逻辑：
    q_final = mp.Queue()
    
    p1 = mp.Process(target=stage_1_preprocess, args=(q1, q2))
    p2 = mp.Process(target=stage_2_process, args=(q2, q_final))
    p3 = mp.Process(target=stage_3_save, args=(q_final,))
    
    # 启动进程
    for p in [p1, p2, p3]: p.start()
    
    # 注入数据
    for item in data_items:
        q1.put(item)
    q1.put(None) # 结束信号
    
    for p in [p1, p2, p3]: p.join()
```

### 3.2.3 主从模式 (Master-Slave / Manager-Worker)

**基本思想**：
一个主进程（Master）负责任务管理、分发和结果收集；多个从进程（Slave/Worker）负责执行具体的计算任务。

**优缺点**：
- ✅ 易于实现，负载均衡容易控制（主进程按需分发）。
- ❌ 主进程可能成为性能瓶颈（单点故障/通信拥堵）。

**适用场景**：
- 任务池模式。
- Web 服务器（请求分发）。
- 参数扫描实验。

### 3.2.4 工作窃取 (Work Stealing)

**基本思想**：
每个处理器维护自己的本地任务队列。当一个处理器完成了自己的所有任务时，它主动从其他繁忙处理器的队列末尾"窃取"任务来执行。

**优势**：
- **自适应负载均衡**：无需中心化的调度器。
- **减少竞争**：窃取通常发生在队列另一端（victim 从头部取，thief 从尾部取）。
- Cilk, Intel TBB, Go Runtime Scheduler 均采用此模式。

---

## 3.3 并行算法性能分析

### 3.3.1 性能度量指标

1.  **加速比 (Speedup)**
    衡量并行化带来的速度提升。
    $$S_p = \frac{T_1}{T_p}$$
    - $T_1$：最佳串行算法的执行时间。
    - $T_p$：使用 $p$ 个处理器的并行执行时间。

2.  **效率 (Efficiency)**
    衡量处理器利用率。
    $$E_p = \frac{S_p}{p} = \frac{T_1}{p \times T_p}$$
    - 理想情况下 $E_p = 1$ (100%)，实际通常 $< 1$。

3.  **可扩展性 (Scalability)**
    随着处理器数量 $p$ 的增加，加速比 $S_p$ 是否能保持线性增长的能力。

### 3.3.2 理论模型

-   **Amdahl 定律 (固定负载)**：
    并行系统的最大加速比受限于程序中只能串行执行的部分。
    $$S_{max} = \frac{1}{s + \frac{1-s}{p}}$$
    - $s$：串行部分比例。
    - 结论：如果程序有 10% 必须串行，无论加多少处理器，加速比永远无法超过 10 倍。

-   **Gustafson 定律 (扩展负载)**：
    如果问题规模随处理器数量增加而扩大，加速比可以突破 Amdahl 定律的限制。
    $$S_p = p - s(p-1)$$
    - 结论：只要计算量增长快于串行部分，大规模并行是有效的。

### 3.3.3 性能分析工具

-   **Python**: `cProfile`, `line_profiler`, `viztracer`.
-   **C/C++**: `gprof`, `Valgrind (Callgrind)`, `Intel VTune`, `perf`.
-   **MPI**: `Tau`, `Vampir`, `HPCToolkit`.

--- 

## 3.4 并行算法设计实例

### 3.4.1 并行排序算法
（见 3.2.1 归并排序示例，另有双调排序 Bitonic Sort 适合 GPU）

### 3.4.2 并行搜索算法
- **并行二分搜索**：将搜索空间划分，多线程同时检查。
- **并行图搜索 (BFS/DFS)**：每一层的节点扩展可以并行化（需处理访问去重）。

### 3.4.3 并行数值计算
- **蒙特卡洛模拟**：极佳的并行性（Embarrassingly Parallel）。每个样本生成完全独立，只需最后汇总。
- **矩阵乘法**：分块算法（Cannon 算法, Fox 算法）。

### 3.4.4 并行图算法
- **并行 Dijkstra**：并发更新邻居节点距离。
- **并行 PageRank**：基于矩阵向量乘法的迭代，非常适合 MapReduce 或 Spark。

---

## 3.5 并行算法优化策略

### 3.5.1 负载均衡优化
- **静态划分**：如果任务大小已知且固定，使用循环划分（Round-robin）或块划分。
- **动态调度**：使用任务队列（Task Queue）。
- **工作窃取**：解决极端不平衡问题。

### 3.5.2 通信优化
- **通信聚合**：将许多小消息合并为一个大消息发送，减少网络延迟（Latency）的影响。
- **异步通信**：使用非阻塞 `Isend`/`Irecv`，在等待数据传输时执行计算（Hiding Communication Latency）。
- **拓扑感知**：根据物理网络拓扑（如环形、网格）安排通信顺序。

### 3.5.3 内存访问优化
- **缓存亲和性 (Cache Affinity)**：
    - 确保线程尽可能在同一个核心上运行，复用 L1/L2 缓存。
    - 数据布局优化：结构数组 (AoS) vs 数组结构 (SoA)。一般而言，SoA 对 SIMD 和 GPU 更友好。
- **减少伪共享 (False Sharing)**：
    - 不同线程频繁写入处于同一缓存行（Cache Line）的不同变量，导致缓存失效风暴。
    - **解决**：对变量进行填充（Padding/Aligning），使其独占缓存行。

### 3.5.4 算法级优化

除了底层的系统优化，选择正确的算法复杂度至关重要：

1.  **降低算法复杂度**：
    - 并行化一个 $O(N^2)$ 的算法（如冒泡排序）即使有线性加速，通常也不如串行的 $O(N \log N)$ 算法（如快速排序）快。
    - **原则**：始终在最优串行算法的基础上进行并行化。

2.  **减少冗余计算**：
    - 有时为了减少通信，会让不同处理器重复计算某些中间值。需权衡"重算成本"与"通信成本"。
    - 避免为了并行而引入过多的额外计算开销（Overhead）。

3.  **预测与推断 (Speculation)**：
    - 在结果确认前预先执行可能的路径（如分支预测）。如果猜对，通过；猜错，回滚。
    - 适用于依赖关系复杂、难以静态并行的场景。

4.  **空间换时间**：
    - 在分布式计算中，通过复制数据（Replication）到各个节点，避免远程读取，牺牲内存/磁盘空间换取本地访问速度。

5.  **近似计算**：
    - 在容许误差的场景（如机器学习训练、图像渲染），使用低精度计算（FP16/INT8）或近似算法来大幅提升并行吞吐量。
