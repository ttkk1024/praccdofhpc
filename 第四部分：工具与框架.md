# 第四部分：工具与框架

## 第10章 MPI编程

### 10.1 MPI基础概念

#### 10.1.1 进程模型
**独立进程通信模型**：
- 每个进程有独立的内存空间
- 通过显式消息传递共享数据
- 进程间无共享内存（不同于OpenMP）
- 支持分布式内存系统

**进程启动方式**：
```c
// 单程序多数据流(Single Program Multiple Data, SPMD)
// 所有进程运行相同程序，通过rank区分角色
int rank, size;
MPI_Init(&argc, &argv);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);

if (rank == 0) {
    // 主进程：负责I/O和协调
    printf("Master process (rank %d) managing %d processes\n", rank, size);
} else {
    // 工作进程：执行计算任务
    printf("Worker process (rank %d) ready\n", rank);
}
```

#### 10.1.2 通信器(Communicator)
**通信器概念**：
- 定义进程组和通信上下文
- 提供进程间通信的命名空间
- 避免不同通信操作间的干扰

**预定义通信器**：
```c
// MPI_COMM_WORLD：包含所有进程的默认通信器
MPI_Comm world_comm = MPI_COMM_WORLD;

// MPI_COMM_SELF：仅包含当前进程的通信器
MPI_Comm self_comm = MPI_COMM_SELF;
```

**通信器创建与管理**：
```c
MPI_Comm new_comm;
int color = rank % 2;  // 按rank奇偶性分组
MPI_Comm_split(world_comm, color, rank, &new_comm);

int new_rank, new_size;
MPI_Comm_rank(new_comm, &new_rank);
MPI_Comm_size(new_comm, &new_size);
printf("Original rank %d -> New rank %d in subgroup of size %d\n",
       rank, new_rank, new_size);
```

#### 10.1.3 排名系统(Rank System)
**排名机制**：
- 每个进程在通信器中有唯一整数标识符
- 排名从0开始，连续分配
- 用于指定消息的发送者和接收者

**进程角色分配**：
```c
#define MASTER_RANK 0

if (rank == MASTER_RANK) {
    // 主进程：协调全局操作
    printf("I am the master (rank %d)\n", rank);
} else {
    // 工作进程：执行分配的任务
    printf("I am worker (rank %d)\n", rank);
}

// 动态角色分配示例
int num_masters = 2;
if (rank < num_masters) {
    printf("Master role assigned to rank %d\n", rank);
} else {
    printf("Worker role assigned to rank %d\n", rank);
}
```

#### 10.1.4 上下文管理(Context Management)
**通信上下文**：
- 提供通信操作的隔离机制
- 避免不同应用或模块间的通信冲突
- 支持安全的并行执行

**上下文创建示例**：
```c
// 创建专用通信上下文
MPI_Comm create_context(MPI_Comm base_comm, int color) {
    MPI_Comm new_comm;
    MPI_Comm_split(base_comm, color, rank, &new_comm);
    return new_comm;
}

// 使用不同上下文进行并行通信
MPI_Comm comm1 = create_context(MPI_COMM_WORLD, 1);
MPI_Comm comm2 = create_context(MPI_COMM_WORLD, 2);

// 两个通信器独立工作，互不干扰
if (rank % 2 == 0) {
    // 使用comm1进行通信
    MPI_Send(data, count, MPI_INT, (rank+2)%size, 0, comm1);
} else {
    // 使用comm2进行通信
    MPI_Send(data, count, MPI_INT, (rank+2)%size, 0, comm2);
}
```

#### 10.1.5 MPI环境管理
**初始化与终止**：
```c
// MPI初始化：必须在所有MPI调用前执行
int argc = 0;
char **argv = NULL;
MPI_Init(&argc, &argv);

// 获取进程信息
int world_size, world_rank;
MPI_Comm_size(MPI_COMM_WORLD, &world_size);
MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

printf("Process %d of %d is alive\n", world_rank, world_size);

// MPI终止：必须在最后调用
MPI_Finalize();
```

**环境查询**：
```c
// MPI版本信息
int version, subversion;
MPI_Get_version(&version, &subversion);
printf("MPI Version: %d.%d\n", version, subversion);

// MPI库实现信息
char version_string[100];
int resultlen;
MPI_Get_library_version(version_string, &resultlen);
printf("MPI Library: %s\n", version_string);

// 进程拓扑信息
int num_nodes, node_id;
MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED,
                    0, MPI_INFO_NULL, &shared_comm);
```

#### 10.1.6 错误处理
**错误处理机制**：
```c
// 默认错误处理：终止程序
MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);

// 自定义错误处理
void custom_error_handler(MPI_Comm *comm, int *error_code, ...) {
    char error_string[100];
    int length;
    MPI_Error_string(*error_code, error_string, &length);
    printf("MPI Error: %s\n", error_string);
    // 可以选择继续执行或终止
}

MPI_Comm_set_errhandler(MPI_COMM_WORLD, custom_error_handler);
```

#### 10.1.7 进程拓扑
**虚拟拓扑结构**：
```c
// 创建2D网格拓扑
int dims[2] = {0, 0};  // 让MPI自动计算维度
int periods[2] = {0, 0};  // 非周期性边界
int reorder = 0;

MPI_Dims_create(size, 2, dims);
MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cart_comm);

// 查询进程在网格中的坐标
int coords[2];
MPI_Cart_coords(cart_comm, rank, 2, coords);
printf("Process %d is at position (%d, %d)\n", rank, coords[0], coords[1]);

// 查找邻居进程
int neighbor_up, neighbor_down;
MPI_Cart_shift(cart_comm, 0, 1, &neighbor_up, &neighbor_down);
printf("Process %d: up neighbor=%d, down neighbor=%d\n",
       rank, neighbor_up, neighbor_down);
```

#### 10.1.8 实际应用示例
**并行向量加法**：
```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv) {
    // 1. MPI初始化
    MPI_Init(&argc, &argv);

    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // 2. 数据分配
    const int N = 1000000;
    int local_n = N / world_size;
    int local_start = world_rank * local_n;
    int local_end = local_start + local_n;

    // 3. 分配本地数组
    double *local_a = malloc(local_n * sizeof(double));
    double *local_b = malloc(local_n * sizeof(double));
    double *local_c = malloc(local_n * sizeof(double));

    // 4. 初始化数据（仅主进程生成完整数据）
    double *global_a = NULL, *global_b = NULL;
    if (world_rank == 0) {
        global_a = malloc(N * sizeof(double));
        global_b = malloc(N * sizeof(double));
        for (int i = 0; i < N; i++) {
            global_a[i] = i * 1.0;
            global_b[i] = i * 2.0;
        }
    }

    // 5. 数据分发
    MPI_Scatter(global_a, local_n, MPI_DOUBLE,
                local_a, local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    MPI_Scatter(global_b, local_n, MPI_DOUBLE,
                local_b, local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // 6. 并行计算
    for (int i = 0; i < local_n; i++) {
        local_c[i] = local_a[i] + local_b[i];
    }

    // 7. 结果收集
    double *global_c = NULL;
    if (world_rank == 0) {
        global_c = malloc(N * sizeof(double));
    }

    MPI_Gather(local_c, local_n, MPI_DOUBLE,
               global_c, local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // 8. 验证结果（仅主进程）
    if (world_rank == 0) {
        int errors = 0;
        for (int i = 0; i < N; i++) {
            if (global_c[i] != global_a[i] + global_b[i]) {
                errors++;
            }
        }
        printf("Vector addition completed with %d errors\n", errors);
    }

    // 9. 内存清理
    free(local_a); free(local_b); free(local_c);
    if (world_rank == 0) {
        free(global_a); free(global_b); free(global_c);
    }

    // 10. MPI终止
    MPI_Finalize();
    return 0;
}
```

**编译和运行**：
```bash
# 编译
mpicc -o vector_add vector_add.c -O2

# 运行（4个进程）
mpirun -np 4 ./vector_add

# 在集群上运行
mpirun -np 16 -hostfile hosts ./vector_add
```

#### 10.1.9 常见陷阱与最佳实践
**常见错误**：
1. **死锁**：进程间相互等待
2. **数据竞争**：多个进程同时访问共享资源
3. **内存泄漏**：未正确释放MPI资源
4. **通信不匹配**：发送和接收操作不匹配

**最佳实践**：
1. **总是检查返回值**：
   ```c
   int error = MPI_Send(data, count, MPI_INT, dest, tag, comm);
   if (error != MPI_SUCCESS) {
       printf("MPI error: %d\n", error);
       MPI_Abort(comm, error);
   }
   ```

2. **使用非阻塞通信避免死锁**：
   ```c
   MPI_Request req;
   MPI_Isend(buffer, count, MPI_INT, dest, tag, comm, &req);
   // ... 其他计算 ...
   MPI_Wait(&req, MPI_STATUS_IGNORE);
   ```

3. **合理使用通信器**：
   ```c
   // 为不同模块创建独立通信器
   MPI_Comm module_comm;
   MPI_Comm_dup(MPI_COMM_WORLD, &module_comm);
   ```

4. **进程同步**：
   ```c
   // 在关键点进行同步
   MPI_Barrier(MPI_COMM_WORLD);
   ```

### 10.2 基本通信操作

#### 10.2.1 初始化与环境管理
**MPI环境初始化**：
```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // 1. MPI初始化：必须在所有MPI调用前执行
    int provided;
    MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);

    // 2. 获取进程信息
    int world_size, world_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // 3. 获取进程名称（主机名）
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    printf("Hello from process %d of %d on %s\n",
           world_rank, world_size, processor_name);

    // 4. MPI终止：必须在最后调用
    MPI_Finalize();
    return 0;
}
```

**线程支持级别**：
```c
// MPI线程支持级别
MPI_Init_thread(argc, &argv, MPI_THREAD_SINGLE, &provided);    // 单线程
MPI_Init_thread(argc, &argv, MPI_THREAD_FUNNELED, &provided);  // 多线程，仅主线程MPI调用
MPI_Init_thread(argc, &argv, MPI_THREAD_SERIALIZED, &provided); // 多线程，序列化MPI调用
MPI_Init_thread(argc, &argv, MPI_THREAD_MULTIPLE, &provided);  // 完全多线程支持
```

#### 10.2.2 点对点通信基础
**阻塞通信操作**：
```c
// 基本发送操作
int MPI_Send(void* buf, int count, MPI_Datatype datatype,
             int dest, int tag, MPI_Comm comm);

// 基本接收操作
int MPI_Recv(void* buf, int count, MPI_Datatype datatype,
             int source, int tag, MPI_Comm comm, MPI_Status* status);

// 状态信息查询
MPI_Status status;
MPI_Recv(buffer, count, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, comm, &status);
int received_count;
MPI_Get_count(&status, MPI_INT, &received_count);
printf("Received %d integers from process %d with tag %d\n",
       received_count, status.MPI_SOURCE, status.MPI_TAG);
```

**通信模式**：
```c
// 标准模式（默认）
MPI_Send(buffer, count, MPI_INT, dest, tag, comm);

// 缓冲模式
MPI_Buffer_attach(buffer, size);
MPI_Bsend(buffer, count, MPI_INT, dest, tag, comm);

// 同步模式
MPI_Ssend(buffer, count, MPI_INT, dest, tag, comm);

// 就绪模式（需要接收方已调用MPI_Recv）
MPI_Rsend(buffer, count, MPI_INT, dest, tag, comm);
```

#### 10.2.3 非阻塞通信
**基本非阻塞操作**：
```c
// 非阻塞发送
MPI_Request send_request;
int MPI_Isend(void* buf, int count, MPI_Datatype datatype,
              int dest, int tag, MPI_Comm comm, MPI_Request* request);

// 非阻塞接收
MPI_Request recv_request;
int MPI_Irecv(void* buf, int count, MPI_Datatype datatype,
              int source, int tag, MPI_Comm comm, MPI_Request* request);

// 等待单个请求完成
int MPI_Wait(MPI_Request* request, MPI_Status* status);

// 等待多个请求完成
int MPI_Waitall(int count, MPI_Request array_of_requests[],
                MPI_Status array_of_statuses[]);
```

**非阻塞通信示例**：
```c
void nonblocking_communication_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_data = rank * 10;
    int recv_data;
    MPI_Request send_req, recv_req;
    MPI_Status send_status, recv_status;

    // 确定通信伙伴
    int left = (rank - 1 + size) % size;
    int right = (rank + 1) % size;

    // 启动非阻塞发送和接收
    MPI_Isend(&send_data, 1, MPI_INT, right, 0, MPI_COMM_WORLD, &send_req);
    MPI_Irecv(&recv_data, 1, MPI_INT, left, 0, MPI_COMM_WORLD, &recv_req);

    // 执行其他计算工作
    double computation_result = 0.0;
    for (int i = 0; i < 1000000; i++) {
        computation_result += sin(i) * cos(i);
    }

    // 等待通信完成
    MPI_Wait(&send_req, &send_status);
    MPI_Wait(&recv_req, &recv_status);

    printf("Process %d: sent %d, received %d, computation: %f\n",
           rank, send_data, recv_data, computation_result);
}
```

#### 10.2.4 通信完成检查
**测试操作**：
```c
// 测试单个请求是否完成
int flag;
MPI_Status status;
MPI_Test(&request, &flag, &status);

if (flag) {
    printf("Request completed\n");
} else {
    printf("Request still pending\n");
}

// 测试多个请求
int outcount;
int indices[10];
MPI_Request requests[10];
MPI_Status statuses[10];

MPI_Testsome(10, requests, &outcount, indices, statuses);
printf("Completed %d requests\n", outcount);

// 等待任意请求完成
int index;
MPI_Status any_status;
MPI_Waitsome(10, requests, &outcount, indices, statuses);
printf("Any request completed: index %d\n", indices[0]);
```

#### 10.2.5 数据类型系统
**预定义数据类型**：
```c
// C/C++基本类型
MPI_CHAR, MPI_SHORT, MPI_INT, MPI_LONG
MPI_UNSIGNED_CHAR, MPI_UNSIGNED_SHORT, MPI_UNSIGNED, MPI_UNSIGNED_LONG
MPI_FLOAT, MPI_DOUBLE, MPI_LONG_DOUBLE
MPI_BYTE, MPI_PACKED

// Fortran类型（通过mpi.h也可以使用）
MPI_INTEGER, MPI_REAL, MPI_DOUBLE_PRECISION
MPI_LOGICAL, MPI_CHARACTER, MPI_COMPLEX
```

**派生数据类型**：
```c
// 创建结构体类型
typedef struct {
    int id;
    double value;
    char name[20];
} DataPacket;

// 方法1：使用结构体类型
int blockcounts[3] = {1, 1, 20};
MPI_Datatype types[3] = {MPI_INT, MPI_DOUBLE, MPI_CHAR};
MPI_Aint displs[3];

DataPacket packet;
MPI_Get_address(&packet.id, &displs[0]);
MPI_Get_address(&packet.value, &displs[1]);
MPI_Get_address(&packet.name, &displs[2]);

// 相对位移计算
for (int i = 1; i < 3; i++) {
    displs[i] = displs[i] - displs[0];
}
displs[0] = 0;

MPI_Datatype packet_type;
MPI_Type_create_struct(3, blockcounts, displs, types, &packet_type);
MPI_Type_commit(&packet_type);

// 使用派生类型进行通信
MPI_Send(&packet, 1, packet_type, dest, tag, comm);
```

**向量类型**：
```c
// 创建向量类型（用于矩阵列）
int count = 4;        // 向量数量
int blocklength = 1;  // 每个向量中的元素数
int stride = 4;       // 向量间的步长（列间距）
MPI_Datatype column_type;

MPI_Type_vector(count, blocklength, stride, MPI_DOUBLE, &column_type);
MPI_Type_commit(&column_type);

// 发送矩阵的第j列
double matrix[16];  // 4x4矩阵
MPI_Send(&matrix[j], 1, column_type, dest, tag, comm);
```

#### 10.2.6 通信优化技术
**通信重叠计算**：
```c
void communication_overlap_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 准备数据
    double *send_buf = malloc(BUFFER_SIZE * sizeof(double));
    double *recv_buf = malloc(BUFFER_SIZE * sizeof(double));
    double *compute_buf = malloc(COMPUTE_SIZE * sizeof(double));

    // 初始化缓冲区
    for (int i = 0; i < BUFFER_SIZE; i++) {
        send_buf[i] = rank + i * 0.1;
    }

    MPI_Request send_req, recv_req;

    // 1. 启动通信
    MPI_Isend(send_buf, BUFFER_SIZE, MPI_DOUBLE, (rank+1)%size, 0,
              MPI_COMM_WORLD, &send_req);
    MPI_Irecv(recv_buf, BUFFER_SIZE, MPI_DOUBLE, (rank-1+size)%size, 0,
              MPI_COMM_WORLD, &recv_req);

    // 2. 执行计算（与通信并行）
    for (int i = 0; i < COMPUTE_SIZE; i++) {
        compute_buf[i] = sin(compute_buf[i]) * cos(compute_buf[i]);
    }

    // 3. 等待通信完成
    MPI_Wait(&send_req, MPI_STATUS_IGNORE);
    MPI_Wait(&recv_req, MPI_STATUS_IGNORE);

    // 4. 处理接收到的数据
    double result = 0.0;
    for (int i = 0; i < BUFFER_SIZE; i++) {
        result += recv_buf[i] * compute_buf[i % COMPUTE_SIZE];
    }

    printf("Process %d: computation result = %f\n", rank, result);

    // 清理
    free(send_buf); free(recv_buf); free(compute_buf);
}
```

**流水线通信**：
```c
void pipeline_communication(int rank, int size) {
    const int PIPELINE_STAGES = 4;
    const int DATA_CHUNKS = 10;

    MPI_Request requests[PIPELINE_STAGES * 2];
    int req_count = 0;

    for (int chunk = 0; chunk < DATA_CHUNKS; chunk++) {
        // 1. 准备当前块数据
        double data[DATA_SIZE];
        for (int i = 0; i < DATA_SIZE; i++) {
            data[i] = chunk * 100 + i;
        }

        // 2. 启动发送（如果非最后一阶段）
        if (rank < size - 1) {
            MPI_Isend(data, DATA_SIZE, MPI_DOUBLE, rank + 1,
                      chunk, MPI_COMM_WORLD, &requests[req_count++]);
        }

        // 3. 启动接收（如果非第一阶段）
        if (rank > 0) {
            double recv_data[DATA_SIZE];
            MPI_Irecv(recv_data, DATA_SIZE, MPI_DOUBLE, rank - 1,
                      chunk, MPI_COMM_WORLD, &requests[req_count++]);

            // 4. 处理接收到的数据
            // ... 计算处理 ...
        }

        // 5. 等待当前流水线阶段完成（可选，取决于流水线深度）
        if (req_count >= PIPELINE_STAGES * 2) {
            MPI_Waitall(PIPELINE_STAGES * 2, requests, MPI_STATUSES_IGNORE);
            req_count = 0;
        }
    }

    // 等待所有剩余通信完成
    if (req_count > 0) {
        MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);
    }
}
```

#### 10.2.7 通信模式选择策略
**选择指南**：
```c
// 根据场景选择合适的通信模式

void communication_strategy_guide() {
    // 1. 小数据量、简单通信
    // 使用：阻塞点对点通信
    MPI_Send(data, 100, MPI_INT, dest, tag, comm);
    MPI_Recv(buffer, 100, MPI_INT, source, tag, comm, &status);

    // 2. 大数据量、需要重叠计算
    // 使用：非阻塞通信 + 计算重叠
    MPI_Isend(large_data, size, MPI_DOUBLE, dest, tag, comm, &req);
    // ... 执行计算 ...
    MPI_Wait(&req, &status);

    // 3. 多对多通信
    // 使用：集体通信
    MPI_Alltoall(sendbuf, sendcount, sendtype,
                 recvbuf, recvcount, recvtype, comm);

    // 4. 树形通信模式
    // 使用：非阻塞 + 异步
    MPI_Ibcast(root_data, count, type, 0, comm, &req);
    // ... 其他操作 ...
    MPI_Wait(&req, &status);

    // 5. 流水线处理
    // 使用：多个非阻塞请求
    for (int i = 0; i < num_stages; i++) {
        MPI_Isend(stage_data[i], size, type, next_rank, i, comm, &requests[i]);
    }
    MPI_Waitall(num_stages, requests, statuses);
}
```

#### 10.2.8 实际应用示例：并行矩阵乘法
```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#define MATRIX_SIZE 1000
#define BLOCK_SIZE 250

void parallel_matrix_multiply() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 1. 数据分配
    int rows_per_process = MATRIX_SIZE / size;
    double *local_A = malloc(rows_per_process * MATRIX_SIZE * sizeof(double));
    double *local_B = malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
    double *local_C = malloc(rows_per_process * MATRIX_SIZE * sizeof(double));

    // 2. 初始化矩阵A（仅主进程）
    double *global_A = NULL;
    if (rank == 0) {
        global_A = malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
        for (int i = 0; i < MATRIX_SIZE * MATRIX_SIZE; i++) {
            global_A[i] = (double)(rand() % 100) / 10.0;
        }
    }

    // 3. 分发矩阵A的行块
    MPI_Scatter(global_A, rows_per_process * MATRIX_SIZE, MPI_DOUBLE,
                local_A, rows_per_process * MATRIX_SIZE, MPI_DOUBLE,
                0, MPI_COMM_WORLD);

    // 4. 广播矩阵B（所有进程都需要）
    if (rank == 0) {
        for (int i = 0; i < MATRIX_SIZE * MATRIX_SIZE; i++) {
            local_B[i] = (double)(rand() % 100) / 10.0;
        }
    }
    MPI_Bcast(local_B, MATRIX_SIZE * MATRIX_SIZE, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    // 5. 并行计算矩阵乘法
    double start_time = MPI_Wtime();

    for (int i = 0; i < rows_per_process; i++) {
        for (int j = 0; j < MATRIX_SIZE; j++) {
            double sum = 0.0;
            for (int k = 0; k < MATRIX_SIZE; k++) {
                sum += local_A[i * MATRIX_SIZE + k] * local_B[k * MATRIX_SIZE + j];
            }
            local_C[i * MATRIX_SIZE + j] = sum;
        }
    }

    double compute_time = MPI_Wtime() - start_time;

    // 6. 收集结果
    double *global_C = NULL;
    if (rank == 0) {
        global_C = malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
    }

    MPI_Gather(local_C, rows_per_process * MATRIX_SIZE, MPI_DOUBLE,
               global_C, rows_per_process * MATRIX_SIZE, MPI_DOUBLE,
               0, MPI_COMM_WORLD);

    // 7. 性能统计（仅主进程）
    if (rank == 0) {
        double total_time = MPI_Wtime() - start_time;
        printf("Matrix multiplication completed:\n");
        printf("  Size: %dx%d\n", MATRIX_SIZE, MATRIX_SIZE);
        printf("  Processes: %d\n", size);
        printf("  Computation time: %.3f seconds\n", compute_time);
        printf("  Total time: %.3f seconds\n", total_time);

        // 验证结果（小规模测试）
        if (MATRIX_SIZE <= 10) {
            printf("Result matrix (first 3x3):\n");
            for (int i = 0; i < 3; i++) {
                for (int j = 0; j < 3; j++) {
                    printf("%.2f ", global_C[i * MATRIX_SIZE + j]);
                }
                printf("\n");
            }
        }
    }

    // 清理内存
    free(local_A); free(local_B); free(local_C);
    if (rank == 0) {
        free(global_A); free(global_C);
    }
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    parallel_matrix_multiply();
    MPI_Finalize();
    return 0;
}
```

#### 10.2.9 调试和性能分析
**通信调试技巧**：
```c
// 1. 添加调试输出
#ifdef DEBUG
    printf("[%d] Sending %d elements to %d\n", rank, count, dest);
    fflush(stdout);
#endif

// 2. 使用MPI_Barrier进行同步调试
MPI_Barrier(MPI_COMM_WORLD);

// 3. 检查通信状态
MPI_Status status;
MPI_Recv(buffer, count, MPI_INT, source, tag, comm, &status);
if (status.MPI_ERROR != MPI_SUCCESS) {
    printf("Error in receive operation\n");
}

// 4. 性能计时
double start_time = MPI_Wtime();
// ... 通信操作 ...
double elapsed_time = MPI_Wtime() - start_time;
printf("Communication time: %f seconds\n", elapsed_time);
```

**常见性能问题**：
1. **通信开销过大**：减少通信频率，增加每次通信的数据量
2. **负载不均衡**：合理分配计算任务
3. **死锁**：使用非阻塞通信，避免循环等待
4. **内存使用**：及时释放不必要的缓冲区

**编译和运行**：
```bash
# 编译
mpicc -O2 -o mpi_example mpi_example.c -lm

# 运行（4个进程）
mpirun -np 4 ./mpi_example

# 在集群上运行
mpirun -np 16 -hostfile hosts ./mpi_example

# 使用性能分析工具
mpiP -o profile.out mpirun -np 4 ./mpi_example
```

### 10.3 集体通信操作

#### 10.3.1 集体通信基础概念
**集体通信特点**：
- 所有进程必须参与相同的集体操作
- 操作在通信器内进行同步
- 提供高效的批量通信机制
- 减少显式同步的需要

**集体通信分类**：
- **数据移动操作**：广播、收集、分发
- **数据组合操作**：规约、扫描
- **同步操作**：屏障同步
- **数据重排操作**：全交换、全收集

#### 10.3.2 广播操作(Broadcast)
**MPI_Bcast基础**：
```c
// 广播函数原型
int MPI_Bcast(void* buffer, int count, MPI_Datatype datatype,
              int root, MPI_Comm comm);

// 基本广播示例
void basic_bcast_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double data;
    if (rank == 0) {
        data = 3.14159;  // 根进程初始化数据
    }

    // 所有进程参与广播，根进程发送，其他进程接收
    MPI_Bcast(&data, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    printf("Process %d received data: %f\n", rank, data);
}
```

**广播优化策略**：
```c
// 1. 树形广播（优化大规模通信）
void tree_bcast_example(double* data, int count, MPI_Datatype datatype,
                        int root, MPI_Comm comm) {
    int rank, size;
    MPI_Comm_rank(comm, &rank);
    MPI_Comm_size(comm, &size);

    if (rank == root) {
        // 根进程向子节点发送数据
        for (int i = 1; i < size; i *= 2) {
            if (i < size) {
                MPI_Send(data, count, datatype, i, 0, comm);
            }
        }
    } else {
        // 非根进程接收数据并转发
        MPI_Recv(data, count, datatype, MPI_ANY_SOURCE, 0, comm, MPI_STATUS_IGNORE);

        // 转发给子节点
        for (int i = 2 * rank + 1; i < size; i += 2 * rank + 2) {
            MPI_Send(data, count, datatype, i, 0, comm);
        }
    }
}

// 2. 使用MPI_Bcast的高级特性
void advanced_bcast_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 广播结构体数据
    typedef struct {
        int id;
        double value;
        char name[50];
    } DataPacket;

    DataPacket packet;
    if (rank == 0) {
        packet.id = 100;
        packet.value = 42.5;
        strcpy(packet.name, "Master Data");
    }

    // 需要先创建派生数据类型
    MPI_Datatype packet_type;
    create_packet_datatype(&packet_type);  // 假设已定义此函数

    MPI_Bcast(&packet, 1, packet_type, 0, MPI_COMM_WORLD);

    printf("Process %d: id=%d, value=%f, name=%s\n",
           rank, packet.id, packet.value, packet.name);

    MPI_Type_free(&packet_type);
}
```

#### 10.3.3 规约操作(Reduction)
**基本规约操作**：
```c
// 规约函数原型
int MPI_Reduce(const void* sendbuf, void* recvbuf, int count,
               MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);

// 规约操作示例
void reduce_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double local_sum = rank * 10.0;
    double global_sum;

    // 各进程贡献本地数据，根进程收集全局结果
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Global sum: %f\n", global_sum);
        // 验证：0+10+20+...+(size-1)*10 = 10*(0+1+2+...+size-1) = 10*size*(size-1)/2
        double expected = 10.0 * size * (size - 1) / 2.0;
        printf("Expected: %f, Match: %s\n", expected,
               (fabs(global_sum - expected) < 1e-10) ? "YES" : "NO");
    }
}

// 使用不同的规约操作
void various_reduce_operations() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_value = rank + 1;
    int global_result;

    // 求和
    MPI_Reduce(&local_value, &global_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
    if (rank == 0) printf("Sum: %d\n", global_result);

    // 求最大值
    MPI_Reduce(&local_value, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);
    if (rank == 0) printf("Max: %d\n", global_result);

    // 求最小值
    MPI_Reduce(&local_value, &global_result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);
    if (rank == 0) printf("Min: %d\n", global_result);

    // 逻辑与
    int local_bool = (rank % 2 == 0) ? 1 : 0;
    int global_bool;
    MPI_Reduce(&local_bool, &global_bool, 1, MPI_INT, MPI_LAND, 0, MPI_COMM_WORLD);
    if (rank == 0) printf("All even: %s\n", global_bool ? "YES" : "NO");
}
```

**自定义规约操作**：
```c
// 自定义规约函数
void custom_reduce_function(void* invec, void* inoutvec, int* len, MPI_Datatype* datatype) {
    double* in = (double*)invec;
    double* inout = (double*)inoutvec;

    for (int i = 0; i < *len; i++) {
        // 自定义操作：计算加权平均
        inout[i] = (inout[i] + in[i]) / 2.0;
    }
}

void custom_reduce_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 注册自定义规约操作
    MPI_Op custom_op;
    MPI_Op_create(custom_reduce_function, 1, &custom_op);

    double local_data = rank * 2.5;
    double result;

    MPI_Reduce(&local_data, &result, 1, MPI_DOUBLE, custom_op, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Custom reduce result: %f\n", result);
    }

    MPI_Op_free(&custom_op);
}
```

#### 10.3.4 全局规约(Allreduce)
**MPI_Allreduce特性**：
- 所有进程都获得规约结果
- 无需指定根进程
- 更适合对称算法

```c
void allreduce_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    double local_data = rank * 1.5;
    double global_result;

    // 所有进程都获得全局规约结果
    MPI_Allreduce(&local_data, &global_result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d: local=%f, global=%f\n", rank, local_data, global_result);

    // 验证所有进程结果相同
    double expected_sum = 1.5 * size * (size - 1) / 2.0;
    if (fabs(global_result - expected_sum) > 1e-10) {
        printf("ERROR: Process %d got wrong result!\n", rank);
    }
}

// 并行计算π的示例
void parallel_pi_calculation(int n) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_n = n / size;
    int local_start = rank * local_n;
    int local_end = (rank == size - 1) ? n : local_start + local_n;

    double local_sum = 0.0;
    for (int i = local_start; i < local_end; i++) {
        double x = (i + 0.5) / n;
        local_sum += 4.0 / (1.0 + x * x);
    }

    double pi;
    MPI_Allreduce(&local_sum, &pi, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
    pi /= n;

    if (rank == 0) {
        printf("π approximation: %f\n", pi);
        printf("Error: %e\n", fabs(pi - 3.14159265358979323846));
    }
}
```

#### 10.3.5 数据分发(Scatter/Gather)
**Scatter操作**：
```c
// Scatter函数原型
int MPI_Scatter(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype,
                int root, MPI_Comm comm);

void scatter_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int data_size = size * 100;
    int *send_data = NULL;
    int *recv_data = malloc(100 * sizeof(int));

    if (rank == 0) {
        // 根进程准备数据
        send_data = malloc(data_size * sizeof(int));
        for (int i = 0; i < data_size; i++) {
            send_data[i] = i;
        }
    }

    // 根进程将数据分发给所有进程
    MPI_Scatter(send_data, 100, MPI_INT,
                recv_data, 100, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received:", rank);
    for (int i = 0; i < 10; i++) {  // 只打印前10个元素
        printf(" %d", recv_data[i]);
    }
    printf("...\n");

    free(recv_data);
    if (rank == 0) {
        free(send_data);
    }
}

// 不均匀数据分发
void uneven_scatter_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int *send_data = NULL;
    int *recv_data;
    int *counts, *displs;

    if (rank == 0) {
        // 计算每个进程的数据量
        counts = malloc(size * sizeof(int));
        displs = malloc(size * sizeof(int));

        int total_size = 1000;
        int base_size = total_size / size;
        int remainder = total_size % size;

        int offset = 0;
        for (int i = 0; i < size; i++) {
            counts[i] = base_size + (i < remainder ? 1 : 0);
            displs[i] = offset;
            offset += counts[i];
        }

        send_data = malloc(total_size * sizeof(int));
        for (int i = 0; i < total_size; i++) {
            send_data[i] = i;
        }
    } else {
        counts = malloc(size * sizeof(int));
        displs = malloc(size * sizeof(int));
    }

    // 广播 counts 和 displs
    MPI_Bcast(counts, size, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Bcast(displs, size, MPI_INT, 0, MPI_COMM_WORLD);

    // 分配接收缓冲区
    recv_data = malloc(counts[rank] * sizeof(int));

    // 不均匀分发
    MPI_Scatterv(send_data, counts, displs, MPI_INT,
                 recv_data, counts[rank], MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received %d elements\n", rank, counts[rank]);

    free(recv_data);
    free(counts);
    free(displs);
    if (rank == 0) {
        free(send_data);
    }
}
```

**Gather操作**：
```c
// Gather函数原型
int MPI_Gather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
               void* recvbuf, int recvcount, MPI_Datatype recvtype,
               int root, MPI_Comm comm);

void gather_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_data[100];
    for (int i = 0; i < 100; i++) {
        local_data[i] = rank * 100 + i;
    }

    int *global_data = NULL;
    if (rank == 0) {
        global_data = malloc(size * 100 * sizeof(int));
    }

    // 收集所有进程的数据到根进程
    MPI_Gather(local_data, 100, MPI_INT,
               global_data, 100, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Collected data:");
        for (int i = 0; i < 10; i++) {  // 只打印前10个元素
            printf(" %d", global_data[i]);
        }
        printf("...\n");
        free(global_data);
    }
}

// 不均匀数据收集
void uneven_gather_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 每个进程生成不同大小的数据
    int local_size = 50 + rank * 10;
    int *local_data = malloc(local_size * sizeof(int));
    for (int i = 0; i < local_size; i++) {
        local_data[i] = rank * 1000 + i;
    }

    int *global_data = NULL;
    int *counts, *displs;

    if (rank == 0) {
        counts = malloc(size * sizeof(int));
        displs = malloc(size * sizeof(int));
    } else {
        counts = malloc(size * sizeof(int));
        displs = malloc(size * sizeof(int));
    }

    // 收集每个进程的数据大小
    MPI_Gather(&local_size, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        // 计算位移
        displs[0] = 0;
        for (int i = 1; i < size; i++) {
            displs[i] = displs[i-1] + counts[i-1];
        }

        // 分配全局缓冲区
        int total_size = displs[size-1] + counts[size-1];
        global_data = malloc(total_size * sizeof(int));
    }

    // 不均匀收集
    MPI_Gatherv(local_data, local_size, MPI_INT,
                global_data, counts, displs, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Uneven gather - total elements: %d\n", displs[size-1] + counts[size-1]);
        free(global_data);
        free(counts);
        free(displs);
    } else {
        free(counts);
        free(displs);
    }

    free(local_data);
}
```

#### 10.3.6 全交换(Alltoall)
**MPI_Alltoall基础**：
```c
// Alltoall函数原型
int MPI_Alltoall(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                 void* recvbuf, int recvcount, MPI_Datatype recvtype,
                 MPI_Comm comm);

void alltoall_basic_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_data[size];
    int recv_data[size];

    // 每个进程向所有其他进程发送不同的数据
    for (int i = 0; i < size; i++) {
        send_data[i] = rank * 100 + i;
    }

    // 全交换：进程i发送给进程j的数据，进程j接收
    MPI_Alltoall(send_data, 1, MPI_INT,
                 recv_data, 1, MPI_INT, MPI_COMM_WORLD);

    printf("Process %d sent:", rank);
    for (int i = 0; i < size; i++) {
        printf(" %d", send_data[i]);
    }
    printf("\n");

    printf("Process %d received:", rank);
    for (int i = 0; i < size; i++) {
        printf(" %d", recv_data[i]);
    }
    printf("\n");
}

// 矩阵转置示例
void matrix_transpose_example(int matrix_size) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int rows_per_proc = matrix_size / size;
    double *local_matrix = malloc(rows_per_proc * matrix_size * sizeof(double));
    double *transposed_matrix = malloc(rows_per_proc * matrix_size * sizeof(double));

    // 初始化局部矩阵块
    for (int i = 0; i < rows_per_proc; i++) {
        for (int j = 0; j < matrix_size; j++) {
            local_matrix[i * matrix_size + j] = rank * 1000 + i * matrix_size + j;
        }
    }

    // 使用Alltoall进行矩阵转置
    MPI_Alltoall(local_matrix, rows_per_proc, MPI_DOUBLE,
                 transposed_matrix, rows_per_proc, MPI_DOUBLE, MPI_COMM_WORLD);

    printf("Process %d - Original matrix block (first 3x3):\n", rank);
    for (int i = 0; i < 3 && i < rows_per_proc; i++) {
        for (int j = 0; j < 3 && j < matrix_size; j++) {
            printf("%6.0f ", local_matrix[i * matrix_size + j]);
        }
        printf("\n");
    }

    printf("Process %d - Transposed matrix block (first 3x3):\n", rank);
    for (int i = 0; i < 3 && i < rows_per_proc; i++) {
        for (int j = 0; j < 3 && j < matrix_size; j++) {
            printf("%6.0f ", transposed_matrix[i * matrix_size + j]);
        }
        printf("\n");
    }

    free(local_matrix);
    free(transposed_matrix);
}
```

#### 10.3.7 扫描操作(Scan)
**MPI_Scan基础**：
```c
// Scan函数原型
int MPI_Scan(const void* sendbuf, void* recvbuf, int count,
             MPI_Datatype datatype, MPI_Op op, MPI_Comm comm);

void scan_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_value = rank + 1;
    int prefix_sum;

    // 前缀求和：每个进程得到前面所有进程的累加和
    MPI_Scan(&local_value, &prefix_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d: local=%d, prefix_sum=%d\n", rank, local_value, prefix_sum);

    // 验证：进程i的前缀和应该是1+2+...+(i+1) = (i+1)(i+2)/2
    int expected = (rank + 1) * (rank + 2) / 2;
    if (prefix_sum != expected) {
        printf("ERROR: Process %d expected %d, got %d\n", rank, expected, prefix_sum);
    }
}

// 并行前缀和应用：数组前缀和计算
void parallel_prefix_sum(double* array, int n) {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_n = n / size;
    int local_start = rank * local_n;
    int local_end = (rank == size - 1) ? n : local_start + local_n;

    // 局部前缀和
    double local_prefix = 0.0;
    for (int i = local_start; i < local_end; i++) {
        local_prefix += array[i];
    }

    // 全局前缀和
    double global_prefix;
    MPI_Scan(&local_prefix, &global_prefix, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

    // 调整局部前缀和
    double local_sum = 0.0;
    for (int i = local_start; i < local_end; i++) {
        local_sum += array[i];
        array[i] = (i == local_start) ? global_prefix : array[i-1] + array[i];
    }

    // 最后一个进程需要调整
    if (rank == size - 1) {
        for (int i = local_start; i < local_end; i++) {
            array[i] += local_sum - local_prefix;
        }
    }
}
```

#### 10.3.8 集体通信优化
**通信模式选择**：
```c
// 根据数据大小选择通信策略
void optimized_collective_communication() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int LARGE_THRESHOLD = 1000000;  // 1MB阈值
    int data_size = 1000000;

    if (data_size < LARGE_THRESHOLD) {
        // 小数据：使用标准集体通信
        if (rank == 0) {
            double *data = malloc(data_size * sizeof(double));
            // ... 初始化数据 ...
            MPI_Bcast(data, data_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
            free(data);
        } else {
            double *data = malloc(data_size * sizeof(double));
            MPI_Bcast(data, data_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
            // ... 处理数据 ...
            free(data);
        }
    } else {
        // 大数据：使用分块通信
        int chunk_size = data_size / 10;  // 分10块
        int num_chunks = (data_size + chunk_size - 1) / chunk_size;

        for (int i = 0; i < num_chunks; i++) {
            int current_size = (i == num_chunks - 1) ?
                (data_size - i * chunk_size) : chunk_size;

            if (rank == 0) {
                double *chunk = malloc(current_size * sizeof(double));
                // ... 准备当前块数据 ...
                MPI_Bcast(chunk, current_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
                free(chunk);
            } else {
                double *chunk = malloc(current_size * sizeof(double));
                MPI_Bcast(chunk, current_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
                // ... 处理当前块 ...
                free(chunk);
            }
        }
    }
}

// 通信重叠示例
void overlapping_communication_computation() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int MATRIX_SIZE = 1000;
    const int BLOCKS = 4;

    double *matrix = malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
    double *result = malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));

    // 初始化矩阵
    for (int i = 0; i < MATRIX_SIZE * MATRIX_SIZE; i++) {
        matrix[i] = (double)(rand() % 100) / 10.0;
    }

    MPI_Request requests[BLOCKS];
    int req_count = 0;

    // 分块处理，重叠通信和计算
    for (int block = 0; block < BLOCKS; block++) {
        int start_row = block * MATRIX_SIZE / BLOCKS;
        int end_row = (block + 1) * MATRIX_SIZE / BLOCKS;

        // 启动广播当前块
        MPI_Ibcast(&matrix[start_row * MATRIX_SIZE],
                   (end_row - start_row) * MATRIX_SIZE,
                   MPI_DOUBLE, 0, MPI_COMM_WORLD, &requests[req_count++]);

        // 等待前一个块的通信完成（如果有）
        if (block > 0) {
            MPI_Wait(&requests[block-1], MPI_STATUS_IGNORE);
        }

        // 计算当前块（与下一个块的通信重叠）
        for (int i = start_row; i < end_row; i++) {
            for (int j = 0; j < MATRIX_SIZE; j++) {
                double sum = 0.0;
                for (int k = 0; k < MATRIX_SIZE; k++) {
                    sum += matrix[i * MATRIX_SIZE + k] * matrix[k * MATRIX_SIZE + j];
                }
                result[i * MATRIX_SIZE + j] = sum;
            }
        }
    }

    // 等待最后一个块的通信完成
    if (req_count > 0) {
        MPI_Wait(&requests[req_count-1], MPI_STATUS_IGNORE);
    }

    free(matrix);
    free(result);
}
```

#### 10.3.9 实际应用示例：并行快速排序
```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

void parallel_quicksort(double* data, int n, int depth, MPI_Comm comm) {
    int rank, size;
    MPI_Comm_rank(comm, &rank);
    MPI_Comm_size(comm, &size);

    if (n <= 1 || depth >= size) {
        // 基本情况：使用串行快速排序
        qsort(data, n, sizeof(double), compare_doubles);
        return;
    }

    // 1. 选择枢轴（使用中位数）
    double pivot;
    if (rank == 0) {
        // 收集所有进程的第一个元素
        double *first_elements = malloc(size * sizeof(double));
        MPI_Gather(&data[0], 1, MPI_DOUBLE, first_elements, 1, MPI_DOUBLE, 0, comm);

        // 选择中位数作为枢轴
        qsort(first_elements, size, sizeof(double), compare_doubles);
        pivot = first_elements[size / 2];
        free(first_elements);
    }

    // 2. 广播枢轴
    MPI_Bcast(&pivot, 1, MPI_DOUBLE, 0, comm);

    // 3. 分区：将数据分为小于和大于枢轴的两部分
    int *send_counts = malloc(size * sizeof(int));
    int *displs = malloc(size * sizeof(int));

    // 统计每个进程需要发送到其他进程的数据量
    for (int i = 0; i < size; i++) {
        send_counts[i] = 0;
    }

    for (int i = 0; i < n; i++) {
        int target_rank = (data[i] < pivot) ? 0 : 1;
        send_counts[target_rank]++;
    }

    // 计算接收数据的位移
    MPI_Alltoall(send_counts, 1, MPI_INT, displs, 1, MPI_INT, comm);

    int total_recv = 0;
    for (int i = 0; i < size; i++) {
        total_recv += displs[i];
    }

    double *recv_data = malloc(total_recv * sizeof(double));
    double *send_data = malloc(n * sizeof(double));

    // 准备发送数据
    int send_index = 0;
    for (int i = 0; i < n; i++) {
        if (data[i] < pivot) {
            send_data[send_index++] = data[i];
        }
    }

    // 发送数据
    MPI_Alltoallv(send_data, send_counts, NULL, MPI_DOUBLE,
                  recv_data, displs, NULL, MPI_DOUBLE, comm);

    // 4. 递归排序
    free(data);
    data = recv_data;
    n = total_recv;

    // 重新分区进程组
    int new_size = size / 2;
    int new_rank = rank % new_size;

    MPI_Comm new_comm;
    MPI_Comm_split(comm, new_rank, rank, &new_comm);

    parallel_quicksort(data, n, depth + 1, new_comm);
    MPI_Comm_free(&new_comm);

    free(send_data);
    free(send_counts);
    free(displs);
}

int compare_doubles(const void* a, const void* b) {
    double da = *(const double*)a;
    double db = *(const double*)b;
    return (da > db) - (da < db);
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int TOTAL_ELEMENTS = 1000000;
    const int LOCAL_ELEMENTS = TOTAL_ELEMENTS / size;

    // 生成随机数据
    double *local_data = malloc(LOCAL_ELEMENTS * sizeof(double));
    srand(time(NULL) + rank);

    for (int i = 0; i < LOCAL_ELEMENTS; i++) {
        local_data[i] = (double)rand() / RAND_MAX * 1000.0;
    }

    double start_time = MPI_Wtime();

    // 执行并行快速排序
    parallel_quicksort(local_data, LOCAL_ELEMENTS, 0, MPI_COMM_WORLD);

    double sort_time = MPI_Wtime() - start_time;

    // 验证排序结果
    int sorted = 1;
    for (int i = 1; i < LOCAL_ELEMENTS; i++) {
        if (local_data[i] < local_data[i-1]) {
            sorted = 0;
            break;
        }
    }

    printf("Process %d: sorted=%d, time=%.3f\n", rank, sorted, sort_time);

    free(local_data);
    MPI_Finalize();
    return 0;
}
```

#### 10.3.10 集体通信性能分析
**性能测试框架**：
```c
void collective_communication_benchmark() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int MAX_SIZE = 10000000;
    const int NUM_TESTS = 10;

    if (rank == 0) {
        printf("Collective Communication Benchmark\n");
        printf("==================================\n");
        printf("Size\tBcast\tReduce\tAllreduce\tGather\tAlltoall\n");
    }

    for (int size_test = 1000; size_test <= MAX_SIZE; size_test *= 10) {
        double *data = malloc(size_test * sizeof(double));
        double *result = malloc(size_test * sizeof(double));

        // 初始化数据
        for (int i = 0; i < size_test; i++) {
            data[i] = (double)rand() / RAND_MAX;
        }

        // 测试广播
        double bcast_time = 0.0;
        for (int i = 0; i < NUM_TESTS; i++) {
            MPI_Barrier(MPI_COMM_WORLD);
            double start = MPI_Wtime();
            MPI_Bcast(data, size_test, MPI_DOUBLE, 0, MPI_COMM_WORLD);
            double end = MPI_Wtime();
            bcast_time += (end - start);
        }
        bcast_time /= NUM_TESTS;

        // 测试规约
        double reduce_time = 0.0;
        for (int i = 0; i < NUM_TESTS; i++) {
            MPI_Barrier(MPI_COMM_WORLD);
            double start = MPI_Wtime();
            MPI_Reduce(data, result, size_test, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
            double end = MPI_Wtime();
            reduce_time += (end - start);
        }
        reduce_time /= NUM_TESTS;

        // 测试全局规约
        double allreduce_time = 0.0;
        for (int i = 0; i < NUM_TESTS; i++) {
            MPI_Barrier(MPI_COMM_WORLD);
            double start = MPI_Wtime();
            MPI_Allreduce(data, result, size_test, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
            double end = MPI_Wtime();
            allreduce_time += (end - start);
        }
        allreduce_time /= NUM_TESTS;

        // 测试收集
        double gather_time = 0.0;
        for (int i = 0; i < NUM_TESTS; i++) {
            MPI_Barrier(MPI_COMM_WORLD);
            double start = MPI_Wtime();
            MPI_Gather(data, size_test, MPI_DOUBLE, result, size_test, MPI_DOUBLE, 0, MPI_COMM_WORLD);
            double end = MPI_Wtime();
            gather_time += (end - start);
        }
        gather_time /= NUM_TESTS;

        // 测试全交换
        double alltoall_time = 0.0;
        for (int i = 0; i < NUM_TESTS; i++) {
            MPI_Barrier(MPI_COMM_WORLD);
            double start = MPI_Wtime();
            MPI_Alltoall(data, size_test, MPI_DOUBLE, result, size_test, MPI_DOUBLE, MPI_COMM_WORLD);
            double end = MPI_Wtime();
            alltoall_time += (end - start);
        }
        alltoall_time /= NUM_TESTS;

        if (rank == 0) {
            printf("%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f\n",
                   size_test, bcast_time, reduce_time, allreduce_time, gather_time, alltoall_time);
        }

        free(data);
        free(result);
    }
}

// 编译和运行命令
/*
# 编译
mpicc -O2 -o collective_benchmark collective_benchmark.c -lm

# 运行（4个进程）
mpirun -np 4 ./collective_benchmark

# 在集群上运行
mpirun -np 16 -hostfile hosts ./collective_benchmark

# 使用性能分析
mpiP -o collective_profile.out mpirun -np 4 ./collective_benchmark
*/
```

### 10.4 高级特性

#### 10.4.1 派生数据类型
**派生数据类型概述**：
- 允许传输复杂数据结构
- 提高通信效率和代码可读性
- 减少多次通信调用
- 支持非连续内存布局

**结构体数据类型**：
```c
// 创建结构体数据类型
typedef struct {
    int id;
    double value;
    char name[20];
} DataPacket;

void create_struct_datatype(MPI_Datatype* packet_type) {
    // 定义结构体成员
    int blockcounts[3] = {1, 1, 20};
    MPI_Datatype types[3] = {MPI_INT, MPI_DOUBLE, MPI_CHAR};

    // 计算相对位移
    MPI_Aint displs[3];
    DataPacket packet;
    MPI_Get_address(&packet.id, &displs[0]);
    MPI_Get_address(&packet.value, &displs[1]);
    MPI_Get_address(&packet.name, &displs[2]);

    // 计算相对位移（相对于第一个成员）
    for (int i = 1; i < 3; i++) {
        displs[i] = displs[i] - displs[0];
    }
    displs[0] = 0;

    // 创建派生数据类型
    MPI_Type_create_struct(3, blockcounts, displs, types, packet_type);

    // 提交数据类型
    MPI_Type_commit(packet_type);
}

void struct_datatype_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    MPI_Datatype packet_type;
    create_struct_datatype(&packet_type);

    DataPacket packet;
    if (rank == 0) {
        packet.id = 100;
        packet.value = 3.14159;
        strcpy(packet.name, "Master");
    }

    // 使用派生数据类型进行通信
    MPI_Bcast(&packet, 1, packet_type, 0, MPI_COMM_WORLD);

    printf("Process %d: id=%d, value=%f, name=%s\n",
           rank, packet.id, packet.value, packet.name);

    // 释放数据类型
    MPI_Type_free(&packet_type);
}
```

**向量数据类型**：
```c
// 创建向量类型（用于矩阵操作）
void create_vector_datatype(int rows, int cols, int stride,
                           MPI_Datatype base_type, MPI_Datatype* vector_type) {
    // 创建向量类型：rows个块，每块cols个元素，步长为stride
    MPI_Type_vector(rows, cols, stride, base_type, vector_type);
    MPI_Type_commit(vector_type);
}

// 矩阵列传输示例
void matrix_column_transfer() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int MATRIX_SIZE = 100;
    double matrix[MATRIX_SIZE][MATRIX_SIZE];

    // 初始化矩阵
    if (rank == 0) {
        for (int i = 0; i < MATRIX_SIZE; i++) {
            for (int j = 0; j < MATRIX_SIZE; j++) {
                matrix[i][j] = i * MATRIX_SIZE + j;
            }
        }
    }

    // 创建列向量类型
    MPI_Datatype column_type;
    create_vector_datatype(MATRIX_SIZE, 1, MATRIX_SIZE, MPI_DOUBLE, &column_type);

    double column[MATRIX_SIZE];

    // 进程i接收第i列
    MPI_Scatter(matrix, 1, column_type,
                column, MATRIX_SIZE, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    printf("Process %d received column %d:\n", rank, rank);
    for (int i = 0; i < 5; i++) {
        printf("  column[%d] = %f\n", i, column[i]);
    }

    MPI_Type_free(&column_type);
}
```

**索引数据类型**：
```c
// 创建索引类型（用于不规则数据访问）
void create_indexed_datatype(int count, int* blocklengths, int* displacements,
                            MPI_Datatype old_type, MPI_Datatype* new_type) {
    MPI_Type_indexed(count, blocklengths, displacements, old_type, new_type);
    MPI_Type_commit(new_type);
}

// 不规则数据传输示例
void irregular_data_transfer() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int ARRAY_SIZE = 100;
    int data[ARRAY_SIZE];

    // 初始化数据
    for (int i = 0; i < ARRAY_SIZE; i++) {
        data[i] = i * rank;
    }

    // 定义要传输的不规则模式
    int indices[] = {0, 5, 10, 15, 20, 25, 30, 35, 40, 45};
    int blocklengths[] = {5, 5, 5, 5, 5, 5, 5, 5, 5, 5};
    int displacements[] = {0, 25, 50, 75, 100, 125, 150, 175, 200, 225};

    MPI_Datatype irregular_type;
    create_indexed_datatype(10, blocklengths, displacements, MPI_INT, &irregular_type);

    int received_data[50];

    // 传输不规则数据
    MPI_Gather(data, 1, irregular_type,
               received_data, 50, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Collected irregular data:\n");
        for (int i = 0; i < 20; i++) {
            printf("  %d", received_data[i]);
        }
        printf("\n");
    }

    MPI_Type_free(&irregular_type);
}
```

**子数组数据类型**：
```c
// 创建子数组类型（用于多维数组的子区域）
void create_subarray_datatype(int ndims, int* sizes, int* subsizes, int* starts,
                             MPI_Datatype oldtype, MPI_Datatype* newtype) {
    MPI_Type_create_subarray(ndims, sizes, subsizes, starts, MPI_ORDER_C, oldtype, newtype);
    MPI_Type_commit(newtype);
}

// 2D子数组传输示例
void subarray_transfer() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int GRID_SIZE = 100;
    const int BLOCK_SIZE = GRID_SIZE / (int)sqrt(size);

    double grid[GRID_SIZE][GRID_SIZE];

    // 初始化网格
    for (int i = 0; i < GRID_SIZE; i++) {
        for (int j = 0; j < GRID_SIZE; j++) {
            grid[i][j] = i * GRID_SIZE + j + rank * 1000;
        }
    }

    // 计算进程在2D网格中的位置
    int grid_dim = (int)sqrt(size);
    int row = rank / grid_dim;
    int col = rank % grid_dim;

    // 定义子数组
    int sizes[2] = {GRID_SIZE, GRID_SIZE};
    int subsizes[2] = {BLOCK_SIZE, BLOCK_SIZE};
    int starts[2] = {row * BLOCK_SIZE, col * BLOCK_SIZE};

    MPI_Datatype subarray_type;
    create_subarray_datatype(2, sizes, subsizes, starts, MPI_DOUBLE, &subarray_type);

    // 收集所有子数组到根进程
    double *global_grid = NULL;
    if (rank == 0) {
        global_grid = malloc(GRID_SIZE * GRID_SIZE * sizeof(double));
    }

    MPI_Gather(grid, 1, subarray_type,
               global_grid, BLOCK_SIZE * BLOCK_SIZE, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Global grid corners:\n");
        printf("  Top-left: %f\n", global_grid[0][0]);
        printf("  Top-right: %f\n", global_grid[0][GRID_SIZE-1]);
        printf("  Bottom-left: %f\n", global_grid[GRID_SIZE-1][0]);
        printf("  Bottom-right: %f\n", global_grid[GRID_SIZE-1][GRID_SIZE-1]);
        free(global_grid);
    }

    MPI_Type_free(&subarray_type);
}
```

#### 10.4.2 拓扑管理
**虚拟拓扑概述**：
- 将物理进程映射到逻辑拓扑
- 提供邻居通信的便利接口
- 支持多种拓扑结构（网格、图、树）

**网格拓扑**：
```c
// 创建2D网格拓扑
void create_2d_topology(MPI_Comm old_comm, int dims[2], int periods[2],
                       MPI_Comm* new_comm) {
    MPI_Dims_create(size, 2, dims);
    MPI_Cart_create(old_comm, 2, dims, periods, 0, new_comm);
}

// 网格拓扑通信示例
void grid_topology_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建2D网格拓扑
    int dims[2] = {0, 0};
    int periods[2] = {0, 0};  // 非周期性边界
    MPI_Comm grid_comm;

    MPI_Dims_create(size, 2, dims);
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &grid_comm);

    // 获取进程在网格中的坐标
    int coords[2];
    MPI_Cart_coords(grid_comm, rank, 2, coords);

    // 查找邻居
    int up, down, left, right;
    MPI_Cart_shift(grid_comm, 0, 1, &up, &down);    // 上下邻居
    MPI_Cart_shift(grid_comm, 1, 1, &left, &right); // 左右邻居

    printf("Process %d at (%d,%d): up=%d, down=%d, left=%d, right=%d\n",
           rank, coords[0], coords[1], up, down, left, right);

    // 网格通信：热方程模拟
    double local_temp = rank * 10.0;
    double neighbor_temps[4];

    // 与邻居交换温度数据
    MPI_Sendrecv(&local_temp, 1, MPI_DOUBLE, up, 0,
                 &neighbor_temps[0], 1, MPI_DOUBLE, down, 0,
                 grid_comm, MPI_STATUS_IGNORE);

    MPI_Sendrecv(&local_temp, 1, MPI_DOUBLE, down, 1,
                 &neighbor_temps[1], 1, MPI_DOUBLE, up, 1,
                 grid_comm, MPI_STATUS_IGNORE);

    MPI_Sendrecv(&local_temp, 1, MPI_DOUBLE, left, 2,
                 &neighbor_temps[2], 1, MPI_DOUBLE, right, 2,
                 grid_comm, MPI_STATUS_IGNORE);

    MPI_Sendrecv(&local_temp, 1, MPI_DOUBLE, right, 3,
                 &neighbor_temps[3], 1, MPI_DOUBLE, left, 3,
                 grid_comm, MPI_STATUS_IGNORE);

    // 计算平均温度
    double new_temp = local_temp;
    for (int i = 0; i < 4; i++) {
        if (neighbor_temps[i] != -1) {  // -1表示边界
            new_temp += neighbor_temps[i];
        }
    }
    new_temp /= (1 + count_valid_neighbors(neighbor_temps));

    printf("Process %d: old_temp=%f, new_temp=%f\n", rank, local_temp, new_temp);

    MPI_Comm_free(&grid_comm);
}

int count_valid_neighbors(double temps[4]) {
    int count = 0;
    for (int i = 0; i < 4; i++) {
        if (temps[i] != -1) count++;
    }
    return count;
}
```

**图拓扑**：
```c
// 创建图拓扑
void create_graph_topology(MPI_Comm old_comm, int* index, int* edges,
                         MPI_Comm* new_comm) {
    MPI_Graph_create(old_comm, size, index, edges, 0, new_comm);
}

// 图拓扑通信示例
void graph_topology_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 定义图结构：每个进程连接到下一个进程（环形）
    int index[size];
    int edges[size];

    for (int i = 0; i < size; i++) {
        index[i] = i + 1;
        edges[i] = (i + 1) % size;
    }

    MPI_Comm graph_comm;
    MPI_Graph_create(MPI_COMM_WORLD, size, index, edges, 0, &graph_comm);

    // 获取邻居信息
    int neighbors_count;
    MPI_Graph_neighbors_count(graph_comm, rank, &neighbors_count);

    int* neighbors = malloc(neighbors_count * sizeof(int));
    MPI_Graph_neighbors(graph_comm, rank, neighbors_count, neighbors);

    printf("Process %d has %d neighbors:", rank, neighbors_count);
    for (int i = 0; i < neighbors_count; i++) {
        printf(" %d", neighbors[i]);
    }
    printf("\n");

    // 图通信：广播到所有邻居
    double message = rank * 100.0;
    for (int i = 0; i < neighbors_count; i++) {
        int dest = neighbors[i];
        MPI_Send(&message, 1, MPI_DOUBLE, dest, 0, graph_comm);
    }

    // 接收来自邻居的消息
    double received_messages[size];
    for (int i = 0; i < neighbors_count; i++) {
        int source = neighbors[i];
        MPI_Recv(&received_messages[i], 1, MPI_DOUBLE, source, 0, graph_comm, MPI_STATUS_IGNORE);
    }

    printf("Process %d received:", rank);
    for (int i = 0; i < neighbors_count; i++) {
        printf(" %f", received_messages[i]);
    }
    printf("\n");

    free(neighbors);
    MPI_Comm_free(&graph_comm);
}
```

#### 10.4.3 非阻塞集合通信
**非阻塞集体通信概述**：
- MPI-3.0引入的特性
- 允许重叠通信和计算
- 提供更好的性能潜力

**非阻塞广播**：
```c
// 非阻塞广播示例
void nonblocking_bcast_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int DATA_SIZE = 1000000;
    double* data = malloc(DATA_SIZE * sizeof(double));

    if (rank == 0) {
        // 初始化数据
        for (int i = 0; i < DATA_SIZE; i++) {
            data[i] = i * 0.1;
        }
    }

    MPI_Request request;
    MPI_Request request_free;

    // 启动非阻塞广播
    MPI_Ibcast(data, DATA_SIZE, MPI_DOUBLE, 0, MPI_COMM_WORLD, &request);

    // 同时进行计算（与通信重叠）
    double computation_result = 0.0;
    for (int i = 0; i < DATA_SIZE / 10; i++) {
        computation_result += sin(data[i]) * cos(data[i]);
    }

    // 等待广播完成
    MPI_Wait(&request, MPI_STATUS_IGNORE);

    printf("Process %d: broadcast completed, computation result = %f\n",
           rank, computation_result);

    free(data);
}
```

**非阻塞规约**：
```c
// 非阻塞规约示例
void nonblocking_reduce_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int DATA_SIZE = 100000;
    double* local_data = malloc(DATA_SIZE * sizeof(double));
    double* result = malloc(DATA_SIZE * sizeof(double));

    // 初始化局部数据
    for (int i = 0; i < DATA_SIZE; i++) {
        local_data[i] = rank * 1000 + i;
    }

    MPI_Request request;

    // 启动非阻塞规约
    MPI_Iallreduce(local_data, result, DATA_SIZE, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD, &request);

    // 同时进行其他计算
    double local_sum = 0.0;
    for (int i = 0; i < DATA_SIZE; i++) {
        local_sum += local_data[i] * local_data[i];
    }

    // 等待规约完成
    MPI_Wait(&request, MPI_STATUS_IGNORE);

    // 验证结果
    double expected = 0.0;
    for (int r = 0; r < size; r++) {
        expected += r * 1000 * DATA_SIZE + DATA_SIZE * (DATA_SIZE - 1) / 2;
    }

    printf("Process %d: local_sum=%f, global_sum=%f, expected=%f\n",
           rank, local_sum, result[0], expected);

    free(local_data);
    free(result);
}
```

#### 10.4.4 动态进程管理
**动态进程创建**：
```c
// 动态进程管理示例
void dynamic_process_example() {
    int rank, size, is_intercomm;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 检查是否为多程序环境
    MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_APPNUM, &is_intercomm, &flag);

    if (rank == 0) {
        printf("Master process starting dynamic processes...\n");

        // 创建新的进程组
        MPI_Comm child_comm;
        char* command = "./worker_program";
        char* argv[] = {"worker_program", NULL};
        int maxprocs = 4;
        MPI_Info info = MPI_INFO_NULL;

        // 启动新的进程
        MPI_Comm_spawn(command, argv, maxprocs, info, 0, MPI_COMM_SELF,
                       &child_comm, MPI_ERRCODES_IGNORE);

        printf("Spawned %d worker processes\n", maxprocs);

        // 与子进程通信
        int work_size = 1000;
        for (int i = 0; i < maxprocs; i++) {
            MPI_Send(&work_size, 1, MPI_INT, i, 0, child_comm);
        }

        // 收集结果
        double results[maxprocs];
        MPI_Gather(NULL, 0, MPI_DOUBLE, results, 1, MPI_DOUBLE, 0, child_comm);

        printf("Results from workers: ");
        for (int i = 0; i < maxprocs; i++) {
            printf("%f ", results[i]);
        }
        printf("\n");

        // 关闭子进程
        MPI_Comm_disconnect(&child_comm);
    }
}
```

**进程组管理**：
```c
// 进程组动态管理
void dynamic_group_management() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建动态进程组
    MPI_Group world_group;
    MPI_Comm_group(MPI_COMM_WORLD, &world_group);

    // 动态创建子组
    int ranks[2] = {0, 2};
    MPI_Group subgroup;
    MPI_Group_incl(world_group, 2, ranks, &subgroup);

    MPI_Comm subcomm;
    MPI_Comm_create_group(MPI_COMM_WORLD, subgroup, 0, &subcomm);

    if (subcomm != MPI_COMM_NULL) {
        int subrank, subsize;
        MPI_Comm_rank(subcomm, &subrank);
        MPI_Comm_size(subcomm, &subsize);

        printf("Subgroup: rank=%d, size=%d\n", subrank, subsize);

        // 在子组内进行通信
        double local_data = rank * 10.0;
        double global_sum;
        MPI_Allreduce(&local_data, &global_sum, 1, MPI_DOUBLE, MPI_SUM, subcomm);

        printf("Subgroup sum: %f\n", global_sum);

        MPI_Comm_free(&subcomm);
    }

    MPI_Group_free(&subgroup);
    MPI_Group_free(&world_group);
}
```

#### 10.4.5 错误处理和容错
**错误处理机制**：
```c
// 自定义错误处理
void error_handler(MPI_Comm* comm, int* error_code, ...) {
    char error_string[100];
    int length;
    MPI_Error_string(*error_code, error_string, &length);

    printf("MPI Error occurred: %s\n", error_string);

    // 可以选择继续执行或终止
    if (*error_code != MPI_SUCCESS) {
        printf("Terminating due to critical error\n");
        MPI_Abort(*comm, *error_code);
    }
}

// 错误处理示例
void error_handling_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 设置自定义错误处理
    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);

    // 可能出错的操作
    int* data = NULL;
    if (rank == 0) {
        data = malloc(100 * sizeof(int));
    }

    // 安全的广播（处理错误）
    int error = MPI_Bcast(data, 100, MPI_INT, 0, MPI_COMM_WORLD);
    if (error != MPI_SUCCESS) {
        error_handler(&MPI_COMM_WORLD, &error);
    }

    // 检查通信状态
    MPI_Status status;
    error = MPI_Recv(data, 100, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG,
                     MPI_COMM_WORLD, &status);

    if (error == MPI_ERR_TRUNCATE) {
        printf("Message truncated - buffer too small\n");
    } else if (error == MPI_ERR_TAG) {
        printf("Invalid message tag\n");
    }

    if (rank == 0) {
        free(data);
    }
}
```

**容错通信**：
```c
// 容错通信示例
void fault_tolerant_communication() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    const int MAX_RETRIES = 3;
    const int DATA_SIZE = 1000;

    double* data = malloc(DATA_SIZE * sizeof(double));
    double* result = malloc(DATA_SIZE * sizeof(double));

    // 初始化数据
    for (int i = 0; i < DATA_SIZE; i++) {
        data[i] = rank * 1000 + i;
    }

    // 带重试的规约操作
    int retries = 0;
    int success = 0;

    while (!success && retries < MAX_RETRIES) {
        int error = MPI_Allreduce(data, result, DATA_SIZE, MPI_DOUBLE,
                                  MPI_SUM, MPI_COMM_WORLD);

        if (error == MPI_SUCCESS) {
            success = 1;
            printf("Process %d: Allreduce successful after %d retries\n",
                   rank, retries);
        } else {
            retries++;
            printf("Process %d: Allreduce failed, retry %d\n", rank, retries);
            // 可以添加延迟或其他恢复策略
        }
    }

    if (!success) {
        printf("Process %d: Allreduce failed after %d retries\n", rank, MAX_RETRIES);
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    free(data);
    free(result);
}
```

#### 10.4.6 性能优化高级技巧
**通信优化策略**：
```c
// 高级通信优化
void advanced_communication_optimization() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 1. 通信缓冲区优化
    const int BUFFER_SIZE = 1024 * 1024;  // 1MB
    double* send_buffer = malloc(BUFFER_SIZE);
    double* recv_buffer = malloc(BUFFER_SIZE);

    // 使用pinned memory提高传输速度
    double* pinned_send;
    double* pinned_recv;
    MPI_Alloc_mem(BUFFER_SIZE, MPI_INFO_NULL, &pinned_send);
    MPI_Alloc_mem(BUFFER_SIZE, MPI_INFO_NULL, &pinned_recv);

    // 2. 通信重叠优化
    MPI_Request requests[4];

    // 启动多个非阻塞通信
    MPI_Isend(pinned_send, BUFFER_SIZE/sizeof(double), MPI_DOUBLE,
              (rank+1)%size, 0, MPI_COMM_WORLD, &requests[0]);
    MPI_Irecv(pinned_recv, BUFFER_SIZE/sizeof(double), MPI_DOUBLE,
              (rank-1+size)%size, 0, MPI_COMM_WORLD, &requests[1]);

    // 计算操作
    for (int i = 0; i < BUFFER_SIZE/sizeof(double); i++) {
        pinned_send[i] = sin(pinned_send[i]) * cos(pinned_send[i]);
    }

    // 等待通信完成
    MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);

    // 3. 数据压缩（简单示例）
    double* compressed_data;
    int compressed_size = compress_data(pinned_send, BUFFER_SIZE, &compressed_data);

    MPI_Isend(compressed_data, compressed_size, MPI_BYTE,
              (rank+1)%size, 1, MPI_COMM_WORLD, &requests[2]);

    // 4. 流水线通信
    const int PIPELINE_STAGES = 4;
    for (int stage = 0; stage < PIPELINE_STAGES; stage++) {
        // 处理当前阶段数据
        process_stage_data(stage, pinned_send);

        // 启动下一阶段通信
        if (stage < PIPELINE_STAGES - 1) {
            MPI_Isend(pinned_send, BUFFER_SIZE/sizeof(double), MPI_DOUBLE,
                      (rank+1)%size, stage+2, MPI_COMM_WORLD, &requests[3]);
        }

        // 等待当前阶段数据到达
        if (stage > 0) {
            MPI_Wait(&requests[3], MPI_STATUS_IGNORE);
        }
    }

    // 清理资源
    MPI_Free_mem(pinned_send);
    MPI_Free_mem(pinned_recv);
    free(send_buffer);
    free(recv_buffer);
}

int compress_data(double* data, int size, double** compressed) {
    // 简单的压缩算法示例
    int compressed_count = 0;
    *compressed = malloc(size);

    for (int i = 0; i < size/sizeof(double); i++) {
        if (fabs(data[i]) > 1e-6) {
            (*compressed)[compressed_count++] = data[i];
        }
    }

    return compressed_count * sizeof(double);
}

void process_stage_data(int stage, double* data) {
    // 模拟数据处理
    for (int i = 0; i < 1000; i++) {
        data[i] = data[i] * stage;
    }
}
```

**内存管理优化**：
```c
// 高级内存管理
void advanced_memory_management() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 1. 内存池管理
    const int POOL_SIZE = 10;
    const int BLOCK_SIZE = 1024 * 1024;

    typedef struct {
        void** blocks;
        int* used;
        int size;
    } MemoryPool;

    MemoryPool pool;
    pool.size = POOL_SIZE;
    pool.blocks = malloc(POOL_SIZE * sizeof(void*));
    pool.used = malloc(POOL_SIZE * sizeof(int));

    // 初始化内存池
    for (int i = 0; i < POOL_SIZE; i++) {
        pool.blocks[i] = malloc(BLOCK_SIZE);
        pool.used[i] = 0;
    }

    // 分配函数
    void* pool_alloc(MemoryPool* pool, size_t size) {
        for (int i = 0; i < pool->size; i++) {
            if (!pool->used[i] && BLOCK_SIZE >= size) {
                pool->used[i] = 1;
                return pool->blocks[i];
            }
        }
        return NULL;  // 内存池耗尽
    }

    // 释放函数
    void pool_free(MemoryPool* pool, void* ptr) {
        for (int i = 0; i < pool->size; i++) {
            if (pool->blocks[i] == ptr) {
                pool->used[i] = 0;
                break;
            }
        }
    }

    // 使用内存池
    void* buffer1 = pool_alloc(&pool, 512 * 1024);
    void* buffer2 = pool_alloc(&pool, 256 * 1024);

    if (buffer1 && buffer2) {
        printf("Successfully allocated buffers from pool\n");

        // 使用缓冲区进行通信
        MPI_Send(buffer1, 512 * 1024, MPI_BYTE, (rank+1)%size, 0, MPI_COMM_WORLD);
        MPI_Recv(buffer2, 256 * 1024, MPI_BYTE, (rank-1+size)%size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

        // 释放缓冲区
        pool_free(&pool, buffer1);
        pool_free(&pool, buffer2);
    }

    // 清理内存池
    for (int i = 0; i < POOL_SIZE; i++) {
        free(pool.blocks[i]);
    }
    free(pool.blocks);
    free(pool.used);
}
```

#### 10.4.7 实际应用示例：并行热传导模拟
```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#define GRID_SIZE 100
#define ITERATIONS 1000
#define ALPHA 0.25

void parallel_heat_conduction() {
    int rank, size;
    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 创建2D网格拓扑
    int dims[2] = {0, 0};
    int periods[2] = {1, 1};  // 周期性边界条件
    MPI_Comm grid_comm;

    MPI_Dims_create(size, 2, dims);
    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 0, &grid_comm);

    // 获取进程坐标
    int coords[2];
    MPI_Cart_coords(grid_comm, rank, 2, coords);

    // 计算局部网格大小
    int local_rows = GRID_SIZE / dims[0];
    int local_cols = GRID_SIZE / dims[1];

    // 分配局部网格
    double *u_old = malloc((local_rows + 2) * (local_cols + 2) * sizeof(double));
    double *u_new = malloc((local_rows + 2) * (local_cols + 2) * sizeof(double));

    // 初始化局部网格
    for (int i = 0; i < local_rows + 2; i++) {
        for (int j = 0; j < local_cols + 2; j++) {
            u_old[i * (local_cols + 2) + j] = 0.0;
            u_new[i * (local_cols + 2) + j] = 0.0;
        }
    }

    // 设置初始条件（只在第一个进程）
    if (rank == 0) {
        int center_i = local_rows / 2 + 1;
        int center_j = local_cols / 2 + 1;
        u_old[center_i * (local_cols + 2) + center_j] = 100.0;
    }

    // 查找邻居
    int up, down, left, right;
    MPI_Cart_shift(grid_comm, 0, 1, &up, &down);
    MPI_Cart_shift(grid_comm, 1, 1, &left, &right);

    // 时间步进循环
    for (int iter = 0; iter < ITERATIONS; iter++) {
        // 边界交换
        MPI_Request requests[4];
        int req_count = 0;

        // 上下边界交换
        MPI_Isend(&u_old[1 * (local_cols + 2)], local_cols, MPI_DOUBLE, up, 0, grid_comm, &requests[req_count++]);
        MPI_Irecv(&u_old[0 * (local_cols + 2)], local_cols, MPI_DOUBLE, down, 0, grid_comm, &requests[req_count++]);
        MPI_Isend(&u_old[local_rows * (local_cols + 2)], local_cols, MPI_DOUBLE, down, 1, grid_comm, &requests[req_count++]);
        MPI_Irecv(&u_old[(local_rows + 1) * (local_cols + 2)], local_cols, MPI_DOUBLE, up, 1, grid_comm, &requests[req_count++]);

        // 左右边界交换
        for (int i = 1; i <= local_rows; i++) {
            MPI_Isend(&u_old[i * (local_cols + 2) + 1], 1, MPI_DOUBLE, left, 2, grid_comm, &requests[req_count++]);
            MPI_Irecv(&u_old[i * (local_cols + 2) + 0], 1, MPI_DOUBLE, right, 2, grid_comm, &requests[req_count++]);
            MPI_Isend(&u_old[i * (local_cols + 2) + local_cols], 1, MPI_DOUBLE, right, 3, grid_comm, &requests[req_count++]);
            MPI_Irecv(&u_old[i * (local_cols + 2) + (local_cols + 1)], 1, MPI_DOUBLE, left, 3, grid_comm, &requests[req_count++]);
        }

        // 等待所有通信完成
        MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);

        // 计算新的温度分布
        for (int i = 1; i <= local_rows; i++) {
            for (int j = 1; j <= local_cols; j++) {
                u_new[i * (local_cols + 2) + j] =
                    u_old[i * (local_cols + 2) + j] +
                    ALPHA * (u_old[(i-1) * (local_cols + 2) + j] +
                             u_old[(i+1) * (local_cols + 2) + j] +
                             u_old[i * (local_cols + 2) + (j-1)] +
                             u_old[i * (local_cols + 2) + (j+1)] -
                             4 * u_old[i * (local_cols + 2) + j]);
            }
        }

        // 交换指针
        double *temp = u_old;
        u_old = u_new;
        u_new = temp;

        // 输出中间结果（每100步）
        if (iter % 100 == 0 && rank == 0) {
            printf("Iteration %d completed\n", iter);
        }
    }

    // 收集结果到根进程
    if (rank == 0) {
        double *global_grid = malloc(GRID_SIZE * GRID_SIZE * sizeof(double));

        // 从每个进程收集数据
        for (int r = 0; r < size; r++) {
            int r_coords[2];
            MPI_Cart_coords(grid_comm, r, 2, r_coords);

            int r_start_i = r_coords[0] * local_rows;
            int r_start_j = r_coords[1] * local_cols;

            double *r_data = malloc(local_rows * local_cols * sizeof(double));

            if (r == 0) {
                // 直接复制根进程的数据
                for (int i = 0; i < local_rows; i++) {
                    for (int j = 0; j < local_cols; j++) {
                        r_data[i * local_cols + j] = u_old[(i+1) * (local_cols + 2) + (j+1)];
                    }
                }
            } else {
                // 从其他进程接收数据
                MPI_Recv(r_data, local_rows * local_cols, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            }

            // 将数据放入全局网格
            for (int i = 0; i < local_rows; i++) {
                for (int j = 0; j < local_cols; j++) {
                    global_grid[(r_start_i + i) * GRID_SIZE + (r_start_j + j)] = r_data[i * local_cols + j];
                }
            }

            free(r_data);
        }

        // 输出结果统计
        double max_temp = 0.0, min_temp = 100.0, avg_temp = 0.0;
        for (int i = 0; i < GRID_SIZE * GRID_SIZE; i++) {
            if (global_grid[i] > max_temp) max_temp = global_grid[i];
            if (global_grid[i] < min_temp) min_temp = global_grid[i];
            avg_temp += global_grid[i];
        }
        avg_temp /= (GRID_SIZE * GRID_SIZE);

        printf("Final temperature distribution:\n");
        printf("  Max: %f\n", max_temp);
        printf("  Min: %f\n", min_temp);
        printf("  Avg: %f\n", avg_temp);

        free(global_grid);
    } else {
        // 发送数据到根进程
        double *send_data = malloc(local_rows * local_cols * sizeof(double));
        for (int i = 0; i < local_rows; i++) {
            for (int j = 0; j < local_cols; j++) {
                send_data[i * local_cols + j] = u_old[(i+1) * (local_cols + 2) + (j+1)];
            }
        }
        MPI_Send(send_data, local_rows * local_cols, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
        free(send_data);
    }

    // 清理资源
    free(u_old);
    free(u_new);
    MPI_Comm_free(&grid_comm);
    MPI_Finalize();
}

// 编译和运行命令
/*
# 编译
mpicc -O2 -o heat_conduction heat_conduction.c -lm

# 运行（4个进程，2x2网格）
mpirun -np 4 ./heat_conduction

# 在集群上运行
mpirun -np 16 -hostfile hosts ./heat_conduction
*/
```

## 第11章 OpenMP

### 11.1 OpenMP基础

#### 11.1.1 OpenMP概述
**OpenMP简介**：
- **Open Multi-Processing**：跨平台的共享内存并行编程API
- **编译指令驱动**：通过编译指令（pragma）控制并行行为
- **多语言支持**：C/C++、Fortran
- **可移植性**：支持多种编译器和操作系统
- **渐进式并行**：可以从串行代码逐步添加并行性

**OpenMP架构**：
```
┌─────────────────────────────────────────┐
│              应用程序                    │
├─────────────────────────────────────────┤
│           OpenMP运行时库                 │
├─────────────────────────────────────────┤
│              操作系统                    │
├─────────────────────────────────────────┤
│            多核处理器                    │
└─────────────────────────────────────────┘
```

**主要组件**：
- **编译指令**：控制并行结构和行为
- **运行时库函数**：动态控制并行行为
- **环境变量**：配置并行参数

#### 11.1.2 编译指令基础
**编译指令语法**：
```c
// 基本语法
#pragma omp directive [clause[ [,] clause] ...]

// 示例
#pragma omp parallel for private(i) reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += array[i];
}
```

**编译器支持**：
```bash
# GCC
gcc -fopenmp program.c -o program

# Intel编译器
icc -qopenmp program.c -o program

# Clang
clang -fopenmp program.c -o program

# Visual Studio
cl /openmp program.c
```

#### 11.1.3 并行区域指令
**parallel指令**：
```c
#include <omp.h>
#include <stdio.h>

void parallel_region_example() {
    printf("Serial code before parallel region\n");

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("Hello from thread %d of %d\n", thread_id, num_threads);
    }

    printf("Serial code after parallel region\n");
}
```

**parallel指令的执行流程**：
1. **主线程遇到#pragma omp parallel**
2. **创建线程团队（team）**
3. **所有线程执行并行区域代码**
4. **隐式同步点（barrier）**
5. **线程团队解散，只留主线程**

**并行区域的线程管理**：
```c
void thread_management_example() {
    int num_threads_requested = 4;

    #pragma omp parallel num_threads(num_threads_requested)
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        int max_threads = omp_get_max_threads();

        printf("Thread %d: num_threads=%d, max_threads=%d\n",
               thread_id, num_threads, max_threads);
    }
}
```

#### 11.1.4 for循环并行化
**基本for并行化**：
```c
void basic_for_parallel() {
    const int N = 1000000;
    double sum = 0.0;
    double array[N];

    // 初始化数组
    for (int i = 0; i < N; i++) {
        array[i] = i * 0.1;
    }

    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        array[i] = array[i] * 2.0;
    }

    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < N; i++) {
        sum += array[i];
    }

    printf("Sum: %f\n", sum);
}
```

**调度策略**：
```c
void scheduling_strategies() {
    const int N = 1000;

    // 静态调度（默认）
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < N; i++) {
        printf("Thread %d processing iteration %d\n",
               omp_get_thread_num(), i);
    }

    // 动态调度
    #pragma omp parallel for schedule(dynamic, 10)
    for (int i = 0; i < N; i++) {
        printf("Thread %d processing iteration %d\n",
               omp_get_thread_num(), i);
    }

    // 运行时调度
    #pragma omp parallel for schedule(runtime)
    for (int i = 0; i < N; i++) {
        printf("Thread %d processing iteration %d\n",
               omp_get_thread_num(), i);
    }

    // 指导性调度
    #pragma omp parallel for schedule(guided, 8)
    for (int i = 0; i < N; i++) {
        printf("Thread %d processing iteration %d\n",
               omp_get_thread_num(), i);
    }
}
```

**调度策略详解**：
- **static**: 编译时分配，适合负载均匀
- **dynamic**: 运行时分配，适合负载不均
- **guided**: 动态调整块大小
- **runtime**: 由环境变量OMP_SCHEDULE决定

#### 11.1.5 sections并行化
**sections指令**：
```c
void sections_example() {
    #pragma omp parallel sections
    {
        #pragma omp section
        {
            printf("Section 1 executed by thread %d\n", omp_get_thread_num());
            // 执行任务1
            for (int i = 0; i < 1000000; i++) {
                // 计算密集型任务
            }
        }

        #pragma omp section
        {
            printf("Section 2 executed by thread %d\n", omp_get_thread_num());
            // 执行任务2
            for (int i = 0; i < 1000000; i++) {
                // 计算密集型任务
            }
        }

        #pragma omp section
        {
            printf("Section 3 executed by thread %d\n", omp_get_thread_num());
            // 执行任务3
            for (int i = 0; i < 1000000; i++) {
                // 计算密集型任务
            }
        }
    }
}
```

**单线程执行**：
```c
void single_vs_master_example() {
    #pragma omp parallel
    {
        // 单线程执行，其他线程等待
        #pragma omp single
        {
            printf("Single region executed by thread %d\n", omp_get_thread_num());
            // 初始化工作
        }

        printf("After single, thread %d continues\n", omp_get_thread_num());

        // 只有主线程执行
        #pragma omp master
        {
            printf("Master region executed by thread %d\n", omp_get_thread_num());
        }

        printf("After master, thread %d continues\n", omp_get_thread_num());
    }
}
```

#### 11.1.6 数据共享属性
**数据属性分类**：
```c
void data_sharing_example() {
    int i;           // 默认共享
    int sum = 0;     // 默认共享
    int local_var;   // 默认共享

    #pragma omp parallel for private(i) firstprivate(local_var) reduction(+:sum)
    for (i = 0; i < 1000; i++) {
        local_var = i * 2;  // 每个线程有自己的副本
        sum += local_var;   // reduction累加到共享变量
    }

    printf("Final sum: %d\n", sum);
}
```

**详细数据属性说明**：
```c
void detailed_data_attributes() {
    int shared_var = 10;      // 共享变量
    int private_var = 20;     // 私有变量
    int firstprivate_var = 30; // 初始值私有化
    int lastprivate_var = 40;  // 最后一次迭代的值
    int reduction_var = 0;    // 规约变量

    #pragma omp parallel for \
        private(private_var) \
        firstprivate(firstprivate_var) \
        lastprivate(lastprivate_var) \
        reduction(+:reduction_var)
    for (int i = 0; i < 100; i++) {
        private_var = i;                    // 每个线程独立
        firstprivate_var += i;              // 基于初始值30
        lastprivate_var = i;                // 最后i=99的值
        reduction_var += i;                 // 累加到共享变量
    }

    printf("shared_var: %d\n", shared_var);
    printf("firstprivate_var: %d\n", firstprivate_var);  // 仍然是30
    printf("lastprivate_var: %d\n", lastprivate_var);    // 99
    printf("reduction_var: %d\n", reduction_var);       // 4950 (0+1+...+99)
}
```

**数据属性规则**：
- **shared**: 所有线程共享同一内存位置
- **private**: 每个线程有独立副本，未初始化
- **firstprivate**: 私有副本，初始化为进入时的值
- **lastprivate**: 私有副本，退出时复制最后的值
- **reduction**: 私有副本，退出时归约到共享变量

#### 11.1.7 运行时库函数
**线程信息函数**：
```c
void runtime_library_example() {
    printf("Max threads: %d\n", omp_get_max_threads());
    printf("Num threads: %d\n", omp_get_num_threads());
    printf("Thread num: %d\n", omp_get_thread_num());

    // 设置线程数
    omp_set_num_threads(4);

    #pragma omp parallel
    {
        printf("Thread %d of %d\n",
               omp_get_thread_num(), omp_get_num_threads());
    }
}
```

**环境查询函数**：
```c
void environment_queries() {
    // 嵌套并行
    printf("Nested parallel: %s\n",
           omp_get_nested() ? "enabled" : "disabled");

    // 动态线程
    printf("Dynamic threads: %s\n",
           omp_get_dynamic() ? "enabled" : "disabled");

    // 并行区域数量
    printf("Active levels: %d\n", omp_get_active_level());
    printf("Max levels: %d\n", omp_get_max_active_levels());

    // 线程亲和性
    #ifdef _OPENMP
        printf("Thread affinity: %s\n",
               omp_get_proc_bind() == omp_proc_bind_false ? "false" :
               omp_get_proc_bind() == omp_proc_bind_true ? "true" :
               omp_get_proc_bind() == omp_proc_bind_master ? "master" : "spread");
    #endif
}
```

**时间和同步函数**：
```c
void timing_and_sync() {
    double start_time = omp_get_wtime();

    #pragma omp parallel
    {
        // 工作...
        #pragma omp barrier
        double thread_time = omp_get_wtime();
        printf("Thread %d reached barrier at %f\n",
               omp_get_thread_num(), thread_time - start_time);
    }

    double end_time = omp_get_wtime();
    printf("Total time: %f seconds\n", end_time - start_time);
}
```

#### 11.1.8 环境变量配置
**常用环境变量**：
```bash
# 设置线程数
export OMP_NUM_THREADS=8

# 设置调度策略
export OMP_SCHEDULE="dynamic,16"

# 启用嵌套并行
export OMP_NESTED=TRUE

# 设置动态线程
export OMP_DYNAMIC=TRUE

# 设置线程绑定
export OMP_PROC_BIND=TRUE

# 设置线程放置
export OMP_PLACES="cores"

# 设置堆栈大小
export OMP_STACKSIZE=2M
```

**运行时配置示例**：
```c
void runtime_configuration() {
    // 通过环境变量设置
    // export OMP_NUM_THREADS=4
    // export OMP_SCHEDULE="dynamic,8"

    #pragma omp parallel
    {
        printf("Thread %d of %d\n",
               omp_get_thread_num(), omp_get_num_threads());
    }

    // 也可以在代码中设置
    omp_set_num_threads(2);
    omp_set_schedule(omp_sched_dynamic, 16);
}
```

#### 11.1.9 实际应用示例：并行矩阵乘法
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define MATRIX_SIZE 1000

void parallel_matrix_multiplication() {
    double *A, *B, *C;
    int i, j, k;

    // 分配内存
    A = (double*)malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
    B = (double*)malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));
    C = (double*)malloc(MATRIX_SIZE * MATRIX_SIZE * sizeof(double));

    // 初始化矩阵
    srand(time(NULL));
    for (i = 0; i < MATRIX_SIZE * MATRIX_SIZE; i++) {
        A[i] = (double)rand() / RAND_MAX;
        B[i] = (double)rand() / RAND_MAX;
        C[i] = 0.0;
    }

    double start_time = omp_get_wtime();

    // 并行矩阵乘法
    #pragma omp parallel for private(i, j, k) schedule(dynamic, 32)
    for (i = 0; i < MATRIX_SIZE; i++) {
        for (j = 0; j < MATRIX_SIZE; j++) {
            double sum = 0.0;
            for (k = 0; k < MATRIX_SIZE; k++) {
                sum += A[i * MATRIX_SIZE + k] * B[k * MATRIX_SIZE + j];
            }
            C[i * MATRIX_SIZE + j] = sum;
        }
    }

    double end_time = omp_get_wtime();
    printf("Parallel matrix multiplication time: %f seconds\n", end_time - start_time);

    // 验证结果（小规模）
    if (MATRIX_SIZE <= 10) {
        printf("Result matrix (first 3x3):\n");
        for (i = 0; i < 3; i++) {
            for (j = 0; j < 3; j++) {
                printf("%8.4f ", C[i * MATRIX_SIZE + j]);
            }
            printf("\n");
        }
    }

    free(A); free(B); free(C);
}

// 性能对比示例
void performance_comparison() {
    const int N = 2000;

    // 串行版本
    double start_time = omp_get_wtime();

    // ... 串行矩阵乘法 ...

    double serial_time = omp_get_wtime() - start_time;

    // 并行版本
    start_time = omp_get_wtime();

    #pragma omp parallel for schedule(dynamic, 32)
    // ... 并行矩阵乘法 ...

    double parallel_time = omp_get_wtime() - start_time;

    printf("Serial time: %f seconds\n", serial_time);
    printf("Parallel time: %f seconds\n", parallel_time);
    printf("Speedup: %f\n", serial_time / parallel_time);
    printf("Efficiency: %f%%\n", (serial_time / parallel_time / omp_get_max_threads()) * 100);
}
```

#### 11.1.10 编译和调试
**编译选项**：
```bash
# 基本编译
gcc -fopenmp program.c -o program

# 优化编译
gcc -fopenmp -O3 program.c -o program

# 调试编译
gcc -fopenmp -g -O0 program.c -o program

# 启用OpenMP检查
gcc -fopenmp -fopenmp-simd -O2 program.c -o program
```

**调试技巧**：
```c
void debugging_tips() {
    // 1. 使用omp_get_thread_num()调试
    #pragma omp parallel
    {
        printf("Thread %d: starting work\n", omp_get_thread_num());

        // 工作代码...

        printf("Thread %d: finished work\n", omp_get_thread_num());
    }

    // 2. 使用flush确保内存一致性
    int flag = 0;

    #pragma omp parallel
    {
        if (omp_get_thread_num() == 0) {
            // 生产者
            data = compute_value();
            #pragma omp flush
            flag = 1;
            #pragma omp flush(flag)
        } else {
            // 消费者
            #pragma omp flush(flag)
            while (flag == 0) {
                #pragma omp flush(flag)
                // 等待
            }
            #pragma omp flush
            use_data(data);
        }
    }

    // 3. 使用critical保护共享资源
    #pragma omp parallel for
    for (int i = 0; i < 1000; i++) {
        double result = compute(i);

        #pragma omp critical
        {
            printf("Thread %d: result[%d] = %f\n",
                   omp_get_thread_num(), i, result);
        }
    }
}
```

**性能分析**：
```bash
# 使用gprof分析
gcc -fopenmp -pg program.c -o program
./program
gprof program gmon.out > analysis.txt

# 使用perf分析
perf record ./program
perf report

# 使用OpenMP特定工具
export OMP_DISPLAY_ENV=TRUE
export OMP_VERBOSE=1
./program
```

#### 11.1.11 最佳实践
**编写高效OpenMP代码的建议**：

1. **负载均衡**：
```c
// 好的做法：使用动态调度处理不均匀负载
#pragma omp parallel for schedule(dynamic, 16)
for (int i = 0; i < n; i++) {
    // 工作量可能不均匀的循环
    process_element(i);
}

// 避免：静态调度可能导致负载不均
#pragma omp parallel for schedule(static)
for (int i = 0; i < n; i++) {
    // 复杂度变化很大的操作
    complex_operation(i);
}
```

2. **数据局部性**：
```c
// 好的做法：保持数据局部性
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // 访问相邻内存位置
    process_array_chunk(data + i * chunk_size, chunk_size);
}

// 避免：随机内存访问模式
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // 随机访问
    process_element(random_indices[i]);
}
```

3. **减少同步开销**：
```c
// 好的做法：减少临界区
#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += compute_value(i);
}

// 避免：频繁的临界区
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    double value = compute_value(i);
    #pragma omp critical
    sum += value;
}
```

4. **内存管理**：
```c
// 好的做法：在线程内部分配内存
#pragma omp parallel
{
    double *local_buffer = malloc(buffer_size);
    // 使用缓冲区...
    free(local_buffer);
}

// 避免：在并行区域外分配大量内存
double *shared_buffer = malloc(large_size);
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // 可能导致内存争用
}
free(shared_buffer);
```

**常见错误和解决方案**：

1. **数据竞争**：
```c
// 错误：数据竞争
int counter = 0;
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    counter++;  // 数据竞争！
}

// 正确：使用reduction
int counter = 0;
#pragma omp parallel for reduction(+:counter)
for (int i = 0; i < n; i++) {
    counter++;  // 正确
}
```

2. **死锁**：
```c
// 错误：可能死锁
#pragma omp parallel
{
    #pragma omp critical
    {
        #pragma omp critical
        {
            // 嵌套critical可能导致死锁
        }
    }
}

// 正确：避免嵌套critical
#pragma omp parallel
{
    #pragma omp critical
    {
        // 单一critical区域
    }
}
```

3. **负载不均**：
```c
// 错误：静态调度导致负载不均
#pragma omp parallel for schedule(static)
for (int i = 0; i < n; i++) {
    if (i % 2 == 0) {
        heavy_computation();  // 重计算
    } else {
        light_computation();  // 轻计算
    }
}

// 正确：动态调度平衡负载
#pragma omp parallel for schedule(dynamic)
for (int i = 0; i < n; i++) {
    if (i % 2 == 0) {
        heavy_computation();
    } else {
        light_computation();
    }
}
```

这个扩展为读者提供了全面的OpenMP基础学习内容，从基本概念到实际应用，从编译配置到性能优化，涵盖了OpenMP编程的核心知识点。

### 11.2 并行区域指令

#### 11.2.1 parallel指令详解
**基本parallel指令**：
```c
#include <omp.h>
#include <stdio.h>

void basic_parallel_example() {
    printf("Before parallel region (thread %d)\n", omp_get_thread_num());

    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();
        printf("Inside parallel region: thread %d of %d\n", thread_id, num_threads);
    }

    printf("After parallel region (thread %d)\n", omp_get_thread_num());
}
```

**parallel指令的执行模型**：
```c
void parallel_execution_model() {
    printf("1. Master thread (thread 0) executing\n");

    #pragma omp parallel num_threads(4)
    {
        int thread_id = omp_get_thread_num();
        printf("2. Thread %d executing parallel region\n", thread_id);

        #pragma omp barrier
        printf("3. Thread %d passed barrier\n", thread_id);

        // 隐式屏障：所有线程必须到达这里才能继续
    }

    printf("4. Master thread continues (only master thread here)\n");
}
```

**parallel指令的子句**：
```c
void parallel_clauses_example() {
    int shared_var = 10;
    int private_var;

    // num_threads: 指定线程数
    #pragma omp parallel num_threads(omp_get_max_threads())
    {
        printf("Thread %d of %d\n", omp_get_thread_num(), omp_get_num_threads());
    }

    // if: 条件并行
    #pragma omp parallel if(num_threads > 1)
    {
        printf("Conditional parallel: thread %d\n", omp_get_thread_num());
    }

    // private: 私有变量
    #pragma omp parallel private(private_var)
    {
        private_var = omp_get_thread_num() * 10;
        printf("Private var for thread %d: %d\n", omp_get_thread_num(), private_var);
    }

    // firstprivate: 初始化私有变量
    #pragma omp parallel firstprivate(shared_var)
    {
        printf("Firstprivate var for thread %d: %d\n", omp_get_thread_num(), shared_var);
        shared_var += 100;  // 修改不影响原变量
    }

    // shared: 显式指定共享变量
    #pragma omp parallel shared(shared_var)
    {
        #pragma omp critical
        {
            printf("Shared var access by thread %d: %d\n", omp_get_thread_num(), shared_var);
            shared_var += 10;
        }
    }

    printf("Final shared_var: %d\n", shared_var);
}
```

#### 11.2.2 work-sharing指令
**for指令**：
```c
void for_directive_example() {
    const int N = 1000;
    int array[N];
    int sum = 0;

    // 初始化数组
    for (int i = 0; i < N; i++) {
        array[i] = i;
    }

    // 基本for并行化
    #pragma omp parallel for
    for (int i = 0; i < N; i++) {
        array[i] *= 2;
    }

    // 带reduction的for
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < N; i++) {
        sum += array[i];
    }

    printf("Sum: %d\n", sum);

    // 带调度策略的for
    #pragma omp parallel for schedule(dynamic, 50)
    for (int i = 0; i < N; i++) {
        printf("Thread %d processing iteration %d\n", omp_get_thread_num(), i);
    }
}
```

**sections指令**：
```c
void sections_directive_example() {
    #pragma omp parallel sections
    {
        #pragma omp section
        {
            printf("Section 1: thread %d\n", omp_get_thread_num());
            // 任务1：数据预处理
            for (int i = 0; i < 1000000; i++) {
                // 模拟计算
            }
        }

        #pragma omp section
        {
            printf("Section 2: thread %d\n", omp_get_thread_num());
            // 任务2：数值计算
            for (int i = 0; i < 1000000; i++) {
                // 模拟计算
            }
        }

        #pragma omp section
        {
            printf("Section 3: thread %d\n", omp_get_thread_num());
            // 任务3：结果后处理
            for (int i = 0; i < 1000000; i++) {
                // 模拟计算
            }
        }
    }
}
```

**single和master指令**：
```c
void single_master_example() {
    #pragma omp parallel
    {
        // single: 任意线程执行，其他等待
        #pragma omp single
        {
            printf("Single region: executed by thread %d\n", omp_get_thread_num());
            // 初始化工作，如读取配置文件
        }

        printf("After single: thread %d continues\n", omp_get_thread_num());

        // master: 仅主线程执行，无等待
        #pragma omp master
        {
            printf("Master region: executed by thread %d\n", omp_get_thread_num());
            // 主线程专用工作
        }

        printf("After master: thread %d continues\n", omp_get_thread_num());
    }
}
```

#### 11.2.3 同步指令
**barrier指令**：
```c
void barrier_example() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();

        printf("Thread %d: phase 1 start\n", thread_id);
        // 第一阶段工作
        for (int i = 0; i < 1000000; i++) {
            // 模拟工作
        }
        printf("Thread %d: phase 1 complete\n", thread_id);

        // 显式屏障
        #pragma omp barrier

        printf("Thread %d: phase 2 start\n", thread_id);
        // 第二阶段工作（依赖第一阶段）
        for (int i = 0; i < 1000000; i++) {
            // 模拟工作
        }
        printf("Thread %d: phase 2 complete\n", thread_id);
    }
}
```

**critical指令**：
```c
void critical_example() {
    int shared_counter = 0;
    int results[1000];

    #pragma omp parallel for
    for (int i = 0; i < 1000; i++) {
        int result = compute_something(i);

        // 保护共享资源
        #pragma omp critical
        {
            results[shared_counter] = result;
            shared_counter++;
        }
    }

    printf("Processed %d results\n", shared_counter);

    // 带名称的critical区域
    int file_counter = 0;
    int db_counter = 0;

    #pragma omp parallel for
    for (int i = 0; i < 1000; i++) {
        #pragma omp critical(file_write)
        {
            // 写文件操作
            file_counter++;
        }

        #pragma omp critical(database_write)
        {
            // 数据库操作
            db_counter++;
        }
    }
}
```

**atomic指令**：
```c
void atomic_example() {
    long long counter = 0;
    double sum = 0.0;

    #pragma omp parallel for
    for (int i = 0; i < 1000000; i++) {
        // 原子递增
        #pragma omp atomic
        counter++;

        // 原子加法
        #pragma omp atomic
        sum += 1.0 / (i + 1);
    }

    printf("Counter: %lld\n", counter);
    printf("Sum: %f\n", sum);
}
```

**flush指令**：
```c
void flush_example() {
    int flag = 0;
    int data = 0;

    #pragma omp parallel
    {
        if (omp_get_thread_num() == 0) {
            // 生产者线程
            data = compute_data();
            #pragma omp flush(data)  // 确保data写入内存
            flag = 1;
            #pragma omp flush(flag)  // 确保flag写入内存
        } else {
            // 消费者线程
            while (1) {
                #pragma omp flush(flag)  // 读取flag的最新值
                if (flag == 1) {
                    #pragma omp flush(data)  // 读取data的最新值
                    process_data(data);
                    break;
                }
            }
        }
    }
}
```

#### 11.2.4 任务并行
**task指令**：
```c
void task_example() {
    #pragma omp parallel
    {
        #pragma omp single
        {
            printf("Creating tasks...\n");

            // 递归任务创建
            create_tasks(10);

            printf("All tasks created\n");
        }
    }
}

void create_tasks(int n) {
    if (n > 0) {
        #pragma omp task
        {
            printf("Task for n=%d, thread=%d\n", n, omp_get_thread_num());
            // 执行任务
            process_data(n);

            // 递归创建子任务
            create_tasks(n - 1);
        }
    }
}
```

**taskwait指令**：
```c
void taskwait_example() {
    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp task
            {
                printf("Task 1: thread %d\n", omp_get_thread_num());
                sleep(1);  // 模拟耗时操作
            }

            #pragma omp task
            {
                printf("Task 2: thread %d\n", omp_get_thread_num());
                sleep(1);  // 模拟耗时操作
            }

            // 等待所有任务完成
            #pragma omp taskwait
            printf("All tasks completed\n");

            #pragma omp task
            {
                printf("Task 3 (after wait): thread %d\n", omp_get_thread_num());
            }
        }
    }
}
```

**taskgroup指令**：
```c
void taskgroup_example() {
    #pragma omp parallel
    {
        #pragma omp single
        {
            #pragma omp taskgroup
            {
                #pragma omp task
                {
                    printf("Group task 1: thread %d\n", omp_get_thread_num());
                    create_subtasks(5);
                }

                #pragma omp task
                {
                    printf("Group task 2: thread %d\n", omp_get_thread_num());
                    create_subtasks(5);
                }

                // taskgroup确保所有任务（包括子任务）完成
            }

            printf("Taskgroup completed\n");
        }
    }
}

void create_subtasks(int n) {
    if (n > 0) {
        #pragma omp task
        {
            printf("Subtask for n=%d\n", n);
            create_subtasks(n - 1);
        }
    }
}
```

#### 11.2.5 数据环境管理
**数据共享子句**：
```c
void data_sharing_clauses() {
    int shared_var = 10;
    int private_var = 20;
    int firstprivate_var = 30;
    int lastprivate_var = 40;
    int reduction_var = 0;

    #pragma omp parallel \
        private(private_var) \
        firstprivate(firstprivate_var) \
        lastprivate(lastprivate_var) \
        reduction(+:reduction_var)
    {
        printf("Thread %d: private_var=%d, firstprivate_var=%d\n",
               omp_get_thread_num(), private_var, firstprivate_var);

        private_var = omp_get_thread_num();
        firstprivate_var += omp_get_thread_num();
        lastprivate_var = omp_get_thread_num();
        reduction_var += omp_get_thread_num();
    }

    printf("After parallel: shared_var=%d, private_var=%d, "
           "firstprivate_var=%d, lastprivate_var=%d, reduction_var=%d\n",
           shared_var, private_var, firstprivate_var,
           lastprivate_var, reduction_var);
}
```

**copyin和copyprivate**：
```c
void copyin_copyprivate_example() {
    static int thread_local_var = 0;  // 静态变量

    #pragma omp parallel copyin(thread_local_var)
    {
        printf("Thread %d: thread_local_var=%d\n",
               omp_get_thread_num(), thread_local_var);

        thread_local_var += omp_get_thread_num();

        #pragma omp single
        {
            printf("Single region: thread_local_var=%d\n", thread_local_var);
            thread_local_var *= 10;

            #pragma omp copyprivate(thread_local_var)
        }

        printf("After copyprivate: thread_local_var=%d\n", thread_local_var);
    }

    printf("Final thread_local_var: %d\n", thread_local_var);
}
```

#### 11.2.6 嵌套并行
**嵌套并行控制**：
```c
void nested_parallel_example() {
    // 启用嵌套并行
    omp_set_nested(1);

    #pragma omp parallel num_threads(2)
    {
        printf("Outer thread %d\n", omp_get_thread_num());

        #pragma omp parallel num_threads(3)
        {
            printf("  Inner thread %d (outer %d)\n",
                   omp_get_thread_num(), omp_get_ancestor_thread_num(1));
        }
    }

    // 禁用嵌套并行
    omp_set_nested(0);
}
```

**嵌套并行的性能考虑**：
```c
void nested_parallel_performance() {
    int matrix[100][100];
    int result[100][100];

    // 初始化矩阵
    for (int i = 0; i < 100; i++) {
        for (int j = 0; j < 100; j++) {
            matrix[i][j] = i + j;
        }
    }

    // 外层并行：行处理
    #pragma omp parallel for
    for (int i = 0; i < 100; i++) {
        // 内层并行：列处理
        #pragma omp parallel for
        for (int j = 0; j < 100; j++) {
            result[i][j] = matrix[i][j] * 2;
        }
    }

    printf("Nested parallel matrix processing completed\n");
}
```

#### 11.2.7 实际应用示例：并行快速排序
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

#define THRESHOLD 1000  // 串行处理的阈值

void parallel_quicksort(int* array, int left, int right) {
    if (left < right) {
        if (right - left < THRESHOLD) {
            // 小数组使用串行快速排序
            serial_quicksort(array, left, right);
        } else {
            int pivot_index = partition(array, left, right);

            #pragma omp task
            {
                parallel_quicksort(array, left, pivot_index - 1);
            }

            #pragma omp task
            {
                parallel_quicksort(array, pivot_index + 1, right);
            }

            #pragma omp taskwait;
        }
    }
}

void parallel_quicksort_driver(int* array, int size) {
    #pragma omp parallel
    {
        #pragma omp single
        {
            parallel_quicksort(array, 0, size - 1);
        }
    }
}

// 辅助函数
int partition(int* array, int left, int right) {
    int pivot = array[right];
    int i = left - 1;

    for (int j = left; j < right; j++) {
        if (array[j] <= pivot) {
            i++;
            swap(&array[i], &array[j]);
        }
    }
    swap(&array[i + 1], &array[right]);
    return i + 1;
}

void swap(int* a, int* b) {
    int temp = *a;
    *a = *b;
    *b = temp;
}

void serial_quicksort(int* array, int left, int right) {
    if (left < right) {
        int pivot_index = partition(array, left, right);
        serial_quicksort(array, left, pivot_index - 1);
        serial_quicksort(array, pivot_index + 1, right);
    }
}
```

#### 11.2.8 实际应用示例：并行归并排序
```c
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define THRESHOLD 1000

void parallel_merge_sort(int* array, int left, int right) {
    if (left < right) {
        if (right - left < THRESHOLD) {
            // 小数组使用串行归并排序
            serial_merge_sort(array, left, right);
        } else {
            int mid = left + (right - left) / 2;

            #pragma omp task
            {
                parallel_merge_sort(array, left, mid);
            }

            #pragma omp task
            {
                parallel_merge_sort(array, mid + 1, right);
            }

            #pragma omp taskwait;

            merge(array, left, mid, right);
        }
    }
}

void merge(int* array, int left, int mid, int right) {
    int n1 = mid - left + 1;
    int n2 = right - mid;

    int* left_arr = malloc(n1 * sizeof(int));
    int* right_arr = malloc(n2 * sizeof(int));

    // 复制数据
    for (int i = 0; i < n1; i++) {
        left_arr[i] = array[left + i];
    }
    for (int j = 0; j < n2; j++) {
        right_arr[j] = array[mid + 1 + j];
    }

    // 合并
    int i = 0, j = 0, k = left;
    while (i < n1 && j < n2) {
        if (left_arr[i] <= right_arr[j]) {
            array[k] = left_arr[i];
            i++;
        } else {
            array[k] = right_arr[j];
            j++;
        }
        k++;
    }

    // 复制剩余元素
    while (i < n1) {
        array[k] = left_arr[i];
        i++; k++;
    }
    while (j < n2) {
        array[k] = right_arr[j];
        j++; k++;
    }

    free(left_arr);
    free(right_arr);
}
```

#### 11.2.9 指令组合使用
**复杂指令组合**：
```c
void complex_directive_combination() {
    const int N = 10000;
    double data[N];
    double results[N];

    // 初始化数据
    for (int i = 0; i < N; i++) {
        data[i] = i * 0.1;
    }

    #pragma omp parallel
    {
        // 每个线程的私有累加器
        double local_sum = 0.0;
        int local_count = 0;

        #pragma omp for schedule(dynamic, 100)
        for (int i = 0; i < N; i++) {
            double value = data[i];
            double result = compute_function(value);

            // 保护共享数组的写入
            #pragma omp critical
            {
                results[i] = result;
            }

            // 局部累加
            local_sum += result;
            local_count++;
        }

        // 收集局部结果
        #pragma omp critical(sum_collection)
        {
            static double global_sum = 0.0;
            static int global_count = 0;

            global_sum += local_sum;
            global_count += local_count;

            if (global_count == N) {
                printf("Global sum: %f, average: %f\n",
                       global_sum, global_sum / N);
            }
        }
    }
}

double compute_function(double x) {
    return sin(x) * cos(x) + log(x + 1);
}
```

#### 11.2.10 性能优化技巧
**指令选择策略**：
```c
void optimization_strategies() {
    const int N = 1000000;
    int array[N];

    // 1. 负载均衡：使用动态调度
    #pragma omp parallel for schedule(dynamic, 1000)
    for (int i = 0; i < N; i++) {
        // 不均匀的工作负载
        process_element(array[i], i);
    }

    // 2. 减少同步开销：使用reduction
    double sum1 = 0.0;
    #pragma omp parallel for reduction(+:sum1)
    for (int i = 0; i < N; i++) {
        sum1 += array[i] * array[i];
    }

    // 3. 避免false sharing
    #pragma omp parallel
    {
        // 每个线程使用独立的缓存行
        __attribute__((aligned(64))) double local_sum;
        #pragma omp for
        for (int i = 0; i < N; i++) {
            local_sum += array[i];
        }
        // 最后合并结果
    }

    // 4. 任务并行：处理不规则工作
    #pragma omp parallel
    {
        #pragma omp single
        {
            for (int i = 0; i < 100; i++) {
                #pragma omp task
                {
                    process_irregular_task(i);
                }
            }
        }
    }
}
```

**内存访问优化**：
```c
void memory_access_optimization() {
    const int ROWS = 1000, COLS = 1000;
    double matrix[ROWS][COLS];
    double result[ROWS][COLS];

    // 优化内存访问模式
    #pragma omp parallel for schedule(static)
    for (int i = 0; i < ROWS; i++) {
        for (int j = 0; j < COLS; j++) {
            // 访问相邻内存位置
            result[i][j] = matrix[i][j] * 2.0;
        }
    }

    // 避免跨步访问
    #pragma omp parallel for schedule(static)
    for (int j = 0; j < COLS; j++) {
        for (int i = 0; i < ROWS; i++) {
            // 不好的访问模式：跨步访问
            // result[i][j] = matrix[i][j] * 2.0;
        }
    }
}
```

这个扩展为读者提供了全面的OpenMP并行区域指令学习内容，包括各种指令的详细用法、实际应用示例和性能优化技巧。

### 11.3 数据共享属性
```c
// 数据共享控制
#pragma omp parallel for private(i) shared(array) reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += array[i];
}

// 默认数据属性
#pragma omp parallel default(shared)
{
    // 默认所有变量共享
}

// 线程本地存储
#pragma omp threadprivate(counter)
```

### 11.4 同步指令
```c
// 关键区段
#pragma omp critical
{
    // 互斥执行
}

// 原子操作
#pragma omp atomic
counter++;

// 屏障同步
#pragma omp barrier;

// 单线程执行
#pragma omp single
{
    // 只有一个线程执行
}

// 主线程执行
#pragma omp master
{
    // 只有主线程执行
}
```

### 11.5 任务并行
```c
// 任务创建
#pragma omp task
{
    // 任务代码
}

// 任务依赖
#pragma omp task depend(in: a) depend(out: b)
{
    // 依赖任务
}

// 任务等待
#pragma omp taskwait;

// 任务组
#pragma omp taskgroup
{
    // 任务组代码
}
```

## 第12章 CUDA/GPU计算

### 12.1 CUDA架构基础

#### 12.1.1 GPU硬件架构
**GPU vs CPU架构对比**：
```
CPU架构：
┌─────────────────────────────────────────┐
│           少量核心 + 大缓存              │
│           复杂控制逻辑                   │
│           高单线程性能                   │
└─────────────────────────────────────────┘

GPU架构：
┌─────────────────────────────────────────┐
│        大量简单核心 + 高并行度            │
│        简化控制逻辑                       │
│        专为并行计算优化                   │
└─────────────────────────────────────────┘
```

**现代GPU架构组件**：
- **GPU Die**：包含多个GPC（Graphics Processing Clusters）
- **GPC**：图形处理集群，包含多个SM
- **SM**：流多处理器，执行并行线程
- **Memory Controllers**：内存控制器，连接显存
- **PCIe Interface**：与CPU通信接口

#### 12.1.2 Streaming Multiprocessor (SM)
**SM架构详解**：
```c
// SM内部结构
┌─────────────────────────────────────────┐
│              SM (流多处理器)              │
├─────────────────────────────────────────┤
│  ├── CUDA Core (流处理器) - 128-1024个   │
│  ├── Tensor Core (张量核心) - AI加速      │
│  ├── Special Function Unit (SFU)         │
│  ├── Warp Scheduler (线程束调度器)        │
│  ├── Dispatch Unit (分发单元)             │
│  ├── Shared Memory (共享内存) - 16-192KB  │
│  └── Register File (寄存器文件)           │
│      ├── 每个SM 64KB-32768KB寄存器        │
│      └── 每个线程块最多65536个寄存器      │
└─────────────────────────────────────────┘
```

**SM工作原理**：
1. **Warp调度**：SM以warp（32个线程）为单位调度
2. **SIMT执行**：单指令多线程，warp内线程执行相同指令
3. **隐藏延迟**：通过切换warp隐藏内存访问延迟
4. **资源分配**：动态分配寄存器和共享内存

**SM资源计算示例**：
```c
// NVIDIA A100 SM资源
- CUDA核心：64个
- Tensor核心：4个
- 最大并发warp：64个
- 最大并发线程：2048个（64 warp × 32 threads）
- 共享内存：164KB
- 寄存器文件：65536个32位寄存器
```

#### 12.1.3 线程层次结构
**CUDA线程组织**：
```
┌─────────────────────────────────────────┐
│                 Grid                    │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐   │
│  │ Block 0 │ │ Block 1 │ │ Block 2 │   │
│  │         │ │         │ │         │   │
│  │ ┌─────┐ │ │ ┌─────┐ │ │ ┌─────┐ │   │
│  │ │Thread│ │ │ │Thread│ │ │ │Thread│ │   │
│  │ │  0  │ │ │ │  0  │ │ │ │  0  │ │   │
│  │ │     │ │ │ │     │ │ │ │     │ │   │
│  │ │ ... │ │ │ │ ... │ │ │ │ ... │ │   │
│  │ │     │ │ │ │     │ │ │ │     │ │   │
│  │ │ 1023│ │ │ │ 1023│ │ │ │ 1023│ │   │
│  │ └─────┘ │ │ └─────┘ │ │ └─────┘ │   │
│  └─────────┘ └─────────┘ └─────────┘   │
│                ...                      │
└─────────────────────────────────────────┘
```

**线程标识系统**：
```cuda
// 线程标识变量
extern __shared__ int shared_data[];

__global__ void example_kernel() {
    // 一维索引
    int global_thread_id = blockIdx.x * blockDim.x + threadIdx.x;

    // 二维索引
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    // 三维索引
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int idy = blockIdx.y * blockDim.y + threadIdx.y;
    int idz = blockIdx.z * blockDim.z + threadIdx.z;

    // 线程块内索引
    int local_thread_id = threadIdx.x + threadIdx.y * blockDim.x;

    // 线程块索引
    int block_id = blockIdx.x + blockIdx.y * gridDim.x;
}
```

**线程层次示例**：
```cuda
// 配置示例：1024个线程，32个线程块，每块32个线程
dim3 block_size(32);
dim3 grid_size(32);

example_kernel<<<grid_size, block_size>>>();

// 线程映射关系：
// Grid: 32 blocks (0-31)
// Block: 32 threads each (0-31)
// Global thread IDs: 0-1023
```

#### 12.1.4 内存层次结构
**CUDA内存类型**：
```c
┌─────────────────────────────────────────┐
│              全局内存 (Global)           │
│  ────────────────────────────────────   │
│              常量内存 (Constant)         │
│  ────────────────────────────────────   │
│              纹理内存 (Texture)          │
│  ────────────────────────────────────   │
│              共享内存 (Shared)           │
│  ────────────────────────────────────   │
│              寄存器 (Register)           │
│  ────────────────────────────────────   │
│              本地内存 (Local)            │
│  ────────────────────────────────────   │
│              只读内存 (Read-only)        │
└─────────────────────────────────────────┘
```

**内存特性对比**：
```c
内存类型        | 访问速度    | 容量       | 生命周期     | 访问范围
---------------|------------|------------|-------------|----------
寄存器         | 最快        | 有限       | Kernel      | 单线程
共享内存       | 快         | 中等       | Kernel      | 线程块
全局内存       | 慢         | 最大       | 应用程序    | 全局
常量内存       | 快         | 小         | 应用程序    | 全局
纹理内存       | 快         | 中等       | 应用程序    | 全局
本地内存       | 慢         | 有限       | 函数调用    | 单线程
```

**内存使用示例**：
```cuda
// 寄存器变量（自动分配）
__global__ void register_example() {
    int local_var = threadIdx.x;  // 通常存储在寄存器
}

// 共享内存
__global__ void shared_memory_example() {
    __shared__ float shared_data[256];  // 每个block共享

    int tid = threadIdx.x;
    shared_data[tid] = tid * 2.0f;
    __syncthreads();  // 同步

    // 使用共享数据
    float result = shared_data[tid];
}

// 全局内存
__global__ void global_memory_example(float *input, float *output) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    output[idx] = input[idx] * 2.0f;  // 访问全局内存
}

// 常量内存
__constant__ float const_data[256];

__global__ void constant_memory_example() {
    float value = const_data[threadIdx.x];  // 访问常量内存
}
```

#### 12.1.5 CUDA核心与Tensor核心
**CUDA核心**：
- **功能**：执行基本的浮点和整数运算
- **类型**：
  - FP32：单精度浮点运算
  - FP64：双精度浮点运算
  - INT32：32位整数运算
- **性能特点**：
  - 高吞吐量
  - 适合通用计算

**Tensor核心**：
- **功能**：专门用于矩阵运算，特别是深度学习
- **运算类型**：
  - 4×4矩阵乘法累加
  - FP16、INT8、INT4等低精度运算
- **性能优势**：
  - 相比CUDA核心高达12倍的吞吐量
  - 专为深度学习优化

**核心使用示例**：
```cuda
// CUDA核心示例：传统浮点运算
__global__ void cuda_core_kernel(float *a, float *b, float *c) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    c[idx] = a[idx] + b[idx] * 2.0f;  // 使用CUDA核心
}

// Tensor核心示例：使用WMMA API
#include <mma.h>

__global__ void tensor_core_kernel(half *a, half *b, half *c) {
    // 使用Tensor核心进行矩阵乘法
    // 需要特定的内存布局和数据类型
}
```

#### 12.1.6 内存访问模式优化
**内存合并访问**：
```cuda
// 好的访问模式：合并访问
__global__ void good_memory_access(float *input, float *output) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 连续的内存访问（合并）
    float value = input[idx];  // 线程0访问input[0], 线程1访问input[1], ...
    output[idx] = value * 2.0f;  // 合并写入
}

// 坏的访问模式：分散访问
__global__ void bad_memory_access(float *input, float *output) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 跨步访问（不合并）
    float value = input[idx * 2];  // 线程0访问input[0], 线程1访问input[2], ...
    output[idx * 2] = value * 2.0f;  // 分散写入
}
```

**共享内存优化**：
```cuda
// 共享内存bank冲突避免
__global__ void shared_memory_optimization() {
    __shared__ float shared_data[32][33];  // 添加padding避免bank冲突

    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // 无bank冲突访问
    float value = shared_data[ty][tx];
    shared_data[ty][tx] = value * 2.0f;
}
```

#### 12.1.7 实际架构示例：NVIDIA A100
**A100架构规格**：
```c
// A100 GPU规格
- 架构：Ampere
- SM数量：108个
- CUDA核心总数：6912个（108 × 64）
- Tensor核心：432个（108 × 4）
- 基础频率：1.41 GHz
- 峰值FP32性能：19.5 TFLOPS
- 峰值FP64性能：9.7 TFLOPS
- 峰值Tensor性能：312 TFLOPS（FP16）
- 显存：40GB HBM2
- 显存带宽：1.6 TB/s
- TDP：400W
```

**A100 SM内部结构**：
```c
每个SM包含：
├── 64个FP32 CUDA核心
├── 64个INT32核心
├── 4个Tensor核心
├── 4个特殊功能单元(SFU)
├── 4个纹理单元
├── 128KB寄存器文件
├── 164KB共享内存/L1缓存
├── 4个warp调度器
└── 8个指令分发单元
```

#### 12.1.8 性能优化指导原则
**内存访问优化**：
1. **合并访问**：确保warp内线程访问连续内存
2. **共享内存使用**：重用数据，减少全局内存访问
3. **内存对齐**：使用对齐的内存访问模式
4. **缓存利用**：合理使用L1/L2缓存

**计算优化**：
1. **指令级并行**：重叠计算和内存操作
2. **warp效率**：避免分支发散
3. **资源利用**：最大化SM占用率
4. **精度选择**：根据需求选择合适的数据类型

**示例优化代码**：
```cuda
// 优化的矩阵乘法
__global__ void optimized_matrix_mult(float *A, float *B, float *C,
                                    int N, int M, int K) {
    // 使用共享内存减少全局内存访问
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;

    int row = by * BLOCK_SIZE + ty;
    int col = bx * BLOCK_SIZE + tx;

    float sum = 0.0f;

    // 分块计算
    for (int p = 0; p < (M + BLOCK_SIZE - 1) / BLOCK_SIZE; ++p) {
        // 加载到共享内存
        if (row < N && p * BLOCK_SIZE + tx < M)
            As[ty][tx] = A[row * M + p * BLOCK_SIZE + tx];
        else
            As[ty][tx] = 0.0f;

        if (col < K && p * BLOCK_SIZE + ty < M)
            Bs[ty][tx] = B[(p * BLOCK_SIZE + ty) * K + col];
        else
            Bs[ty][tx] = 0.0f;

        __syncthreads();

        // 计算部分结果
        for (int k = 0; k < BLOCK_SIZE; ++k)
            sum += As[ty][k] * Bs[k][tx];

        __syncthreads();
    }

    // 写回结果
    if (row < N && col < K)
        C[row * K + col] = sum;
}
```

#### 12.1.9 CUDA架构演进
**主要架构版本**：
```c
Tesla (2006)     → 早期CUDA架构
Fermi (2010)     → 改进的内存层次和缓存
Kepler (2012)    → 能效优化
Maxwell (2014)   → 内存压缩和能效
Pascal (2016)    → HBM2和NVLink
Volta (2017)     → Tensor核心引入
Turing (2018)    → RT核心和DLSS
Ampere (2020)    → 第二代Tensor核心
Ada Lovelace (2022) → 第三代RT核心
Hopper (2022)    → H100，专业AI架构
```

**架构改进趋势**：
- **并行度增加**：更多SM和CUDA核心
- **内存带宽提升**：HBM技术，更高带宽
- **专用硬件**：Tensor核心、RT核心
- **能效优化**：每瓦特性能持续提升
- **编程模型**：更简单的并行编程接口

### 12.2 Kernel函数

#### 12.2.1 Kernel函数基础
**Kernel函数定义**：
```cuda
// Kernel函数语法
__global__ void function_name(parameters) {
    // Kernel代码
}

// 示例：向量加法
__global__ void vectorAdd(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
```

**Kernel函数特性**：
- **执行位置**：在GPU设备上执行
- **调用位置**：从主机（CPU）代码调用
- **并行执行**：多个线程同时执行
- **内存空间**：访问设备内存

**CUDA函数类型修饰符**：
```cuda
// 在主机上执行，从主机调用
__host__ void host_function() {
    // 主机代码
}

// 在设备上执行，从设备调用
__device__ void device_function() {
    // 设备代码
}

// 在设备上执行，从主机调用（Kernel函数）
__global__ void kernel_function() {
    // Kernel代码
}

// 同时在主机和设备上编译
__host__ __device__ void both_function() {
    // 通用代码
}
```

#### 12.2.2 Kernel配置和执行
**Kernel配置语法**：
```cuda
// 基本语法
kernel_name<<<grid_size, block_size, shared_memory_size, stream>>>(parameters);

// 参数说明
// grid_size: 网格维度（dim3）
// block_size: 线程块维度（dim3）
// shared_memory_size: 共享内存大小（字节）
// stream: CUDA流（可选）
```

**配置参数详解**：
```cuda
// 一维配置
dim3 block_size(256);      // 每块256个线程
dim3 grid_size(1024);      // 1024个线程块
kernel<<<grid_size, block_size>>>(data);

// 二维配置
dim3 block_size(16, 16);   // 16x16 = 256个线程/块
dim3 grid_size(64, 64);    // 64x64 = 4096个线程块
kernel<<<grid_size, block_size>>>(matrix);

// 三维配置
dim3 block_size(8, 8, 8);  // 8x8x8 = 512个线程/块
dim3 grid_size(16, 16, 16); // 16x16x16 = 4096个线程块
kernel<<<grid_size, block_size>>>(volume);
```

**配置计算示例**：
```cuda
// 计算合适的配置
void calculate_launch_config(int total_threads, dim3 &grid_size, dim3 &block_size) {
    // 选择线程块大小（通常为2的幂，最大1024）
    block_size.x = 256;
    block_size.y = 1;
    block_size.z = 1;

    // 计算网格大小
    grid_size.x = (total_threads + block_size.x - 1) / block_size.x;
    grid_size.y = 1;
    grid_size.z = 1;

    printf("配置: %dx%d 线程块, %dx%d 网格\n",
           block_size.x, block_size.y, grid_size.x, grid_size.y);
}

// 使用示例
int total_elements = 1000000;
dim3 grid, block;
calculate_launch_config(total_elements, grid, block);
vectorAdd<<<grid, block>>>(d_a, d_b, d_c, total_elements);
```

#### 12.2.3 线程索引计算
**一维索引**：
```cuda
__global__ void process_1d_array(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 处理data[idx]
        data[idx] *= 2.0f;
    }
}
```

**二维索引**：
```cuda
__global__ void process_2d_array(float *matrix, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        int idx = y * width + x;
        matrix[idx] *= 2.0f;
    }
}
```

**三维索引**：
```cuda
__global__ void process_3d_volume(float *volume, int width, int height, int depth) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    int z = blockIdx.z * blockDim.z + threadIdx.z;

    if (x < width && y < height && z < depth) {
        int idx = z * width * height + y * width + x;
        volume[idx] *= 2.0f;
    }
}
```

**高级索引模式**：
```cuda
// 行优先访问
__global__ void row_major_access(float *matrix, int rows, int cols) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < rows && col < cols) {
        matrix[row * cols + col] = row + col;
    }
}

// 列优先访问
__global__ void column_major_access(float *matrix, int rows, int cols) {
    int col = blockIdx.y * blockDim.y + threadIdx.y;
    int row = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < rows && col < cols) {
        matrix[col * rows + row] = row + col;
    }
}
```

#### 12.2.4 Kernel内存管理
**共享内存使用**：
```cuda
__global__ void shared_memory_kernel(int *input, int *output, int n) {
    // 声明共享内存
    extern __shared__ int shared_data[];

    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int block_size = blockDim.x;

    // 加载数据到共享内存
    int idx = bid * block_size + tid;
    if (idx < n) {
        shared_data[tid] = input[idx];
    } else {
        shared_data[tid] = 0;
    }

    // 同步所有线程
    __syncthreads();

    // 处理共享内存中的数据
    if (tid > 0) {
        shared_data[tid] += shared_data[tid - 1];
    }

    // 同步
    __syncthreads();

    // 写回结果
    if (idx < n) {
        output[idx] = shared_data[tid];
    }
}
```

**动态共享内存分配**：
```cuda
// Kernel声明时指定动态共享内存大小
__global__ void dynamic_shared_kernel(int n) {
    extern __shared__ float shared_data[];
    // shared_data大小在Kernel调用时指定
}

// 调用时指定共享内存大小
int shared_mem_size = 1024 * sizeof(float);
dynamic_shared_kernel<<<grid_size, block_size, shared_mem_size>>>(n);
```

**常量内存使用**：
```cuda
// 声明常量内存
__constant__ float const_matrix[256];

// 主机代码中复制数据到常量内存
float h_matrix[256];
// ... 初始化h_matrix ...
cudaMemcpyToSymbol(const_matrix, h_matrix, 256 * sizeof(float));

// Kernel中使用常量内存
__global__ void constant_memory_kernel(float *result) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < 256) {
        result[idx] = const_matrix[idx] * 2.0f;
    }
}
```

#### 12.2.5 Kernel同步和通信
**线程块内同步**：
```cuda
__global__ void block_synchronization_example(float *data, int n) {
    int tid = threadIdx.x;
    int bid = blockIdx.x;

    // 每个线程块处理一部分数据
    int start_idx = bid * blockDim.x;
    int end_idx = min(start_idx + blockDim.x, n);

    // 计算局部和
    float local_sum = 0.0f;
    for (int i = start_idx + tid; i < end_idx; i += blockDim.x) {
        local_sum += data[i];
    }

    // 使用共享内存聚合结果
    __shared__ float shared_sums[256];
    shared_sums[tid] = local_sum;
    __syncthreads();

    // 块内归约
    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared_sums[tid] += shared_sums[tid + stride];
        }
        __syncthreads();
    }

    // 块首线程写入结果
    if (tid == 0) {
        data[bid] = shared_sums[0];
    }
}
```

**Warp级原语**：
```cuda
__global__ void warp_primitives_example(float *data) {
    int tid = threadIdx.x;

    // Warp内求和
    float value = data[tid];
    for (int offset = 16; offset > 0; offset /= 2) {
        value += __shfl_down_sync(0xffffffff, value, offset);
    }

    // 所有线程都有相同的求和结果
    data[tid] = value;
}
```

**原子操作**：
```cuda
__global__ void atomic_operations_example(int *counter, float *histogram, int *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 原子递增
        atomicAdd(counter, 1);

        // 原子加法到直方图
        int bin = data[idx] % 256;
        atomicAdd(&histogram[bin], 1.0f);

        // 原子比较交换
        int expected = 0;
        atomicCAS(counter, expected, 1);
    }
}
```

#### 12.2.6 Kernel优化技术
**内存访问优化**：
```cuda
// 优化的矩阵转置
__global__ void optimized_transpose(float *input, float *output, int width, int height) {
    __shared__ float tile[32][33];  // 添加padding避免bank冲突

    int x = blockIdx.x * 32 + threadIdx.x;
    int y = blockIdx.y * 32 + threadIdx.y;

    // 加载到共享内存（转置顺序）
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }

    __syncthreads();

    // 从共享内存写回（转置）
    x = blockIdx.y * 32 + threadIdx.x;
    y = blockIdx.x * 32 + threadIdx.y;

    if (x < height && y < width) {
        output[y * height + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

**计算优化**：
```cuda
// 使用寄存器优化
__global__ void register_optimized(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 使用寄存器变量减少内存访问
        float reg_val = input[idx];
        reg_val = reg_val * reg_val + reg_val;  // 复杂计算
        output[idx] = reg_val;
    }
}

// 使用向量化加载
__global__ void vectorized_load(float4 *input, float4 *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 一次加载4个float
        float4 data = input[idx];
        data.x *= 2.0f;
        data.y *= 2.0f;
        data.z *= 2.0f;
        data.w *= 2.0f;
        output[idx] = data;
    }
}
```

#### 12.2.7 Kernel错误处理
**异步错误检查**：
```cuda
// Kernel调用
vectorAdd<<<grid_size, block_size>>>(d_a, d_b, d_c, n);

// 检查Kernel启动错误
cudaError_t launch_error = cudaGetLastError();
if (launch_error != cudaSuccess) {
    printf("Kernel启动错误: %s\n", cudaGetErrorString(launch_error));
}

// 同步并检查执行错误
cudaError_t sync_error = cudaDeviceSynchronize();
if (sync_error != cudaSuccess) {
    printf("Kernel执行错误: %s\n", cudaGetErrorString(sync_error));
}
```

**Kernel内错误处理**：
```cuda
__global__ void error_handling_kernel(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 检查边界条件
        if (data[idx] < 0.0f) {
            // 设置错误标志
            data[n] = -1.0f;  // 使用额外位置存储错误
            return;
        }

        // 正常处理
        data[idx] = sqrt(data[idx]);
    }
}
```

#### 12.2.8 实际应用示例
**图像处理Kernel**：
```cuda
// 图像模糊处理
__global__ void gaussian_blur(unsigned char *input, unsigned char *output,
                             int width, int height, int channels) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        int center_idx = (y * width + x) * channels;

        for (int c = 0; c < channels; c++) {
            float sum = 0.0f;
            float weight_sum = 0.0f;

            // 3x3高斯核
            for (int dy = -1; dy <= 1; dy++) {
                for (int dx = -1; dx <= 1; dx++) {
                    int nx = x + dx;
                    int ny = y + dy;

                    if (nx >= 0 && nx < width && ny >= 0 && ny < height) {
                        int neighbor_idx = (ny * width + nx) * channels + c;
                        float pixel = input[neighbor_idx];

                        // 高斯权重
                        float weight = expf(-(dx*dx + dy*dy) / 2.0f);
                        sum += pixel * weight;
                        weight_sum += weight;
                    }
                }
            }

            output[center_idx + c] = (unsigned char)(sum / weight_sum);
        }
    }
}
```

**数值计算Kernel**：
```cuda
// Jacobi迭代求解泊松方程
__global__ void jacobi_iteration(float *u_new, float *u_old, float *f,
                                int width, int height, float h2_inv) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x > 0 && x < width - 1 && y > 0 && y < height - 1) {
        int idx = y * width + x;

        // Jacobi迭代公式
        float u_left = u_old[idx - 1];
        float u_right = u_old[idx + 1];
        float u_up = u_old[idx - width];
        float u_down = u_old[idx + width];

        u_new[idx] = (u_left + u_right + u_up + u_down - f[idx] * h2_inv) * 0.25f;
    }
}
```

**性能分析Kernel**：
```cuda
// 性能测试Kernel
__global__ void performance_test(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 简单的计算密集型操作
        float sum = 0.0f;
        for (int i = 0; i < 1000; i++) {
            sum += sinf(data[idx] * i) * cosf(data[idx] * i);
        }
        data[idx] = sum;
    }
}

// 测试函数
void benchmark_kernel() {
    float *d_data;
    int n = 1000000;

    cudaMalloc(&d_data, n * sizeof(float));

    dim3 block_size(256);
    dim3 grid_size((n + block_size.x - 1) / block_size.x);

    // 记录时间
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    performance_test<<<grid_size, block_size>>>(d_data, n);
    cudaEventRecord(stop);

    cudaEventSynchronize(stop);
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);

    printf("Kernel执行时间: %f ms\n", milliseconds);

    cudaFree(d_data);
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
}
```

#### 12.2.9 Kernel最佳实践
**设计原则**：
1. **最大化并行度**：确保足够的线程和块
2. **内存访问合并**：线程访问连续内存
3. **减少分支发散**：warp内线程执行相同路径
4. **合理使用共享内存**：减少全局内存访问
5. **平衡计算和内存**：避免内存带宽瓶颈

**性能调优检查清单**：
```cuda
// ✅ 好的做法
__global__ void good_kernel(float *data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    __shared__ float cache[256];

    // 合并内存访问
    cache[threadIdx.x] = data[idx];
    __syncthreads();

    // 计算
    float result = cache[threadIdx.x] * 2.0f;

    // 合并写入
    data[idx] = result;
}

// ❌ 需要避免的做法
__global__ void bad_kernel(float *data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 随机内存访问
    int random_idx = (idx * 17) % 1000;
    float value = data[random_idx];

    // 复杂分支
    if (value > 0.5f) {
        value *= 2.0f;
    } else if (value > 0.25f) {
        value *= 1.5f;
    } else {
        value *= 0.5f;
    }

    // 分散写入
    data[random_idx] = value;
}
```

这个扩展为读者提供了全面的CUDA Kernel函数知识，从基础概念到高级优化技术，包含了大量实用的代码示例和最佳实践指导。

### 12.3 内存管理

#### 12.3.1 CUDA内存类型详解
**内存层次结构**：
```
┌─────────────────────────────────────────┐
│              主机内存 (Host)             │
│  ┌─────────────────────────────────────┐ │
│  │          系统内存 (System)           │ │
│  └─────────────────────────────────────┘ │
└─────────────────────────────────────────┘
┌─────────────────────────────────────────┐
│              设备内存 (Device)           │
│  ┌─────────────────────────────────────┐ │
│  │           全局内存 (Global)          │ │
│  ├─────────────────────────────────────┤ │
│  │           常量内存 (Constant)        │ │
│  ├─────────────────────────────────────┤ │
│  │           纹理内存 (Texture)         │ │
│  ├─────────────────────────────────────┤ │
│  │           共享内存 (Shared)          │ │
│  ├─────────────────────────────────────┤ │
│  │           寄存器 (Register)          │ │
│  └─────────────────────────────────────┘ │
└─────────────────────────────────────────┘
```

**内存类型特性对比**：
```c
内存类型      | 位置        | 访问速度   | 容量       | 生命周期     | 访问范围
-------------|------------|-----------|------------|-------------|----------
寄存器       | GPU        | 极快       | 有限       | Kernel      | 单线程
共享内存     | GPU        | 快         | 中等       | Kernel      | 线程块
常量内存     | GPU        | 快         | 小         | 应用程序    | 全局
纹理内存     | GPU        | 快         | 中等       | 应用程序    | 全局
全局内存     | GPU        | 慢         | 最大       | 应用程序    | 全局
主机内存     | CPU        | 慢         | 最大       | 应用程序    | 全局
```

#### 12.3.2 主机内存管理
**标准主机内存**：
```cuda
// 分配主机内存
float *h_data;
size_t size = 1024 * sizeof(float);
h_data = (float*)malloc(size);

// 使用内存
for (int i = 0; i < 1024; i++) {
    h_data[i] = i * 1.0f;
}

// 释放内存
free(h_data);
```

**固定内存（Pinned Memory）**：
```cuda
// 分配固定内存
float *h_pinned_data;
cudaMallocHost((void**)&h_pinned_data, size);

// 使用固定内存
for (int i = 0; i < 1024; i++) {
    h_pinned_data[i] = i * 2.0f;
}

// 释放固定内存
cudaFreeHost(h_pinned_data);

// 固定现有内存
float *h_regular_data = (float*)malloc(size);
cudaHostRegister(h_regular_data, size, cudaHostRegisterDefault);

// 取消固定
cudaHostUnregister(h_regular_data);
```

**统一内存（Unified Memory）**：
```cuda
// 分配统一内存
float *h_unified_data;
cudaMallocManaged((void**)&h_unified_data, size);

// CPU端使用
for (int i = 0; i < 1024; i++) {
    h_unified_data[i] = i * 3.0f;
}

// GPU端使用（在Kernel中）
__global__ void use_unified_memory(float *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] *= 2.0f;
    }
}

// 同步内存访问
cudaDeviceSynchronize();

// 释放统一内存
cudaFree(h_unified_data);
```

#### 12.3.3 设备内存管理
**全局内存分配**：
```cuda
// 分配设备内存
float *d_data;
size_t size = 1024 * sizeof(float);
cudaError_t err = cudaMalloc((void**)&d_data, size);

if (err != cudaSuccess) {
    printf("设备内存分配失败: %s\n", cudaGetErrorString(err));
    return -1;
}

// 使用设备内存
// ... 在Kernel中使用d_data ...

// 释放设备内存
cudaFree(d_data);
```

**多GPU内存管理**：
```cuda
// 设置当前GPU设备
int device_count;
cudaGetDeviceCount(&device_count);
printf("可用GPU数量: %d\n", device_count);

// 为每个GPU分配内存
float **d_data_per_gpu = (float**)malloc(device_count * sizeof(float*));
size_t size = 1024 * sizeof(float);

for (int dev = 0; dev < device_count; dev++) {
    cudaSetDevice(dev);
    cudaMalloc((void**)&d_data_per_gpu[dev], size);
}

// 使用特定GPU的内存
cudaSetDevice(1);  // 使用第二个GPU
// ... 在第二个GPU上执行Kernel ...

// 释放所有GPU内存
for (int dev = 0; dev < device_count; dev++) {
    cudaSetDevice(dev);
    cudaFree(d_data_per_gpu[dev]);
}

cudaSetDevice(0);  // 恢复默认设备
free(d_data_per_gpu);
```

#### 12.3.4 内存拷贝操作
**主机到设备拷贝**：
```cuda
// 同步拷贝
float *h_data, *d_data;
size_t size = 1024 * sizeof(float);

// 分配内存
h_data = (float*)malloc(size);
cudaMalloc((void**)&d_data, size);

// 初始化主机数据
for (int i = 0; i < 1024; i++) {
    h_data[i] = i * 1.0f;
}

// 同步拷贝到设备
cudaError_t err = cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);
if (err != cudaSuccess) {
    printf("主机到设备拷贝失败: %s\n", cudaGetErrorString(err));
}

// 释放内存
cudaFree(d_data);
free(h_data);
```

**设备到主机拷贝**：
```cuda
// 同步拷贝
float *h_result, *d_result;
size_t size = 1024 * sizeof(float);

// 分配内存
h_result = (float*)malloc(size);
cudaMalloc((void**)&d_result, size);

// ... 在设备上计算结果 ...

// 同步拷贝回主机
cudaError_t err = cudaMemcpy(h_result, d_result, size, cudaMemcpyDeviceToHost);
if (err != cudaSuccess) {
    printf("设备到主机拷贝失败: %s\n", cudaGetErrorString(err));
}

// 处理结果
for (int i = 0; i < 1024; i++) {
    printf("结果[%d] = %f\n", i, h_result[i]);
}

// 释放内存
cudaFree(d_result);
free(h_result);
```

**设备间拷贝**：
```cuda
// 设备间直接拷贝
float *d_data_gpu0, *d_data_gpu1;
size_t size = 1024 * sizeof(float);

// 在GPU 0上分配内存
cudaSetDevice(0);
cudaMalloc((void**)&d_data_gpu0, size);

// 在GPU 1上分配内存
cudaSetDevice(1);
cudaMalloc((void**)&d_data_gpu1, size);

// 设置设备间拷贝
cudaSetDevice(0);
cudaMemcpyPeer(d_data_gpu1, 1, d_data_gpu0, 0, size);

// 释放内存
cudaSetDevice(0);
cudaFree(d_data_gpu0);
cudaSetDevice(1);
cudaFree(d_data_gpu1);
```

#### 12.3.5 异步内存操作
**异步内存拷贝**：
```cuda
// 创建CUDA流
cudaStream_t stream;
cudaStreamCreate(&stream);

// 异步内存拷贝
float *h_data, *d_data;
size_t size = 1024 * sizeof(float);

// 分配固定内存（用于异步拷贝）
cudaMallocHost((void**)&h_data, size);
cudaMalloc((void**)&d_data, size);

// 异步拷贝
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);

// 同时可以执行其他操作
// ... 执行其他计算任务 ...

// 等待异步操作完成
cudaStreamSynchronize(stream);

// 释放资源
cudaFreeHost(h_data);
cudaFree(d_data);
cudaStreamDestroy(stream);
```

**异步内存分配**：
```cuda
// 异步内存分配（CUDA 11.2+）
float *d_async_data;
size_t size = 1024 * sizeof(float);
cudaStream_t stream;
cudaStreamCreate(&stream);

// 异步分配
cudaMallocAsync((void**)&d_async_data, size, stream);

// 异步拷贝
float *h_data;
cudaMallocHost((void**)&h_data, size);
cudaMemcpyAsync(d_async_data, h_data, size, cudaMemcpyHostToDevice, stream);

// 同步
cudaStreamSynchronize(stream);

// 异步释放
cudaFreeAsync(d_async_data, stream);

// 释放资源
cudaFreeHost(h_data);
cudaStreamDestroy(stream);
```

#### 12.3.6 内存池管理
**CUDA内存池**：
```cuda
// 创建内存池
cudaMemPool_t mempool;
cudaDeviceGetDefaultMemPool(&mempool, 0);

// 配置内存池
cudaMemPoolAttr pool_attr;
size_t threshold = 1024 * 1024;  // 1MB
cudaMemPoolSetAttribute(mempool, cudaMemPoolAttrReleaseThreshold, &threshold);

// 分配内存池内存
float *d_pooled_data;
size_t size = 1024 * sizeof(float);
cudaMallocFromPoolAsync((void**)&d_pooled_data, size, mempool, 0);

// 使用内存池内存
// ... 执行计算任务 ...

// 释放内存池内存
cudaFreeAsync(d_pooled_data, 0);

// 销毁内存池
cudaDeviceReset();
```

**自定义内存分配器**：
```cuda
// 自定义分配器类
class CudaAllocator {
private:
    std::unordered_map<void*, size_t> allocations;

public:
    template<typename T>
    T* allocate(size_t count) {
        size_t size = count * sizeof(T);
        T* ptr;
        cudaMalloc((void**)&ptr, size);
        allocations[ptr] = size;
        return ptr;
    }

    template<typename T>
    void deallocate(T* ptr) {
        if (ptr && allocations.find(ptr) != allocations.end()) {
            cudaFree(ptr);
            allocations.erase(ptr);
        }
    }

    size_t get_allocation_size(void* ptr) {
        auto it = allocations.find(ptr);
        return (it != allocations.end()) ? it->second : 0;
    }

    void print_allocations() {
        size_t total = 0;
        for (const auto& alloc : allocations) {
            total += alloc.second;
            printf("地址: %p, 大小: %zu 字节\n", alloc.first, alloc.second);
        }
        printf("总分配: %zu 字节\n", total);
    }
};
```

#### 12.3.7 内存对齐和优化
**内存对齐**：
```cuda
// 对齐内存分配
void* aligned_malloc(size_t size, size_t alignment) {
    void* ptr;
    cudaMallocHost(&ptr, size + alignment - 1);
    void* aligned_ptr = (void*)(((size_t)ptr + alignment - 1) & ~(alignment - 1));
    return aligned_ptr;
}

void aligned_free(void* ptr) {
    cudaFreeHost(ptr);
}

// 使用对齐内存
float* aligned_data = (float*)aligned_malloc(1024 * sizeof(float), 256);
// ... 使用对齐内存 ...
aligned_free(aligned_data);
```

**内存合并访问优化**：
```cuda
// 优化的数据结构
struct AlignedData {
    float4 values[4];  // 使用向量类型提高合并访问
};

__global__ void optimized_access(AlignedData* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 合并访问模式
        float4 vec = data[idx].values[0];
        vec.x *= 2.0f;
        vec.y *= 2.0f;
        vec.z *= 2.0f;
        vec.w *= 2.0f;
        data[idx].values[0] = vec;
    }
}

// 主机端内存分配
AlignedData* h_data;
AlignedData* d_data;
size_t size = n * sizeof(AlignedData);

cudaMallocHost((void**)&h_data, size);
cudaMalloc((void**)&d_data, size);

// 初始化数据
for (int i = 0; i < n; i++) {
    for (int j = 0; j < 4; j++) {
        h_data[i].values[j].x = i + j;
        h_data[i].values[j].y = i + j + 1;
        h_data[i].values[j].z = i + j + 2;
        h_data[i].values[j].w = i + j + 3;
    }
}
```

#### 12.3.8 内存错误检测
**内存访问错误检测**：
```cuda
// 使用CUDA-MEMCHECK
// 在命令行运行: cuda-memcheck ./your_program

// 程序内错误检查
#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            printf("CUDA错误 [%s:%d]: %s\n", __FILE__, __LINE__, \
                   cudaGetErrorString(error)); \
            exit(1); \
        } \
    } while(0)

// 使用示例
float *d_data;
size_t size = 1024 * sizeof(float);

CUDA_CHECK(cudaMalloc((void**)&d_data, size));
CUDA_CHECK(cudaMemset(d_data, 0, size));
CUDA_CHECK(cudaFree(d_data));
```

**内存泄漏检测**：
```cuda
// 内存使用监控
class MemoryMonitor {
private:
    size_t total_allocated = 0;
    size_t peak_usage = 0;
    std::unordered_map<void*, size_t> active_allocations;

public:
    void track_allocation(void* ptr, size_t size) {
        active_allocations[ptr] = size;
        total_allocated += size;
        peak_usage = std::max(peak_usage, total_allocated);
        printf("分配: %p, 大小: %zu, 总计: %zu\n", ptr, size, total_allocated);
    }

    void track_deallocation(void* ptr) {
        auto it = active_allocations.find(ptr);
        if (it != active_allocations.end()) {
            total_allocated -= it->second;
            active_allocations.erase(it);
            printf("释放: %p, 大小: %zu, 总计: %zu\n", ptr, it->second, total_allocated);
        }
    }

    void print_leaks() {
        if (!active_allocations.empty()) {
            printf("内存泄漏检测:\n");
            for (const auto& alloc : active_allocations) {
                printf("  泄漏: %p, 大小: %zu 字节\n", alloc.first, alloc.second);
            }
        } else {
            printf("无内存泄漏\n");
        }
        printf("峰值内存使用: %zu 字节\n", peak_usage);
    }
};
```

#### 12.3.9 实际应用示例
**大型数组处理**：
```cuda
// 处理大型数组的内存管理
class LargeArrayProcessor {
private:
    float* d_data;
    float* h_pinned_data;
    size_t array_size;
    size_t chunk_size;
    cudaStream_t* streams;
    int num_streams;

public:
    LargeArrayProcessor(size_t total_size, int stream_count = 4) {
        array_size = total_size;
        chunk_size = total_size / stream_count;
        num_streams = stream_count;

        // 分配设备内存
        cudaMalloc((void**)&d_data, array_size * sizeof(float));

        // 分配固定内存
        cudaMallocHost((void**)&h_pinned_data, array_size * sizeof(float));

        // 创建流
        streams = new cudaStream_t[num_streams];
        for (int i = 0; i < num_streams; i++) {
            cudaStreamCreate(&streams[i]);
        }
    }

    ~LargeArrayProcessor() {
        cudaFree(d_data);
        cudaFreeHost(h_pinned_data);
        for (int i = 0; i < num_streams; i++) {
            cudaStreamDestroy(streams[i]);
        }
        delete[] streams;
    }

    void process_in_chunks() {
        for (int i = 0; i < num_streams; i++) {
            size_t offset = i * chunk_size;
            size_t size = (i == num_streams - 1) ? (array_size - offset) : chunk_size;

            // 异步拷贝数据到设备
            cudaMemPrefetchAsync(h_pinned_data + offset, size * sizeof(float),
                               cudaCpuDeviceId, streams[i]);

            // 异步执行Kernel
            process_chunk<<<(size + 255) / 256, 256, 0, streams[i]>>>(
                d_data + offset, size);

            // 异步拷贝结果回主机
            cudaMemPrefetchAsync(d_data + offset, size * sizeof(float),
                               cudaCpuDeviceId, streams[i]);
        }

        // 等待所有流完成
        for (int i = 0; i < num_streams; i++) {
            cudaStreamSynchronize(streams[i]);
        }
    }

private:
    __global__ void process_chunk(float* data, size_t size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            data[idx] = sqrtf(data[idx]) * 2.0f;
        }
    }
};
```

**多GPU内存管理**：
```cuda
// 多GPU内存管理类
class MultiGPUManager {
private:
    std::vector<float*> device_data;
    std::vector<int> device_ids;
    int num_devices;

public:
    MultiGPUManager() {
        cudaGetDeviceCount(&num_devices);
        device_data.resize(num_devices);
        device_ids.resize(num_devices);

        for (int i = 0; i < num_devices; i++) {
            device_ids[i] = i;
            cudaSetDevice(i);
            size_t size = 1024 * sizeof(float);
            cudaMalloc((void**)&device_data[i], size);
        }
    }

    ~MultiGPUManager() {
        for (int i = 0; i < num_devices; i++) {
            cudaSetDevice(i);
            cudaFree(device_data[i]);
        }
    }

    void distribute_data(float* h_data, size_t total_size) {
        size_t chunk_size = total_size / num_devices;

        for (int i = 0; i < num_devices; i++) {
            cudaSetDevice(i);
            size_t offset = i * chunk_size;
            size_t size = (i == num_devices - 1) ?
                         (total_size - offset) : chunk_size;

            cudaMemcpy(device_data[i], h_data + offset, size * sizeof(float),
                      cudaMemcpyHostToDevice);
        }
    }

    void synchronize_all() {
        for (int i = 0; i < num_devices; i++) {
            cudaSetDevice(i);
            cudaDeviceSynchronize();
        }
    }
};
```

#### 12.3.10 内存管理最佳实践
**内存管理检查清单**：
```c
✅ 分配前检查错误
✅ 使用合适的内存类型（固定内存用于频繁传输）
✅ 确保内存对齐
✅ 使用异步操作提高性能
✅ 及时释放不再使用的内存
✅ 避免内存泄漏
✅ 使用内存池减少分配开销
✅ 监控内存使用情况
✅ 多GPU应用中正确管理设备上下文
✅ 使用内存对齐提高访问效率
```

**常见内存错误及解决方案**：
```c
// 1. 内存访问越界
// 错误：访问超出分配范围的内存
// 解决：检查数组边界，使用正确的索引计算

// 2. 内存泄漏
// 错误：分配的内存未释放
// 解决：配对使用cudaMalloc/cudaFree

// 3. 空指针访问
// 错误：使用未分配或已释放的指针
// 解决：分配后检查指针有效性

// 4. 内存类型不匹配
// 错误：主机内存和设备内存混用
// 解决：明确区分内存类型，正确使用拷贝函数

// 5. 同步问题
// 错误：在数据传输完成前使用数据
// 解决：使用cudaDeviceSynchronize()或流同步
```

**性能优化建议**：
1. **使用固定内存**：对于频繁的主机-设备传输
2. **批量操作**：减少API调用次数
3. **异步操作**：重叠计算和数据传输
4. **内存池**：减少动态分配开销
5. **对齐访问**：提高内存访问效率
6. **统一内存**：简化内存管理，自动迁移
7. **多流并行**：提高设备利用率
8. **设备端分配**：减少不必要的主机-设备传输

### 12.4 内存优化

#### 12.4.1 内存访问模式优化
**内存合并访问**：
```cuda
// ✅ 好的访问模式：合并访问
__global__ void good_memory_access(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 合并访问：连续的内存访问
        float value = input[idx];  // 线程0访问input[0], 线程1访问input[1], ...
        output[idx] = value * 2.0f;  // 合并写入
    }
}

// ❌ 坏的访问模式：分散访问
__global__ void bad_memory_access(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 分散访问：跨步访问
        float value = input[idx * 2];  // 线程0访问input[0], 线程1访问input[2], ...
        output[idx * 2] = value * 2.0f;  // 分散写入
    }
}

// 半合并访问：部分合并
__global__ void half_coalesced_access(float *matrix, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        // 行优先访问（合并）
        float value = matrix[y * width + x];
        // 列优先访问（不合并）
        float value2 = matrix[x * height + y];  // 不合并访问
    }
}
```

**内存对齐优化**：
```cuda
// 使用对齐的数据结构
struct AlignedData {
    float4 values[4];  // 16字节对齐，提高访问效率
    int padding[4];    // 额外填充避免bank冲突
};

__global__ void aligned_access(AlignedData *data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 对齐访问，提高带宽利用率
        float4 vec = data[idx].values[0];
        vec.x *= 2.0f;
        vec.y *= 2.0f;
        vec.z *= 2.0f;
        vec.w *= 2.0f;
        data[idx].values[0] = vec;
    }
}

// 对齐内存分配
void* aligned_malloc(size_t size, size_t alignment) {
    void* ptr;
    cudaMallocHost(&ptr, size + alignment - 1);
    void* aligned_ptr = (void*)(((size_t)ptr + alignment - 1) & ~(alignment - 1));
    return aligned_ptr;
}

// 使用示例
AlignedData* h_data = (AlignedData*)aligned_malloc(n * sizeof(AlignedData), 256);
AlignedData* d_data;
cudaMalloc((void**)&d_data, n * sizeof(AlignedData));
```

#### 12.4.2 共享内存优化
**共享内存bank冲突避免**：
```cuda
// 避免bank冲突的共享内存布局
__global__ void bank_conflict_optimization() {
    // 添加padding避免bank冲突
    __shared__ float shared_data[32][33];  // 32个bank，33列避免冲突
    // 而不是 __shared__ float shared_data[32][32];

    int tx = threadIdx.x;
    int ty = threadIdx.y;

    // 无bank冲突访问
    float value = shared_data[ty][tx];
    shared_data[ty][tx] = value * 2.0f;

    // 如果使用32x32布局，会出现bank冲突
    // __shared__ float bad_data[32][32];
    // bad_data[ty][tx] = ...;  // 可能导致bank冲突
}

// 共享内存数据重用
__global__ void shared_memory_reuse(float *input, float *output, int n) {
    __shared__ float cache[BLOCK_SIZE];

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;

    // 一次加载，多次使用
    if (idx < n) {
        cache[tid] = input[idx];
    } else {
        cache[tid] = 0.0f;
    }
    __syncthreads();

    // 多次使用共享内存数据
    float sum = 0.0f;
    for (int i = 0; i < 4; i++) {
        sum += cache[tid] * (i + 1);
    }

    if (idx < n) {
        output[idx] = sum;
    }
}
```

**共享内存分块计算**：
```cuda
// 分块矩阵乘法
__global__ void blocked_matrix_mult(float *A, float *B, float *C,
                                   int N, int M, int K) {
    // 共享内存块大小
    const int TILE_SIZE = 32;

    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;

    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;

    float sum = 0.0f;

    // 分块计算
    for (int p = 0; p < (M + TILE_SIZE - 1) / TILE_SIZE; ++p) {
        // 加载到共享内存
        if (row < N && p * TILE_SIZE + tx < M)
            As[ty][tx] = A[row * M + p * TILE_SIZE + tx];
        else
            As[ty][tx] = 0.0f;

        if (col < K && p * TILE_SIZE + ty < M)
            Bs[ty][tx] = B[(p * TILE_SIZE + ty) * K + col];
        else
            Bs[ty][tx] = 0.0f;

        __syncthreads();

        // 计算部分结果
        for (int k = 0; k < TILE_SIZE; ++k)
            sum += As[ty][k] * Bs[k][tx];

        __syncthreads();
    }

    // 写回结果
    if (row < N && col < K)
        C[row * K + col] = sum;
}
```

#### 12.4.3 寄存器优化
**寄存器变量使用**：
```cuda
// 寄存器优化示例
__global__ void register_optimized(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 使用寄存器变量减少内存访问
        float reg_val = input[idx];
        float reg_val2 = reg_val * reg_val;
        float reg_val3 = reg_val2 + reg_val;

        // 复杂计算使用寄存器
        for (int i = 0; i < 10; i++) {
            reg_val3 = reg_val3 * reg_val + reg_val2;
        }

        output[idx] = reg_val3;
    }
}

// 向量化寄存器访问
__global__ void vectorized_register_access(float4 *input, float4 *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 一次加载4个float到寄存器
        float4 data = input[idx];

        // 向量化计算
        data.x *= 2.0f;
        data.y *= 2.0f;
        data.z *= 2.0f;
        data.w *= 2.0f;

        output[idx] = data;
    }
}
```

**寄存器压力管理**：
```cuda
// 减少寄存器使用的策略
__global__ void register_pressure_optimization(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 避免过多的临时变量
        // 不好的做法：
        // float temp1 = input[idx];
        // float temp2 = temp1 * 2.0f;
        // float temp3 = temp2 + 1.0f;
        // float temp4 = sqrt(temp3);
        // output[idx] = temp4;

        // 好的做法：链式计算
        output[idx] = sqrtf(input[idx] * 2.0f + 1.0f);
    }
}

// 使用#pragma unroll优化循环
__global__ void unrolled_loop_optimization(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        float sum = 0.0f;

        // 编译器自动展开循环
        #pragma unroll
        for (int i = 0; i < 8; i++) {
            sum += input[idx + i];
        }

        output[idx] = sum;
    }
}
```

#### 12.4.4 常量内存和纹理内存优化
**常量内存使用**：
```cuda
// 声明常量内存
__constant__ float const_kernel[256];
__constant__ float const_matrix[64][64];

// 主机端初始化常量内存
void initialize_constants() {
    float h_kernel[256];
    float h_matrix[64][64];

    // 初始化数据
    for (int i = 0; i < 256; i++) {
        h_kernel[i] = i * 0.1f;
    }

    for (int i = 0; i < 64; i++) {
        for (int j = 0; j < 64; j++) {
            h_matrix[i][j] = (i + j) * 0.01f;
        }
    }

    // 复制到常量内存
    cudaMemcpyToSymbol(const_kernel, h_kernel, 256 * sizeof(float));
    cudaMemcpyToSymbol(const_matrix, h_matrix, 64 * 64 * sizeof(float));
}

// Kernel中使用常量内存
__global__ void constant_memory_kernel(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        float sum = 0.0f;

        // 使用常量内存中的核函数
        for (int i = 0; i < 256; i++) {
            sum += input[idx + i] * const_kernel[i];
        }

        // 使用常量内存中的矩阵
        float matrix_val = const_matrix[idx % 64][idx / 64];
        output[idx] = sum * matrix_val;
    }
}
```

**纹理内存使用**：
```cuda
// 纹理内存声明
texture<float, 2, cudaReadModeElementType> tex_input;

// Kernel中使用纹理内存
__global__ void texture_memory_kernel(float *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        // 纹理内存访问（自动插值和缓存）
        float value = tex2D(tex_input, x + 0.5f, y + 0.5f);
        output[y * width + x] = value * 2.0f;
    }
}

// 主机端绑定纹理
void bind_texture(float *d_input, int width, int height) {
    cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<float>();
    cudaArray *cuArray;
    cudaMallocArray(&cuArray, &channelDesc, width, height);

    cudaMemcpyToArray(cuArray, 0, 0, d_input,
                     width * height * sizeof(float), cudaMemcpyDeviceToDevice);

    tex_input.addressMode[0] = cudaAddressModeClamp;
    tex_input.addressMode[1] = cudaAddressModeClamp;
    tex_input.filterMode = cudaFilterModeLinear;
    tex_input.normalized = false;

    cudaBindTextureToArray(tex_input, cuArray, channelDesc);

    // 使用后解绑
    // cudaUnbindTexture(tex_input);
    // cudaFreeArray(cuArray);
}
```

#### 12.4.5 内存带宽优化
**内存带宽最大化**：
```cuda
// 优化的向量操作
__global__ void bandwidth_optimized_vector_ops(float4 *input, float4 *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 使用向量类型最大化带宽
        float4 data = input[idx];

        // 向量化操作
        data.x = data.x * data.x + data.y;
        data.y = data.y * data.z + data.w;
        data.z = data.z * data.w + data.x;
        data.w = data.w * data.x + data.y;

        output[idx] = data;
    }
}

// 减少内存事务
__global__ void reduced_memory_transactions(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 一次读取，多次使用
        float val = input[idx];

        // 多次计算使用同一个值
        float result1 = val * val;
        float result2 = val + 1.0f;
        float result3 = val * 2.0f;
        float result4 = sqrtf(val);

        // 一次写入
        output[idx] = (result1 + result2 + result3 + result4) / 4.0f;
    }
}
```

**内存访问模式分析**：
```cuda
// 内存访问模式分析工具
class MemoryAccessAnalyzer {
public:
    static void analyze_access_pattern(float *data, int size) {
        printf("内存访问模式分析:\n");
        printf("数据大小: %d 个元素\n", size);
        printf("总内存: %zu 字节\n", size * sizeof(float));

        // 分析访问模式
        int stride = 1;
        int conflict_count = 0;

        for (int i = 0; i < 32; i++) {
            if (data[i * stride] != 0) {
                conflict_count++;
            }
        }

        printf("Bank冲突率: %d%%\n", (conflict_count * 100) / 32);
    }
};
```

#### 12.4.6 内存压缩和编码
**数据压缩**：
```cuda
// 压缩浮点数到半精度
__global__ void compress_to_half(half *output, float *input, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 压缩到半精度浮点
        output[idx] = __float2half(input[idx]);
    }
}

// 压缩整数编码
__global__ void compress_integers(unsigned char *output, int *input, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 假设数据范围在0-255之间
        if (input[idx] >= 0 && input[idx] <= 255) {
            output[idx] = (unsigned char)input[idx];
        }
    }
}

// 解压缩
__global__ void decompress_half(float *output, half *input, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        output[idx] = __half2float(input[idx]);
    }
}
```

#### 12.4.7 内存预取优化
**显式内存预取**：
```cuda
// 使用内存预取
__global__ void memory_prefetch_optimization(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 预取未来需要的数据
        if (idx + 1024 < n) {
            cuda::memcpy_async(&input[idx + 1024], &input[idx + 1024], sizeof(float));
        }

        // 当前计算
        float val = input[idx];
        val = val * val + val;

        // 等待预取完成
        cuda::memcpy_async_wait();

        output[idx] = val;
    }
}

// 多级预取
__global__ void multi_level_prefetch(float *input, float *output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        // 多级预取
        int prefetch_offsets[] = {256, 512, 1024, 2048};

        for (int offset : prefetch_offsets) {
            if (idx + offset < n) {
                cuda::memcpy_async(&input[idx + offset], &input[idx + offset], sizeof(float));
            }
        }

        // 执行计算
        float result = 0.0f;
        for (int i = 0; i < 10; i++) {
            result += input[idx + i];
        }

        // 等待所有预取完成
        cuda::memcpy_async_wait();

        output[idx] = result;
    }
}
```

#### 12.4.8 实际优化案例
**图像处理优化**：
```cuda
// 优化的图像模糊处理
__global__ void optimized_image_blur(unsigned char *input, unsigned char *output,
                                   int width, int height, int channels) {
    // 使用共享内存缓存图像块
    __shared__ unsigned char shared_block[34][34][3];  // 添加边界

    int x = blockIdx.x * 32 + threadIdx.x;
    int y = blockIdx.y * 32 + threadIdx.y;
    int tx = threadIdx.x + 1;  // 添加边界偏移
    int ty = threadIdx.y + 1;

    // 加载到共享内存（包含边界）
    for (int c = 0; c < channels; c++) {
        int global_x = x;
        int global_y = y;

        // 边界处理
        if (global_x >= 0 && global_x < width && global_y >= 0 && global_y < height) {
            shared_block[ty][tx][c] = input[(global_y * width + global_x) * channels + c];
        } else {
            shared_block[ty][tx][c] = 0;
        }
    }
    __syncthreads();

    // 边界线程也参与加载
    if (threadIdx.x == 0) {
        for (int c = 0; c < channels; c++) {
            int global_x = x - 1;
            int global_y = y;
            if (global_x >= 0 && global_x < width) {
                shared_block[ty][0][c] = input[(global_y * width + global_x) * channels + c];
            }
        }
    }
    __syncthreads();

    // 执行模糊计算
    if (x < width && y < height) {
        for (int c = 0; c < channels; c++) {
            float sum = 0.0f;

            // 3x3高斯模糊
            for (int dy = -1; dy <= 1; dy++) {
                for (int dx = -1; dx <= 1; dx++) {
                    sum += shared_block[ty + dy][tx + dx][c] * 0.111f;
                }
            }

            output[(y * width + x) * channels + c] = (unsigned char)sum;
        }
    }
}
```

**数值计算优化**：
```cuda
// 优化的FFT计算内存访问
__global__ void optimized_fft_memory_access(float2 *data, int n, int step) {
    __shared__ float2 shared_data[256];

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;

    // 加载到共享内存
    if (idx < n) {
        shared_data[tid] = data[idx];
    }
    __syncthreads();

    // FFT蝶形运算
    int k = step * tid;

    float2 u = shared_data[tid];
    float2 w = shared_data[tid + step];
    float2 t = w;

    // 复数乘法
    t.x = w.x * cosf(2.0f * M_PI * k / n) - w.y * sinf(2.0f * M_PI * k / n);
    t.y = w.x * sinf(2.0f * M_PI * k / n) + w.y * cosf(2.0f * M_PI * k / n);

    // 更新数据
    shared_data[tid] = make_float2(u.x + t.x, u.y + t.y);
    shared_data[tid + step] = make_float2(u.x - t.x, u.y - t.y);
    __syncthreads();

    // 写回全局内存
    if (idx < n) {
        data[idx] = shared_data[tid];
    }
}
```

#### 12.4.9 内存优化工具和分析
**内存使用分析**：
```cuda
// 内存使用监控工具
class MemoryProfiler {
private:
    cudaEvent_t start, stop;
    float total_memory_used;
    float peak_memory_used;

public:
    MemoryProfiler() {
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
        total_memory_used = 0;
        peak_memory_used = 0;
    }

    ~MemoryProfiler() {
        cudaEventDestroy(start);
        cudaEventDestroy(stop);
    }

    void start_profiling() {
        cudaEventRecord(start);
    }

    void stop_profiling() {
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);

        float milliseconds = 0;
        cudaEventElapsedTime(&milliseconds, start, stop);
        printf("执行时间: %f ms\n", milliseconds);
    }

    void measure_memory_usage() {
        size_t free_mem, total_mem;
        cudaMemGetInfo(&free_mem, &total_mem);

        float used = (total_mem - free_mem) / (1024.0f * 1024.0f);
        peak_memory_used = std::max(peak_memory_used, used);

        printf("当前内存使用: %.2f MB\n", used);
        printf("峰值内存使用: %.2f MB\n", peak_memory_used);
    }

    static void print_memory_bandwidth(float bytes_transferred, float time_ms) {
        float bandwidth = (bytes_transferred / (1024 * 1024)) / time_ms;
        printf("内存带宽: %.2f GB/s\n", bandwidth);
    }
};

// 使用示例
void profile_kernel_performance() {
    MemoryProfiler profiler;

    profiler.start_profiling();
    profiler.measure_memory_usage();

    // 执行Kernel
    your_kernel<<<grid, block>>>(args);

    profiler.stop_profiling();
    profiler.measure_memory_usage();

    // 计算带宽
    float bytes = 1024 * 1024 * sizeof(float) * 2;  // 读写各一次
    MemoryProfiler::print_memory_bandwidth(bytes, 1.5f);
}
```

**Nsight Compute分析**：
```bash
# 使用Nsight Compute分析内存性能
ncu --set full --metrics gld_throughput,gst_throughput,achieved_occupancy ./your_cuda_program

# 分析内存访问模式
ncu --metrics gld_transactions_per_request,gst_transactions_per_request ./your_program

# 分析共享内存使用
ncu --metrics shdmem_l1_bkconflict,shdmem_l1_throughput ./your_program

# 分析缓存命中率
ncu --metrics l1_cache_hit_rate,l2_cache_hit_rate ./your_program
```

#### 12.4.10 内存优化最佳实践
**内存优化检查清单**：
```c
✅ 确保内存访问合并
✅ 使用适当的内存类型（共享、常量、纹理）
✅ 避免共享内存bank冲突
✅ 最大化寄存器使用
✅ 减少全局内存访问次数
✅ 使用向量化数据类型
✅ 实现内存预取
✅ 优化数据结构对齐
✅ 使用内存压缩技术
✅ 监控内存带宽使用
✅ 分析内存访问模式
```

**常见优化陷阱及解决方案**：
```c
// 陷阱1：过度使用共享内存
// 问题：共享内存不足导致占用率下降
// 解决：合理分配共享内存大小，使用动态分配

// 陷阱2：寄存器压力过大
// 问题：过多变量导致寄存器溢出
// 解决：减少临时变量，使用链式计算

// 陷阱3：内存访问不合并
// 问题：分散访问导致带宽浪费
// 解决：重新组织数据布局，使用合并访问

// 陷阱4：频繁的主机-设备传输
// 问题：传输开销超过计算收益
// 解决：批量传输，使用固定内存，减少传输次数

// 陷阱5：不合理的内存分配
// 问题：频繁分配释放导致碎片
// 解决：使用内存池，预分配大块内存
```

**性能优化策略**：
1. **分析优先**：使用Nsight Compute分析瓶颈
2. **数据布局**：优化数据结构提高访问效率
3. **内存层次**：合理使用各级内存
4. **并行访问**：最大化内存并行度
5. **缓存利用**：充分利用硬件缓存
6. **传输优化**：减少不必要的数据传输
7. **压缩技术**：使用数据压缩减少内存使用
8. **预取策略**：实现智能预取机制

### 12.5 流和异步执行

#### 12.5.1 CUDA流基础
**流的概念和作用**：
```cuda
// CUDA流的基本概念
// - 流是GPU上操作的有序队列
// - 不同流中的操作可以并发执行
// - 同一流中的操作按顺序执行
// - 支持异步操作，提高CPU-GPU并行度

// 流的创建和销毁
cudaStream_t stream;
cudaError_t err = cudaStreamCreate(&stream);
if (err != cudaSuccess) {
    printf("流创建失败: %s\n", cudaGetErrorString(err));
    return -1;
}

// 使用流
// ... 执行异步操作 ...

// 销毁流
cudaStreamDestroy(stream);
```

**流的类型**：
```cuda
// 默认流（NULL流）
// - 所有操作同步执行
// - 与其他流中的操作不并发
// - 使用cudaMemcpy、普通Kernel调用时使用
cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);  // 同步
kernel<<<grid, block>>>(args);  // 同步

// 非默认流
// - 支持异步操作
// - 可以并发执行
// - 需要显式指定流参数
cudaStream_t stream;
cudaStreamCreate(&stream);
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);
kernel<<<grid, block, 0, stream>>>(args);
cudaStreamDestroy(stream);
```

#### 12.5.2 异步内存操作
**异步内存拷贝**：
```cuda
// 异步主机到设备拷贝
cudaStream_t h2d_stream, d2h_stream;
cudaStreamCreate(&h2d_stream);
cudaStreamCreate(&d2h_stream);

// 异步拷贝数据到设备
cudaMemcpyAsync(d_input1, h_input1, size, cudaMemcpyHostToDevice, h2d_stream);
cudaMemcpyAsync(d_input2, h_input2, size, cudaMemcpyHostToDevice, h2d_stream);

// 异步Kernel执行
process_kernel<<<grid, block, 0, h2d_stream>>>(d_input1, d_output1, size);
process_kernel<<<grid, block, 0, h2d_stream>>>(d_input2, d_output2, size);

// 异步拷贝结果回主机
cudaMemcpyAsync(h_output1, d_output1, size, cudaMemcpyDeviceToHost, d2h_stream);
cudaMemcpyAsync(h_output2, d_output2, size, cudaMemcpyDeviceToHost, d2h_stream);

// 同步所有流
cudaStreamSynchronize(h2d_stream);
cudaStreamSynchronize(d2h_stream);

// 清理
cudaStreamDestroy(h2d_stream);
cudaStreamDestroy(d2h_stream);
```

**异步内存分配**：
```cuda
// 异步内存分配（CUDA 11.2+）
cudaStream_t alloc_stream;
cudaStreamCreate(&alloc_stream);

// 异步分配设备内存
float *d_data1, *d_data2;
size_t size = 1024 * sizeof(float);
cudaMallocAsync((void**)&d_data1, size, alloc_stream);
cudaMallocAsync((void**)&d_data2, size, alloc_stream);

// 异步拷贝数据
cudaMemcpyAsync(d_data1, h_data1, size, cudaMemcpyHostToDevice, alloc_stream);
cudaMemcpyAsync(d_data2, h_data2, size, cudaMemcpyHostToDevice, alloc_stream);

// 异步执行Kernel
process_kernel<<<grid, block, 0, alloc_stream>>>(d_data1, d_result1, size);
process_kernel<<<grid, block, 0, alloc_stream>>>(d_data2, d_result2, size);

// 异步释放内存
cudaFreeAsync(d_data1, alloc_stream);
cudaFreeAsync(d_data2, alloc_stream);

// 同步
cudaStreamSynchronize(alloc_stream);
cudaStreamDestroy(alloc_stream);
```

**异步内存预取**：
```cuda
// 内存预取优化
cudaStream_t prefetch_stream;
cudaStreamCreate(&prefetch_stream);

// 预取数据到GPU
cudaMemPrefetchAsync(h_data, size, cudaCpuDeviceId, prefetch_stream);

// 在其他流中使用预取的数据
cudaStream_t compute_stream;
cudaStreamCreate(&compute_stream);
process_kernel<<<grid, block, 0, compute_stream>>>(d_data, d_result, size);

// 等待预取完成
cudaStreamWaitEvent(prefetch_stream, prefetch_event, 0);

cudaStreamDestroy(prefetch_stream);
cudaStreamDestroy(compute_stream);
```

#### 12.5.3 多流并行执行
**计算与传输重叠**：
```cuda
// 实现计算与数据传输的重叠
class OverlapExecutor {
private:
    static const int NUM_STREAMS = 4;
    cudaStream_t streams[NUM_STREAMS];
    cudaEvent_t events[NUM_STREAMS];

public:
    OverlapExecutor() {
        // 创建流和事件
        for (int i = 0; i < NUM_STREAMS; i++) {
            cudaStreamCreate(&streams[i]);
            cudaEventCreate(&events[i]);
        }
    }

    ~OverlapExecutor() {
        // 清理资源
        for (int i = 0; i < NUM_STREAMS; i++) {
            cudaStreamDestroy(streams[i]);
            cudaEventDestroy(events[i]);
        }
    }

    void execute_overlapped(float **h_inputs, float **d_inputs,
                          float **d_outputs, float **h_outputs,
                          int num_chunks, int chunk_size) {
        for (int i = 0; i < num_chunks; i++) {
            int stream_id = i % NUM_STREAMS;

            // 异步拷贝输入数据到设备
            cudaMemcpyAsync(d_inputs[stream_id], h_inputs[i],
                           chunk_size * sizeof(float),
                           cudaMemcpyHostToDevice, streams[stream_id]);

            // 异步执行计算
            process_chunk<<<(chunk_size + 255) / 256, 256, 0, streams[stream_id]>>>(
                d_inputs[stream_id], d_outputs[stream_id], chunk_size);

            // 异步拷贝结果回主机
            cudaMemcpyAsync(h_outputs[i], d_outputs[stream_id],
                           chunk_size * sizeof(float),
                           cudaMemcpyDeviceToHost, streams[stream_id]);

            // 记录事件
            cudaEventRecord(events[stream_id], streams[stream_id]);
        }

        // 等待所有操作完成
        for (int i = 0; i < NUM_STREAMS; i++) {
            cudaStreamSynchronize(streams[i]);
        }
    }

private:
    __global__ void process_chunk(float *input, float *output, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            output[idx] = sqrtf(input[idx]) * 2.0f;
        }
    }
};
```

**多Kernel并行执行**：
```cuda
// 多个Kernel在不同流中并行执行
void parallel_kernels_execution() {
    const int NUM_KERNELS = 3;
    cudaStream_t streams[NUM_KERNELS];

    // 创建流
    for (int i = 0; i < NUM_KERNELS; i++) {
        cudaStreamCreate(&streams[i]);
    }

    // 在不同流中启动Kernel
    kernel1<<<grid1, block1, 0, streams[0]>>>(d_data1, d_result1, size1);
    kernel2<<<grid2, block2, 0, streams[1]>>>(d_data2, d_result2, size2);
    kernel3<<<grid3, block3, 0, streams[2]>>>(d_data3, d_result3, size3);

    // 同步所有流
    for (int i = 0; i < NUM_KERNELS; i++) {
        cudaStreamSynchronize(streams[i]);
    }

    // 清理
    for (int i = 0; i < NUM_KERNELS; i++) {
        cudaStreamDestroy(streams[i]);
    }
}
```

#### 12.5.4 流同步机制
**流间同步**：
```cuda
// 流间依赖和同步
cudaStream_t stream1, stream2;
cudaEvent_t event;

cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);
cudaEventCreate(&event);

// 在stream1中执行操作并记录事件
operation1<<<grid, block, 0, stream1>>>(d_data1, d_temp, size);
cudaEventRecord(event, stream1);

// stream2等待事件完成
cudaStreamWaitEvent(stream2, event, 0);

// 在stream2中执行依赖操作
operation2<<<grid, block, 0, stream2>>>(d_temp, d_result, size);

// 同步
cudaStreamSynchronize(stream1);
cudaStreamSynchronize(stream2);

// 清理
cudaEventDestroy(event);
cudaStreamDestroy(stream1);
cudaStreamDestroy(stream2);
```

**事件同步**：
```cuda
// 使用CUDA事件进行精确同步
class EventSynchronizer {
private:
    cudaEvent_t start_event, stop_event;

public:
    EventSynchronizer() {
        cudaEventCreate(&start_event);
        cudaEventCreate(&stop_event);
    }

    ~EventSynchronizer() {
        cudaEventDestroy(start_event);
        cudaEventDestroy(stop_event);
    }

    void time_operation(cudaStream_t stream, std::function<void()> operation) {
        // 记录开始时间
        cudaEventRecord(start_event, stream);

        // 执行操作
        operation();

        // 记录结束时间
        cudaEventRecord(stop_event, stream);

        // 等待完成并计算时间
        cudaEventSynchronize(stop_event);
        float milliseconds = 0;
        cudaEventElapsedTime(&milliseconds, start_event, stop_event);

        printf("操作耗时: %f ms\n", milliseconds);
    }

    void synchronize_streams(const std::vector<cudaStream_t>& streams) {
        for (cudaStream_t stream : streams) {
            cudaStreamSynchronize(stream);
        }
    }
};
```

#### 12.5.5 流优先级和调度
**流优先级设置**：
```cuda
// 设置流优先级
int min_priority, max_priority;
cudaDeviceGetStreamPriorityRange(&min_priority, &max_priority);

printf("流优先级范围: %d 到 %d\n", min_priority, max_priority);

// 创建高优先级流
cudaStream_t high_priority_stream;
cudaStreamCreateWithPriority(&high_priority_stream, cudaStreamNonBlocking, max_priority);

// 创建低优先级流
cudaStream_t low_priority_stream;
cudaStreamCreateWithPriority(&low_priority_stream, cudaStreamNonBlocking, min_priority);

// 使用不同优先级的流
high_priority_kernel<<<grid, block, 0, high_priority_stream>>>(d_critical_data, size);
low_priority_kernel<<<grid, block, 0, low_priority_stream>>>(d_background_data, size);

// 清理
cudaStreamDestroy(high_priority_stream);
cudaStreamDestroy(low_priority_stream);
```

**流调度优化**：
```cuda
// 智能流调度器
class SmartScheduler {
private:
    std::vector<cudaStream_t> streams;
    std::vector<int> stream_load;  // 每个流的负载
    int num_streams;

public:
    SmartScheduler(int num_streams = 4) : num_streams(num_streams) {
        streams.resize(num_streams);
        stream_load.resize(num_streams, 0);

        // 创建流
        for (int i = 0; i < num_streams; i++) {
            cudaStreamCreate(&streams[i]);
        }
    }

    ~SmartScheduler() {
        for (int i = 0; i < num_streams; i++) {
            cudaStreamDestroy(streams[i]);
        }
    }

    // 智能选择负载最低的流
    cudaStream_t get_least_loaded_stream() {
        int min_load = stream_load[0];
        int best_stream = 0;

        for (int i = 1; i < num_streams; i++) {
            if (stream_load[i] < min_load) {
                min_load = stream_load[i];
                best_stream = i;
            }
        }

        stream_load[best_stream]++;
        return streams[best_stream];
    }

    // 执行任务
    void execute_task(std::function<void(cudaStream_t)> task) {
        cudaStream_t stream = get_least_loaded_stream();
        task(stream);
    }

    // 重置负载计数
    void reset_load() {
        std::fill(stream_load.begin(), stream_load.end(), 0);
    }

    // 等待所有流完成
    void wait_all() {
        for (int i = 0; i < num_streams; i++) {
            cudaStreamSynchronize(streams[i]);
        }
    }
};
```

#### 12.5.6 异步执行高级技术
**异步内存池**：
```cuda
// 异步内存池管理
class AsyncMemoryPool {
private:
    cudaMemPool_t mempool;
    std::queue<void*> free_blocks;
    std::mutex pool_mutex;
    size_t block_size;

public:
    AsyncMemoryPool(size_t block_size) : block_size(block_size) {
        // 创建内存池
        cudaDeviceGetDefaultMemPool(&mempool, 0);

        // 配置内存池
        size_t threshold = 1024 * 1024;  // 1MB
        cudaMemPoolSetAttribute(mempool, cudaMemPoolAttrReleaseThreshold, &threshold);
    }

    ~AsyncMemoryPool() {
        cudaDeviceReset();
    }

    // 异步分配
    void* allocate_async(cudaStream_t stream) {
        void* ptr;
        cudaMallocFromPoolAsync(&ptr, block_size, mempool, stream);
        return ptr;
    }

    // 异步释放
    void deallocate_async(void* ptr, cudaStream_t stream) {
        cudaFreeAsync(ptr, stream);
    }

    // 批量分配
    std::vector<void*> batch_allocate(cudaStream_t stream, int count) {
        std::vector<void*> pointers(count);
        for (int i = 0; i < count; i++) {
            pointers[i] = allocate_async(stream);
        }
        return pointers;
    }
};
```

**异步错误处理**：
```cuda
// 异步操作的错误处理
class AsyncErrorHandler {
public:
    static void check_async_error(cudaStream_t stream, const char* operation) {
        cudaError_t error = cudaStreamQuery(stream);
        if (error != cudaSuccess && error != cudaErrorNotReady) {
            printf("异步操作错误 [%s]: %s\n", operation, cudaGetErrorString(error));
        }
    }

    static void check_last_error(const char* operation) {
        cudaError_t error = cudaGetLastError();
        if (error != cudaSuccess) {
            printf("CUDA错误 [%s]: %s\n", operation, cudaGetErrorString(error));
        }
    }

    static void safe_async_copy(void* dst, const void* src, size_t count,
                               cudaMemcpyKind kind, cudaStream_t stream) {
        cudaError_t error = cudaMemcpyAsync(dst, src, count, kind, stream);
        if (error != cudaSuccess) {
            printf("异步拷贝失败: %s\n", cudaGetErrorString(error));
        }
    }
};
```

#### 12.5.7 实际应用示例
**流水线处理**：
```cuda
// 流水线数据处理
class PipelineProcessor {
private:
    static const int PIPELINE_STAGES = 3;
    cudaStream_t streams[PIPELINE_STAGES];
    cudaEvent_t events[PIPELINE_STAGES];

public:
    PipelineProcessor() {
        // 创建流和事件
        for (int i = 0; i < PIPELINE_STAGES; i++) {
            cudaStreamCreate(&streams[i]);
            cudaEventCreate(&events[i]);
        }
    }

    ~PipelineProcessor() {
        // 清理资源
        for (int i = 0; i < PIPELINE_STAGES; i++) {
            cudaStreamDestroy(streams[i]);
            cudaEventDestroy(events[i]);
        }
    }

    void process_pipeline(float *input_data, float *output_data, int data_size) {
        int chunk_size = data_size / PIPELINE_STAGES;

        // 流水线执行
        for (int stage = 0; stage < PIPELINE_STAGES; stage++) {
            int offset = stage * chunk_size;

            // 阶段1: 数据预处理
            if (stage == 0) {
                preprocess_kernel<<<(chunk_size + 255) / 256, 256, 0, streams[0]>>>(
                    input_data + offset, chunk_size);
                cudaEventRecord(events[0], streams[0]);
            }

            // 阶段2: 主要计算（等待阶段1完成）
            if (stage >= 1) {
                cudaStreamWaitEvent(streams[1], events[0], 0);
                compute_kernel<<<(chunk_size + 255) / 256, 256, 0, streams[1]>>>(
                    input_data + offset, chunk_size);
                cudaEventRecord(events[1], streams[1]);
            }

            // 阶段3: 结果后处理（等待阶段2完成）
            if (stage >= 2) {
                cudaStreamWaitEvent(streams[2], events[1], 0);
                postprocess_kernel<<<(chunk_size + 255) / 256, 256, 0, streams[2]>>>(
                    input_data + offset, output_data + offset, chunk_size);
                cudaEventRecord(events[2], streams[2]);
            }
        }

        // 等待所有阶段完成
        for (int i = 0; i < PIPELINE_STAGES; i++) {
            cudaStreamSynchronize(streams[i]);
        }
    }

private:
    __global__ void preprocess_kernel(float *data, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            data[idx] = data[idx] * 2.0f;
        }
    }

    __global__ void compute_kernel(float *data, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            data[idx] = sqrtf(data[idx]);
        }
    }

    __global__ void postprocess_kernel(float *input, float *output, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            output[idx] = input[idx] + 1.0f;
        }
    }
};
```

**多GPU异步执行**：
```cuda
// 多GPU异步执行管理
class MultiGPUAsyncManager {
private:
    std::vector<cudaStream_t> streams;
    std::vector<int> devices;
    int num_gpus;

public:
    MultiGPUAsyncManager(int num_gpus) : num_gpus(num_gpus) {
        streams.resize(num_gpus);
        devices.resize(num_gpus);

        // 获取可用GPU
        int device_count;
        cudaGetDeviceCount(&device_count);
        num_gpus = std::min(num_gpus, device_count);

        for (int i = 0; i < num_gpus; i++) {
            devices[i] = i;
            cudaSetDevice(i);
            cudaStreamCreate(&streams[i]);
        }
    }

    ~MultiGPUAsyncManager() {
        for (int i = 0; i < num_gpus; i++) {
            cudaSetDevice(devices[i]);
            cudaStreamDestroy(streams[i]);
        }
    }

    void distribute_work(float **h_data, float **d_data, int *sizes, int total_work) {
        int work_per_gpu = total_work / num_gpus;

        for (int gpu = 0; gpu < num_gpus; gpu++) {
            cudaSetDevice(devices[gpu]);

            // 分配GPU内存
            cudaMalloc((void**)&d_data[gpu], sizes[gpu] * sizeof(float));

            // 异步拷贝数据
            cudaMemcpyAsync(d_data[gpu], h_data[gpu],
                           sizes[gpu] * sizeof(float),
                           cudaMemcpyHostToDevice, streams[gpu]);

            // 异步执行计算
            gpu_kernel<<<(sizes[gpu] + 255) / 256, 256, 0, streams[gpu]>>>(
                d_data[gpu], sizes[gpu]);
        }

        // 同步所有GPU
        for (int gpu = 0; gpu < num_gpus; gpu++) {
            cudaSetDevice(devices[gpu]);
            cudaStreamSynchronize(streams[gpu]);
        }
    }

private:
    __global__ void gpu_kernel(float *data, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            data[idx] = data[idx] * data[idx];
        }
    }
};
```

#### 12.5.8 性能分析和调优
**流性能分析**：
```cuda
// 流性能分析工具
class StreamProfiler {
private:
    cudaEvent_t start_event, stop_event;
    std::vector<float> stage_times;

public:
    StreamProfiler() {
        cudaEventCreate(&start_event);
        cudaEventCreate(&stop_event);
    }

    ~StreamProfiler() {
        cudaEventDestroy(start_event);
        cudaEventDestroy(stop_event);
    }

    void profile_stream(cudaStream_t stream, const char* name,
                       std::function<void()> operation) {
        cudaEventRecord(start_event, stream);
        operation();
        cudaEventRecord(stop_event, stream);
        cudaEventSynchronize(stop_event);

        float time_ms;
        cudaEventElapsedTime(&time_ms, start_event, stop_event);

        printf("流 %s 执行时间: %.2f ms\n", name, time_ms);
        stage_times.push_back(time_ms);
    }

    void analyze_concurrency(const std::vector<cudaStream_t>& streams) {
        printf("\n=== 并发分析 ===\n");

        // 分析流之间的重叠
        for (size_t i = 0; i < streams.size(); i++) {
            for (size_t j = i + 1; j < streams.size(); j++) {
                // 这里可以添加更复杂的并发分析逻辑
                printf("流 %zu 和流 %zu 的并发性分析\n", i, j);
            }
        }
    }

    float get_total_time() {
        float total = 0;
        for (float time : stage_times) {
            total += time;
        }
        return total;
    }
};
```

**最佳实践和性能调优**：
```cuda
// 流使用的最佳实践
class StreamBestPractices {
public:
    // 1. 合理的流数量
    static int get_optimal_stream_count() {
        int multiprocessor_count;
        cudaDeviceGetAttribute(&multiprocessor_count,
                              cudaDevAttrMultiProcessorCount, 0);
        return std::min(multiprocessor_count / 2, 16);  // 经验值
    }

    // 2. 内存对齐优化
    static void* aligned_malloc_async(size_t size, size_t alignment,
                                    cudaStream_t stream) {
        void* ptr;
        cudaMallocHost(&ptr, size + alignment - 1);
        void* aligned_ptr = (void*)(((size_t)ptr + alignment - 1) & ~(alignment - 1));
        return aligned_ptr;
    }

    // 3. 批量操作优化
    static void batch_operations(const std::vector<std::function<void()>>& operations,
                                cudaStream_t stream) {
        for (const auto& op : operations) {
            op();  // 在同一个流中批量执行
        }
    }

    // 4. 错误处理
    static void safe_stream_operation(cudaStream_t stream,
                                   std::function<void()> operation) {
        try {
            operation();
            cudaError_t error = cudaStreamQuery(stream);
            if (error != cudaSuccess && error != cudaErrorNotReady) {
                printf("流操作错误: %s\n", cudaGetErrorString(error));
            }
        } catch (const std::exception& e) {
            printf("异常: %s\n", e.what());
        }
    }
};
```

#### 12.5.9 流和异步执行最佳实践
**设计原则**：
```c
✅ 根据硬件能力选择合适的流数量
✅ 实现计算与数据传输的重叠
✅ 使用事件进行精确的同步控制
✅ 合理设置流优先级
✅ 实施完善的错误处理机制
✅ 监控和分析流的性能
✅ 避免过多的流创建和销毁
✅ 使用内存池减少分配开销
✅ 实现智能的负载均衡
✅ 考虑多GPU的协同工作
```

**性能优化检查清单**：
```c
✅ 分析GPU利用率，确保流的使用带来性能提升
✅ 检查内存带宽利用率，优化数据传输
✅ 监控Kernel执行时间，识别瓶颈
✅ 评估流间依赖关系，减少不必要的同步
✅ 测试不同流数量的性能表现
✅ 验证异步操作的正确性
✅ 检查内存使用情况，避免内存泄漏
✅ 分析并发度，确保资源充分利用
```

**常见问题和解决方案**：
```c
// 问题1：流太多导致开销增加
// 解决：限制流数量，通常2-8个流足够

// 问题2：同步点过多影响并发
// 解决：减少不必要的同步，使用事件精确控制

// 问题3：内存分配成为瓶颈
// 解决：使用内存池，预分配内存

// 问题4：多GPU协调复杂
// 解决：使用统一内存，简化数据管理

// 问题5：错误处理不完善
// 解决：实施全面的错误检测和恢复机制
```

**性能调优策略**：
1. **分析优先**：使用Nsight Systems分析流的使用情况
2. **逐步优化**：从单流开始，逐步增加流数量
3. **监控资源**：密切关注GPU利用率和内存使用
4. **测试验证**：在不同硬件配置上测试性能
5. **文档记录**：记录优化过程和效果
6. **持续改进**：根据应用需求调整优化策略

## 第13章 其他并行框架

### 13.1 OpenACC

#### 13.1.1 OpenACC概述
**什么是OpenACC**：
```c
// OpenACC简介
// - 编译器指令驱动的并行编程模型
// - 支持C、C++和Fortran
// - 自动GPU加速，无需编写CUDA代码
// - 与OpenMP类似，但专注于加速器编程
// - 支持多架构：NVIDIA GPU、AMD GPU、Intel Xe等
```

**OpenACC vs CUDA**：
```c
// CUDA方式
__global__ void vector_add(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
// 调用: vector_add<<<blocks, threads>>>(a, b, c, n);

// OpenACC方式
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
// 编译器自动生成GPU代码
```

**OpenACC优势**：
- **易用性**：无需学习GPU架构细节
- **可移植性**：代码可在不同加速器上运行
- **渐进式迁移**：可以逐步加速现有代码
- **维护性**：源代码保持清晰易读

#### 13.1.2 OpenACC编译器和工具链
**主流编译器支持**：
```bash
# NVIDIA HPC SDK (推荐)
nvc -acc -gpu=cc80 vector_add.c
nvc++ -acc -gpu=cc80 vector_add.cpp
nvfortran -acc -gpu=cc80 vector_add.f90

# GCC (实验性支持)
gcc -fopenacc -foffload=nvptx-none vector_add.c

# Cray编译器
cc -h acc vector_add.c

# PGI编译器 (已被NVIDIA收购)
pgcc -acc vector_add.c
```

**编译器选项**：
```bash
# 基本编译选项
-acc                    # 启用OpenACC
-acc=no-libs           # 不链接OpenACC运行时库
-gpu=cc80              # 指定GPU计算能力
-mp=gpu                # 启用GPU多处理器支持

# 调试选项
-Minfo=accel           # 显示加速器信息
-Minfo=inline          # 显示内联信息
-g                     # 生成调试信息
-Mbounds               # 数组边界检查

# 优化选项
-O3                    # 最高优化级别
-fast                  # 快速编译优化
-Mipa=fast             # 过程间分析优化
```

**运行时环境配置**：
```bash
# 设置OpenACC环境变量
export ACC_DEVICE_TYPE=nvidia        # 指定设备类型
export ACC_DEVICE_NUM=0              # 指定设备编号
export ACC_DEBUG=1                   # 启用调试输出
export PGI_ACC_TIME=1               # 启用时间统计

# 查询设备信息
pgaccelinfo                          # PGI编译器设备信息
nvidia-smi                          # NVIDIA设备信息
```

#### 13.1.3 OpenACC基本指令
**并行区域指令**：
```c
// 并行循环
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// 并行区域
#pragma acc parallel
{
    #pragma acc loop
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

// 工作共享循环
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    // 每个线程处理一个迭代
    result[i] = compute(data[i]);
}
```

**数据管理指令**：
```c
// 数据拷贝到设备
#pragma acc data copy(a[0:n], b[0:n], c[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}

// 数据创建和拷贝
#pragma acc data create(temp[0:n]) copyin(a[0:n], b[0:n]) copyout(c[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        temp[i] = a[i] * b[i];
        c[i] = temp[i] + 1.0f;
    }
}

// 数据更新
#pragma acc update device(a[0:n])    // 主机到设备
#pragma acc update host(c[0:n])      // 设备到主机
```

**核函数指令**：
```c
// 核函数执行
#pragma acc kernels
{
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }

    for (int i = 0; i < n; i++) {
        d[i] = c[i] * 2.0f;
    }
}

// 核函数循环
#pragma acc kernels loop
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
```

#### 13.1.4 OpenACC高级特性
**循环调度和优化**：
```c
// 循环调度策略
#pragma acc parallel loop gang(128)    // 指定gang数量
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

#pragma acc parallel loop vector(32)   // 指定向量长度
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

#pragma acc parallel loop gang vector   // 自动调度
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// 循环优化提示
#pragma acc parallel loop independent   // 声明循环独立
#pragma acc parallel loop private(i)   // 私有变量
#pragma acc parallel loop reduction(+:sum)  // 归约操作
```

**内存管理和优化**：
```c
// 内存分配策略
#pragma acc data copy(a[0:n]) alloc(b[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        b[i] = a[i] * 2.0f;
    }
}

// 异步执行
#pragma acc data copy(a[0:n], b[0:n]) async(1)
{
    #pragma acc parallel loop async(1)
    for (int i = 0; i < n; i++) {
        b[i] = a[i] * 2.0f;
    }
    #pragma acc wait(1)
}

// 内存预取
#pragma acc data present(a[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        b[i] = a[i] * 2.0f;
    }
}
```

**设备选择和多GPU**：
```c
// 设备选择
#pragma acc init device_type(gpu) device_num(0)
#pragma acc parallel device_type(gpu) device_num(1)
{
    // 在指定设备上执行
}

// 多GPU编程
void multi_gpu_example(float **data, int num_gpus, int size_per_gpu) {
    #pragma acc parallel loop num_gangs(num_gpus) gang
    for (int gpu = 0; gpu < num_gpus; gpu++) {
        #pragma acc loop vector
        for (int i = 0; i < size_per_gpu; i++) {
            data[gpu][i] = data[gpu][i] * 2.0f;
        }
    }
}
```

#### 13.1.5 OpenACC性能优化
**编译器优化提示**：
```c
// 内联函数
#pragma acc routine seq inline
float compute(float x) {
    return x * x + 1.0f;
}

// 向量化提示
#pragma acc parallel loop vector(32)
for (int i = 0; i < n; i++) {
    result[i] = compute(data[i]);
}

// 循环融合
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    temp[i] = a[i] + b[i];     // 第一个循环
}

#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    result[i] = temp[i] * 2.0f; // 第二个循环
}
```

**内存访问优化**：
```c
// 连续内存访问
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    for (int j = 0; j < m; j++) {
        matrix[i][j] = matrix[i][j] * 2.0f;  // 好：行优先访问
    }
}

// 避免分支发散
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    // 避免复杂的条件分支
    float val = data[i];
    if (val > 0.0f) {
        result[i] = sqrtf(val);  // warp内所有线程执行相同路径
    } else {
        result[i] = 0.0f;
    }
}

// 共享内存使用
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    #pragma acc cache(a[i:i+32])  // 缓存数据块
    float sum = 0.0f;
    for (int j = 0; j < 32; j++) {
        sum += a[i + j];
    }
    result[i] = sum;
}
```

**性能分析和调试**：
```c
// 性能分析
#pragma acc parallel loop timer
for (int i = 0; i < n; i++) {
    result[i] = compute(data[i]);
}

// 调试信息
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    #pragma acc atomic
    printf("Processing element %d\n", i);
}

// 性能计数器
#include <openacc.h>

void performance_monitoring() {
    acc_init(acc_device_nvidia);
    acc_set_device_num(0, acc_device_nvidia);

    // 获取设备信息
    int num_devices = acc_get_num_devices(acc_device_nvidia);
    printf("可用设备数: %d\n", num_devices);

    // 内存使用统计
    size_t free_mem, total_mem;
    acc_get_property(acc_get_device_num(acc_device_nvidia),
                    acc_property_memory_free, &free_mem);
    acc_get_property(acc_get_device_num(acc_device_nvidia),
                    acc_property_memory_total, &total_mem);

    printf("可用内存: %zu MB, 总内存: %zu MB\n",
           free_mem / (1024*1024), total_mem / (1024*1024));
}
```

#### 13.1.6 OpenACC与CUDA混合编程
**OpenACC + CUDA混合编程**：
```c
// OpenACC管理数据，CUDA实现核函数
#include <openacc.h>
#include <cuda_runtime.h>

// CUDA核函数
__global__ void cuda_kernel(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] * b[idx] + sinf(a[idx]);
    }
}

// OpenACC管理数据传输
void mixed_programming_example(float *h_a, float *h_b, float *h_c, int n) {
    float *d_a, *d_b, *d_c;

    // 使用OpenACC分配和传输数据
    #pragma acc enter data create(d_a[0:n], d_b[0:n], d_c[0:n])
    #pragma acc update device(h_a[0:n], h_b[0:n]) to:d_a[0:n], d_b[0:n]

    // 调用CUDA核函数
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    cuda_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);

    // 使用OpenACC传输结果
    #pragma acc update host(d_c[0:n]) from:h_c[0:n]

    // 释放设备内存
    #pragma acc exit data delete(d_a[0:n], d_b[0:n], d_c[0:n])
}
```

**OpenACC + Thrust混合编程**：
```c
#include <openacc.h>
#include <thrust/device_vector.h>
#include <thrust/transform.h>
#include <thrust/functional.h>

// 使用Thrust进行并行算法
void thrust_with_openacc_example(float *data, int n) {
    // 使用OpenACC管理数据
    #pragma acc data copy(data[0:n])
    {
        // 创建Thrust设备向量
        thrust::device_vector<float> d_data(data, data + n);

        // 使用Thrust算法
        thrust::transform(d_data.begin(), d_data.end(),
                         d_data.begin(),
                         thrust::placeholders::_1 * 2.0f);

        // 复制回主机
        thrust::copy(d_data.begin(), d_data.end(), data);
    }
}
```

#### 13.1.7 OpenACC实际应用案例
**矩阵乘法优化**：
```c
// OpenACC矩阵乘法
void matrix_multiply_openacc(float *A, float *B, float *C,
                            int M, int N, int K) {
    #pragma acc data copyin(A[0:M*K], B[0:K*N]) copyout(C[0:M*N])
    {
        #pragma acc parallel loop gang(32) vector(32)
        for (int i = 0; i < M; i++) {
            #pragma acc loop gang(32) vector(32)
            for (int j = 0; j < N; j++) {
                float sum = 0.0f;
                #pragma acc loop reduction(+:sum)
                for (int k = 0; k < K; k++) {
                    sum += A[i * K + k] * B[k * N + j];
                }
                C[i * N + j] = sum;
            }
        }
    }
}

// 性能优化版本
void optimized_matrix_multiply(float *A, float *B, float *C,
                             int M, int N, int K) {
    const int TILE_SIZE = 32;

    #pragma acc data copyin(A[0:M*K], B[0:K*N]) copyout(C[0:M*N])
    {
        #pragma acc parallel loop gang(TILE_SIZE) vector(TILE_SIZE)
        for (int i = 0; i < M; i += TILE_SIZE) {
            for (int j = 0; j < N; j += TILE_SIZE) {
                #pragma acc loop gang(TILE_SIZE) vector(TILE_SIZE)
                for (int k = 0; k < K; k += TILE_SIZE) {
                    // 分块计算
                    for (int ii = i; ii < min(i + TILE_SIZE, M); ii++) {
                        for (int jj = j; jj < min(j + TILE_SIZE, N); jj++) {
                            float sum = 0.0f;
                            for (int kk = k; kk < min(k + TILE_SIZE, K); kk++) {
                                sum += A[ii * K + kk] * B[kk * N + jj];
                            }
                            C[ii * N + jj] += sum;
                        }
                    }
                }
            }
        }
    }
}
```

**图像处理应用**：
```c
// OpenACC图像模糊处理
void image_blur_openacc(unsigned char *input, unsigned char *output,
                       int width, int height, int channels) {
    int size = width * height * channels;

    #pragma acc data copyin(input[0:size]) copyout(output[0:size])
    {
        #pragma acc parallel loop gang(16) vector(16)
        for (int y = 1; y < height - 1; y++) {
            for (int x = 1; x < width - 1; x++) {
                for (int c = 0; c < channels; c++) {
                    float sum = 0.0f;
                    for (int dy = -1; dy <= 1; dy++) {
                        for (int dx = -1; dx <= 1; dx++) {
                            int nx = x + dx;
                            int ny = y + dy;
                            sum += input[(ny * width + nx) * channels + c];
                        }
                    }
                    output[(y * width + x) * channels + c] = (unsigned char)(sum / 9.0f);
                }
            }
        }
    }
}

// 性能优化版本
void optimized_image_blur(unsigned char *input, unsigned char *output,
                         int width, int height, int channels) {
    const int TILE_SIZE = 32;

    #pragma acc data copyin(input[0:width*height*channels]) \
                      copyout(output[0:width*height*channels])
    {
        #pragma acc parallel loop gang(TILE_SIZE) vector(TILE_SIZE)
        for (int y = 1; y < height - 1; y += TILE_SIZE) {
            for (int x = 1; x < width - 1; x += TILE_SIZE) {
                // 分块处理，减少内存访问
                unsigned char tile[TILE_SIZE+2][TILE_SIZE+2][3];

                // 加载数据块到共享内存
                #pragma acc loop gang(TILE_SIZE) vector(TILE_SIZE)
                for (int ty = 0; ty < TILE_SIZE + 2; ty++) {
                    for (int tx = 0; tx < TILE_SIZE + 2; tx++) {
                        int gy = y + ty - 1;
                        int gx = x + tx - 1;
                        if (gy >= 0 && gy < height && gx >= 0 && gx < width) {
                            for (int c = 0; c < channels; c++) {
                                tile[ty][tx][c] = input[(gy * width + gx) * channels + c];
                            }
                        }
                    }
                }

                // 执行模糊操作
                #pragma acc loop gang(TILE_SIZE) vector(TILE_SIZE)
                for (int ty = 1; ty <= TILE_SIZE; ty++) {
                    for (int tx = 1; tx <= TILE_SIZE; tx++) {
                        int gy = y + ty - 1;
                        int gx = x + tx - 1;

                        if (gy > 0 && gy < height - 1 && gx > 0 && gx < width - 1) {
                            for (int c = 0; c < channels; c++) {
                                float sum = 0.0f;
                                for (int dy = -1; dy <= 1; dy++) {
                                    for (int dx = -1; dx <= 1; dx++) {
                                        sum += tile[ty + dy][tx + dx][c];
                                    }
                                }
                                output[(gy * width + gx) * channels + c] =
                                    (unsigned char)(sum / 9.0f);
                            }
                        }
                    }
                }
            }
        }
    }
}
```

#### 13.1.8 OpenACC调试和性能分析
**调试工具**：
```bash
# 编译器信息输出
nvc -Minfo=accel -acc vector_add.c

# 运行时调试
export ACC_DEBUG=1
./vector_add

# 性能分析
export ACC_TIME=1
./vector_add

# 详细性能统计
export PGI_ACC_TIME=1
./vector_add
```

**性能分析工具**：
```c
// 使用OpenACC性能计数器
#include <openacc.h>
#include <stdio.h>

void performance_analysis() {
    acc_init(acc_device_nvidia);

    // 获取设备信息
    int device_num = acc_get_device_num(acc_device_nvidia);
    printf("当前设备: %d\n", device_num);

    // 内存信息
    size_t free_mem, total_mem;
    acc_get_property(device_num, acc_property_memory_free, &free_mem);
    acc_get_property(device_num, acc_property_memory_total, &total_mem);
    printf("内存使用: %zu / %zu MB\n",
           (total_mem - free_mem) / (1024*1024),
           total_mem / (1024*1024));

    // 执行时间统计
    double start_time = acc_clock();
    // ... OpenACC代码 ...
    double end_time = acc_clock();
    printf("执行时间: %f ms\n", end_time - start_time);
}

// 使用Nsight Systems分析
// nsys profile --trace=cuda,openacc ./your_program
```

**常见问题和解决方案**：
```c
// 问题1：数据传输开销大
// 解决：使用数据持久化
#pragma acc data copyin(a[0:n]) create(b[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        b[i] = a[i] * 2.0f;
    }

    // 复用b数组
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        b[i] = b[i] + 1.0f;
    }
}

// 问题2：循环依赖
// 解决：使用reduction或atomic
#pragma acc parallel loop reduction(+:sum)
for (int i = 0; i < n; i++) {
    sum += data[i];
}

// 问题3：内存访问不连续
// 解决：重新组织数据结构
struct Point {
    float x, y, z;
};

// 好的访问模式
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    points[i].x *= 2.0f;
    points[i].y *= 2.0f;
    points[i].z *= 2.0f;
}

// 避免的访问模式
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    x_coords[i] *= 2.0f;
}
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    y_coords[i] *= 2.0f;
}
```

#### 13.1.9 OpenACC最佳实践
**代码设计原则**：
```c
// 1. 渐进式加速
// 先用OpenACC指令标记热点函数
#pragma acc routine seq
float compute_element(float x, float y) {
    return sinf(x) * cosf(y) + sqrtf(x*x + y*y);
}

// 2. 数据管理优化
#pragma acc data copyin(input[0:n]) copyout(output[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        output[i] = compute_element(input[i], 1.0f);
    }
}

// 3. 循环优化
#pragma acc parallel loop gang vector
for (int i = 0; i < n; i++) {
    #pragma acc loop vector
    for (int j = 0; j < m; j++) {
        result[i][j] = matrix1[i][j] + matrix2[i][j];
    }
}

// 4. 内存访问模式
// 连续访问优于跨步访问
#pragma acc parallel loop
for (int i = 0; i < height; i++) {
    for (int j = 0; j < width; j++) {
        output[i * width + j] = input[i * width + j] * 2.0f;  // 好
    }
}
```

**性能优化策略**：
```c
// 1. 循环融合
// 不好的方式
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    temp[i] = a[i] + b[i];
}
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    result[i] = temp[i] * 2.0f;
}

// 好的方式
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    result[i] = (a[i] + b[i]) * 2.0f;
}

// 2. 减少数据传输
#pragma acc data copyin(a[0:n], b[0:n]) create(temp[0:n]) copyout(result[0:n])
{
    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        temp[i] = a[i] + b[i];
    }

    #pragma acc parallel loop
    for (int i = 0; i < n; i++) {
        result[i] = temp[i] * 2.0f;
    }
}

// 3. 使用合适的调度策略
#pragma acc parallel loop gang(128) vector(32)
for (int i = 0; i < n; i++) {
    result[i] = compute(data[i]);
}

// 4. 避免分支发散
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    float val = data[i];
    if (val > threshold) {
        result[i] = val * 2.0f;  // 尽量让所有线程执行相同路径
    } else {
        result[i] = val * 0.5f;
    }
}
```

**代码示例总结**：
```c
// 完整的OpenACC示例：热传导模拟
#include <openacc.h>
#include <math.h>

void heat_equation_solver(float *temperature, float *new_temperature,
                         int width, int height, float alpha, int iterations) {
    int size = width * height;

    #pragma acc data copy(temperature[0:size], new_temperature[0:size])
    {
        for (int iter = 0; iter < iterations; iter++) {
            #pragma acc parallel loop gang(16) vector(16)
            for (int y = 1; y < height - 1; y++) {
                for (int x = 1; x < width - 1; x++) {
                    int idx = y * width + x;
                    float center = temperature[idx];
                    float north = temperature[idx - width];
                    float south = temperature[idx + width];
                    float east = temperature[idx + 1];
                    float west = temperature[idx - 1];

                    new_temperature[idx] = center +
                        alpha * (north + south + east + west - 4.0f * center);
                }
            }

            // 交换数组
            #pragma acc parallel loop
            for (int i = 0; i < size; i++) {
                temperature[i] = new_temperature[i];
            }
        }
    }
}

// 编译和运行
// nvc -acc -gpu=cc80 -Minfo=accel heat_solver.c
// ./a.out
```

#### 13.1.10 OpenACC与其他框架比较
**OpenACC vs OpenMP**：
```c
// OpenACC - 专注于加速器
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// OpenMP - CPU多线程
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// OpenMP + OpenACC 混合
#pragma omp parallel for
for (int chunk = 0; chunk < num_chunks; chunk++) {
    #pragma acc parallel loop
    for (int i = chunk * chunk_size; i < (chunk + 1) * chunk_size; i++) {
        c[i] = a[i] + b[i];
    }
}
```

**OpenACC vs CUDA**：
```c
// CUDA - 显式GPU编程
__global__ void vector_add_cuda(float *a, float *b, float *c, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// OpenACC - 编译器自动优化
#pragma acc parallel loop
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}

// 选择建议：
// - 快速原型开发：OpenACC
// - 性能极致优化：CUDA
// - 可移植性要求：OpenACC
// - 现有代码迁移：OpenACC
```

这个扩展为读者提供了全面的OpenACC编程知识，从基础概念到高级优化技术，包含了大量实用的代码示例和最佳实践指导。

### 13.2 Thrust

#### 13.2.1 Thrust概述
**什么是Thrust**：
```cpp
// Thrust简介
// - NVIDIA开发的C++模板库
// - 类似STL的并行算法库
// - 支持CUDA设备和主机CPU
// - 提供高性能的并行原语
// - 自动选择最优执行后端
```

**Thrust vs STL**：
```cpp
// STL方式
std::vector<int> data = {5, 2, 8, 1, 9};
std::sort(data.begin(), data.end());

// Thrust方式
thrust::device_vector<int> d_data = {5, 2, 8, 1, 9};
thrust::sort(d_data.begin(), d_data.end());

// Thrust的优势：
// - 自动并行化
// - GPU加速
// - 更简洁的API
// - 更好的性能
```

**Thrust核心特性**：
- **高性能算法**：优化的并行排序、搜索、规约
- **容器抽象**：device_vector、host_vector等
- **执行策略**：串行、并行、GPU执行
- **互操作性**：与CUDA、OpenMP无缝集成
- **类型安全**：编译时类型检查

#### 13.2.2 Thrust容器
**device_vector**：
```cpp
#include <thrust/device_vector.h>
#include <thrust/host_vector.h>
#include <thrust/generate.h>
#include <thrust/sort.h>
#include <thrust/copy.h>
#include <algorithm>
#include <cstdlib>

// 设备向量
thrust::device_vector<int> d_vec(1000);
thrust::device_vector<float> d_floats(1024);

// 初始化设备向量
thrust::generate(d_vec.begin(), d_vec.end(), rand);
thrust::fill(d_floats.begin(), d_floats.end(), 1.0f);

// 访问元素
d_vec[0] = 42;
int first_element = d_vec[0];

// 向量操作
d_vec.push_back(100);  // 注意：device_vector不支持push_back
d_vec.resize(2000);    // 可以调整大小

// 向量大小和容量
size_t size = d_vec.size();
size_t capacity = d_vec.capacity();
bool empty = d_vec.empty();
```

**host_vector**：
```cpp
// 主机向量
thrust::host_vector<int> h_vec(1000);
thrust::host_vector<float> h_floats(1024);

// 从STL向量创建
std::vector<int> std_vec = {1, 2, 3, 4, 5};
thrust::host_vector<int> h_from_std(std_vec);

// 从设备向量创建
thrust::device_vector<int> d_data(100);
thrust::host_vector<int> h_data = d_data;  // 自动拷贝

// 拷贝回设备
d_data = h_data;  // 自动拷贝
```

**容器转换和数据传输**：
```cpp
// 设备和主机之间的数据传输
thrust::host_vector<int> h_data(100);
thrust::device_vector<int> d_data(100);

// 主机到设备
thrust::copy(h_data.begin(), h_data.end(), d_data.begin());

// 设备到主机
thrust::copy(d_data.begin(), d_data.end(), h_data.begin());

// 构造函数自动拷贝
thrust::device_vector<int> d_from_h(h_data);  // h_data -> d_from_h
thrust::host_vector<int> h_from_d(d_data);    // d_data -> h_from_d

// 使用assign方法
d_data.assign(h_data.begin(), h_data.end());
h_data.assign(d_data.begin(), d_data.end());
```

**自定义容器**：
```cpp
// 使用thrust::host_vector作为STL容器的分配器
#include <vector>
#include <thrust/system/omp/vector.h>

// OpenMP后端向量
thrust::host_vector<int, thrust::system::omp::tag> omp_vec(1000);

// TBB后端向量
#include <thrust/system/tbb/vector.h>
thrust::host_vector<int, thrust::system::tbb::tag> tbb_vec(1000);

// CUDA后端向量
thrust::device_vector<int> cuda_vec(1000);
```

#### 13.2.3 Thrust执行策略
**执行策略类型**：
```cpp
#include <thrust/execution_policy.h>

// 串行执行策略
thrust::host_vector<int> h_data(1000);
thrust::sort(thrust::seq, h_data.begin(), h_data.end());

// 并行执行策略（OpenMP）
thrust::sort(thrust::par, h_data.begin(), h_data.end());

// CUDA执行策略
thrust::device_vector<int> d_data(1000);
thrust::sort(thrust::cuda::par, d_data.begin(), d_data.end());

// TBB执行策略
thrust::sort(thrust::tbb::par, h_data.begin(), h_data.end());
```

**选择执行策略**：
```cpp
// 根据数据类型自动选择策略
template<typename Vector>
void sort_data(Vector& data) {
    if (std::is_same<Vector, thrust::device_vector<int>>::value) {
        thrust::sort(thrust::cuda::par, data.begin(), data.end());
    } else {
        thrust::sort(thrust::par, data.begin(), data.end());
    }
}

// 运行时选择策略
void runtime_strategy_selection() {
    bool use_gpu = true;  // 运行时决定
    thrust::device_vector<int> d_data(1000);
    thrust::host_vector<int> h_data(1000);

    if (use_gpu) {
        thrust::sort(thrust::cuda::par, d_data.begin(), d_data.end());
    } else {
        thrust::sort(thrust::par, h_data.begin(), h_data.end());
    }
}
```

**自定义执行策略**：
```cpp
// 创建自定义执行策略
struct custom_policy {
    using type = thrust::cuda::par_t;
};

template<typename Policy>
void custom_algorithm(Policy policy, thrust::device_vector<int>& data) {
    thrust::sort(policy, data.begin(), data.end());
}

// 使用自定义策略
void use_custom_policy() {
    thrust::device_vector<int> data(1000);
    custom_algorithm(thrust::cuda::par, data);
}
```

#### 13.2.4 Thrust并行算法
**排序算法**：
```cpp
#include <thrust/sort.h>

void sorting_examples() {
    // 基本排序
    thrust::device_vector<int> data = {5, 2, 8, 1, 9, 3};
    thrust::sort(data.begin(), data.end());  // 升序

    // 降序排序
    thrust::sort(data.begin(), data.end(), thrust::greater<int>());

    // 部分排序
    thrust::device_vector<int> partial_data = {5, 2, 8, 1, 9, 3, 7, 4};
    thrust::sort(partial_data.begin(), partial_data.begin() + 5);

    // 稳定排序
    thrust::device_vector<std::pair<int, char>> pairs = {
        {3, 'a'}, {1, 'b'}, {3, 'c'}, {1, 'd'}
    };
    thrust::stable_sort(pairs.begin(), pairs.end());

    // 根据键排序
    thrust::device_vector<int> keys = {3, 1, 4, 1, 5};
    thrust::device_vector<char> values = {'c', 'a', 'd', 'a', 'e'};
    thrust::sort_by_key(keys.begin(), keys.end(), values.begin());
}
```

**搜索算法**：
```cpp
#include <thrust/binary_search.h>
#include <thrust/find.h>

void searching_examples() {
    thrust::device_vector<int> data = {1, 2, 3, 5, 8, 13, 21, 34};

    // 二分查找
    bool found = thrust::binary_search(data.begin(), data.end(), 8);

    // 查找元素
    auto it = thrust::find(data.begin(), data.end(), 13);
    bool exists = (it != data.end());

    // 查找满足条件的元素
    struct is_even {
        __host__ __device__
        bool operator()(int x) const {
            return x % 2 == 0;
        }
    };
    auto even_it = thrust::find_if(data.begin(), data.end(), is_even());

    // 统计满足条件的元素
    int even_count = thrust::count_if(data.begin(), data.end(), is_even());
}
```

**规约操作**：
```cpp
#include <thrust/reduce.h>
#include <thrust/transform_reduce.h>

void reduction_examples() {
    thrust::device_vector<int> data = {1, 2, 3, 4, 5};

    // 求和
    int sum = thrust::reduce(data.begin(), data.end(), 0, thrust::plus<int>());

    // 最大值
    int max_val = thrust::reduce(data.begin(), data.end(),
                               std::numeric_limits<int>::min(),
                               thrust::maximum<int>());

    // 最小值
    int min_val = thrust::reduce(data.begin(), data.end(),
                               std::numeric_limits<int>::max(),
                               thrust::minimum<int>());

    // 自定义规约操作
    struct multiply {
        __host__ __device__
        int operator()(int a, int b) const {
            return a * b;
        }
    };
    int product = thrust::reduce(data.begin(), data.end(), 1, multiply());

    // 变换后规约
    struct square {
        __host__ __device__
        int operator()(int x) const {
            return x * x;
        }
    };
    int sum_of_squares = thrust::transform_reduce(
        data.begin(), data.end(), square(), 0, thrust::plus<int>()
    );
}
```

**变换操作**：
```cpp
#include <thrust/transform.h>

void transform_examples() {
    thrust::device_vector<float> input = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f};
    thrust::device_vector<float> output(input.size());

    // 一元变换
    struct square_root {
        __host__ __device__
        float operator()(float x) const {
            return sqrtf(x);
        }
    };
    thrust::transform(input.begin(), input.end(), output.begin(), square_root());

    // 二元变换
    thrust::device_vector<float> input2 = {2.0f, 3.0f, 4.0f, 5.0f, 6.0f};
    thrust::device_vector<float> result(input.size());

    struct add {
        __host__ __device__
        float operator()(float a, float b) const {
            return a + b;
        }
    };
    thrust::transform(input.begin(), input.end(), input2.begin(),
                     result.begin(), add());

    // 就地变换
    thrust::transform(input.begin(), input.end(), input.begin(),
                     thrust::placeholders::_1 * 2.0f);
}
```

**扫描操作**：
```cpp
#include <thrust/scan.h>

void scan_examples() {
    thrust::device_vector<int> input = {1, 2, 3, 4, 5};
    thrust::device_vector<int> output(input.size());

    // 前缀和
    thrust::inclusive_scan(input.begin(), input.end(), output.begin());

    // 排他前缀和
    thrust::exclusive_scan(input.begin(), input.end(), output.begin());

    // 自定义扫描操作
    struct multiply {
        __host__ __device__
        int operator()(int a, int b) const {
            return a * b;
        }
    };
    thrust::inclusive_scan(input.begin(), input.end(), output.begin(), multiply());

    // 条件扫描
    thrust::device_vector<int> flags = {1, 0, 1, 1, 0};
    thrust::device_vector<int> values = {10, 20, 30, 40, 50};
    thrust::device_vector<int> result(values.size());

    // 只对flags为1的位置进行扫描
    thrust::inclusive_scan_by_key(flags.begin(), flags.end(),
                                 values.begin(), result.begin());
}
```

#### 13.2.5 Thrust高级特性
**自定义函数对象**：
```cpp
// 一元函数对象
struct custom_unary_op {
    float factor;
    int offset;

    custom_unary_op(float f, int o) : factor(f), offset(o) {}

    __host__ __device__
    float operator()(float x) const {
        return x * factor + offset;
    }
};

// 二元函数对象
struct custom_binary_op {
    __host__ __device__
    float operator()(float a, float b) const {
        return (a + b) * 0.5f;  // 平均值
    }
};

// 使用自定义函数对象
void custom_functors() {
    thrust::device_vector<float> data(100);
    thrust::device_vector<float> result(100);

    // 使用带参数的函数对象
    custom_unary_op op(2.0f, 10);
    thrust::transform(data.begin(), data.end(), result.begin(), op);

    // 使用lambda表达式（C++11）
    auto lambda_op = [] __device__ (float x) { return x * x + 1.0f; };
    thrust::transform(data.begin(), data.end(), result.begin(), lambda_op);
}
```

**谓词和比较器**：
```cpp
// 自定义谓词
struct is_prime {
    __host__ __device__
    bool operator()(int n) const {
        if (n <= 1) return false;
        for (int i = 2; i * i <= n; i++) {
            if (n % i == 0) return false;
        }
        return true;
    }
};

// 自定义比较器
struct custom_comparator {
    __host__ __device__
    bool operator()(const std::pair<int, std::string>& a,
                   const std::pair<int, std::string>& b) const {
        return a.first < b.first;  // 按第一个元素排序
    }
};

void predicates_and_comparators() {
    thrust::device_vector<int> numbers = {2, 3, 4, 5, 6, 7, 8, 9, 10, 11};

    // 过滤素数
    thrust::device_vector<int> primes(numbers.size());
    auto end_iter = thrust::copy_if(numbers.begin(), numbers.end(),
                                  primes.begin(), is_prime());
    primes.resize(end_iter - primes.begin());

    // 自定义排序
    thrust::device_vector<std::pair<int, std::string>> pairs = {
        {3, "three"}, {1, "one"}, {4, "four"}, {2, "two"}
    };
    thrust::sort(pairs.begin(), pairs.end(), custom_comparator());
}
```

**迭代器和范围**：
```cpp
#include <thrust/iterator/counting_iterator.h>
#include <thrust/iterator/transform_iterator.h>
#include <thrust/iterator/zip_iterator.h>

void iterator_examples() {
    // 计数迭代器
    thrust::counting_iterator<int> first(0);
    thrust::counting_iterator<int> last(100);

    thrust::device_vector<int> indices(100);
    thrust::copy(first, last, indices.begin());

    // 变换迭代器
    struct square {
        __host__ __device__
        int operator()(int x) const {
            return x * x;
        }
    };
    auto transform_iter = thrust::make_transform_iterator(first, square());
    thrust::device_vector<int> squares(100);
    thrust::copy(transform_iter, transform_iter + 100, squares.begin());

    // zip迭代器
    thrust::device_vector<int> keys = {1, 2, 3, 4, 5};
    thrust::device_vector<float> values = {1.1f, 2.2f, 3.3f, 4.4f, 5.5f};

    auto zip_begin = thrust::make_zip_iterator(thrust::make_tuple(
        keys.begin(), values.begin()));
    auto zip_end = thrust::make_zip_iterator(thrust::make_tuple(
        keys.end(), values.end()));

    // 处理配对数据
    thrust::for_each(zip_begin, zip_end,
                    [] __device__ (thrust::tuple<int, float>& pair) {
                        thrust::get<1>(pair) *= thrust::get<0>(pair);
                    });
}
```

#### 13.2.6 Thrust与CUDA集成
**CUDA内核与Thrust混合编程**：
```cpp
#include <thrust/device_vector.h>
#include <thrust/host_vector.h>

// CUDA核函数
__global__ void cuda_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] = data[idx] * 2.0f + 1.0f;
    }
}

// Thrust管理数据，CUDA执行计算
void mixed_cuda_thrust() {
    thrust::device_vector<float> data(1000);

    // 初始化数据
    thrust::sequence(data.begin(), data.end(), 0.0f, 1.0f);

    // 调用CUDA核函数
    int threadsPerBlock = 256;
    int blocksPerGrid = (data.size() + threadsPerBlock - 1) / threadsPerBlock;
    cuda_kernel<<<blocksPerGrid, threadsPerBlock>>>(
        thrust::raw_pointer_cast(data.data()), data.size());

    // 使用Thrust进行后续处理
    thrust::sort(data.begin(), data.end());
    float sum = thrust::reduce(data.begin(), data.end());
}
```

**Thrust与CUDA流**：
```cpp
// 使用CUDA流与Thrust
void thrust_with_cuda_streams() {
    cudaStream_t stream1, stream2;
    cudaStreamCreate(&stream1);
    cudaStreamCreate(&stream2);

    thrust::device_vector<float> data1(1000);
    thrust::device_vector<float> data2(1000);

    // 在不同流中执行Thrust操作
    // 注意：Thrust不直接支持流，但可以与CUDA流结合使用

    // 方法1：使用CUDA API管理流
    cudaStreamSynchronize(stream1);
    cudaStreamSynchronize(stream2);

    cudaStreamDestroy(stream1);
    cudaStreamDestroy(stream2);
}
```

**Thrust内存管理**：
```cpp
// 自定义内存分配器
#include <thrust/system/cuda/vector.h>
#include <thrust/system/cuda/memory.h>

// 使用CUDA内存池
void thrust_memory_pool() {
    // 创建设备向量
    thrust::device_vector<int> data(1000);

    // 获取原始指针
    int* raw_ptr = thrust::raw_pointer_cast(data.data());

    // 可以与CUDA API直接使用
    cudaMemset(raw_ptr, 0, data.size() * sizeof(int));

    // 内存拷贝
    thrust::host_vector<int> h_data(1000);
    thrust::copy(h_data.begin(), h_data.end(), data.begin());
}

// 内存对齐优化
void aligned_memory_access() {
    // Thrust自动处理内存对齐
    thrust::device_vector<float4> aligned_data(256);  // 16字节对齐

    // 使用对齐的内存访问模式
    thrust::transform(aligned_data.begin(), aligned_data.end(),
                     aligned_data.begin(),
                     [] __device__ (float4 x) {
                         return make_float4(x.x * 2.0f, x.y * 2.0f,
                                          x.z * 2.0f, x.w * 2.0f);
                     });
}
```

#### 13.2.7 Thrust性能优化
**算法选择优化**：
```cpp
// 根据数据特性选择最优算法
template<typename Vector>
void optimized_sort(Vector& data) {
    size_t n = data.size();

    if (n < 1000) {
        // 小数据集使用串行排序
        thrust::sort(thrust::seq, data.begin(), data.end());
    } else if (n < 100000) {
        // 中等数据集使用并行排序
        thrust::sort(thrust::par, data.begin(), data.end());
    } else {
        // 大数据集使用GPU排序
        thrust::sort(thrust::cuda::par, data.begin(), data.end());
    }
}

// 内存访问模式优化
void memory_access_optimization() {
    thrust::device_vector<float> data(1000000);

    // 连续内存访问（推荐）
    thrust::transform(data.begin(), data.end(), data.begin(),
                     [] __device__ (float x) { return x * 2.0f; });

    // 避免随机访问模式
    // thrust::device_vector<int> indices = {...};
    // thrust::gather(indices.begin(), indices.end(), data.begin(), result.begin());
}
```

**并行度优化**：
```cpp
// 调整并行度参数
void parallelism_tuning() {
    thrust::device_vector<int> data(1000000);

    // 使用自定义执行策略
    thrust::cuda::par_t custom_policy;
    custom_policy.block_size = 512;  // 设置线程块大小
    custom_policy.grid_size = 1024;  // 设置网格大小

    thrust::sort(custom_policy, data.begin(), data.end());

    // 根据GPU特性调整参数
    int device;
    cudaGetDevice(&device);

    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, device);

    int optimal_block_size = prop.maxThreadsPerBlock;
    int optimal_grid_size = (data.size() + optimal_block_size - 1) / optimal_block_size;
}
```

**内存带宽优化**：
```cpp
// 减少内存传输
void bandwidth_optimization() {
    // 使用就地操作减少内存使用
    thrust::device_vector<float> data(1000000);

    // 好的做法：就地变换
    thrust::transform(data.begin(), data.end(), data.begin(),
                     thrust::placeholders::_1 * 2.0f);

    // 避免：创建临时向量
    // thrust::device_vector<float> temp = data;
    // thrust::transform(data.begin(), data.end(), temp.begin(), ...);

    // 批量操作减少kernel启动开销
    thrust::device_vector<float> input(1000000);
    thrust::device_vector<float> output(1000000);

    // 单次kernel调用
    thrust::transform(input.begin(), input.end(), output.begin(),
                     [] __device__ (float x) {
                         return sqrtf(x * x + 1.0f);
                     });
}
```

#### 13.2.8 Thrust实际应用案例
**图像处理**：
```cpp
// 图像模糊处理
void image_blur_thrust(unsigned char* input, unsigned char* output,
                      int width, int height, int channels) {
    int size = width * height * channels;

    // 创建设备向量
    thrust::device_vector<unsigned char> d_input(input, input + size);
    thrust::device_vector<unsigned char> d_output(size);

    // 高斯模糊核
    float kernel[9] = {1.0f/16, 2.0f/16, 1.0f/16,
                      2.0f/16, 4.0f/16, 2.0f/16,
                      1.0f/16, 2.0f/16, 1.0f/16};

    thrust::device_vector<float> d_kernel(kernel, kernel + 9);

    // 实现图像模糊（简化版本）
    auto blur_op = [=] __device__ (int idx) -> unsigned char {
        if (idx < channels || idx >= size - channels) return input[idx];

        float sum = 0.0f;
        int row = idx / (width * channels);
        int col = (idx % (width * channels)) / channels;

        for (int dy = -1; dy <= 1; dy++) {
            for (int dx = -1; dx <= 1; dx++) {
                int ny = row + dy;
                int nx = col + dx;
                if (ny >= 0 && ny < height && nx >= 0 && nx < width) {
                    int nidx = (ny * width + nx) * channels + (idx % channels);
                    int kidx = (dy + 1) * 3 + (dx + 1);
                    sum += d_input[nidx] * d_kernel[kidx];
                }
            }
        }
        return static_cast<unsigned char>(sum);
    };

    thrust::counting_iterator<int> first(0);
    thrust::counting_iterator<int> last(size);

    thrust::transform(first, last, d_output.begin(), blur_op);

    // 拷贝结果
    thrust::copy(d_output.begin(), d_output.end(), output);
}
```

**数值计算**：
```cpp
// 矩阵乘法
void matrix_multiply_thrust(float* A, float* B, float* C,
                           int M, int N, int K) {
    // 创建设备向量
    thrust::device_vector<float> d_A(A, A + M * K);
    thrust::device_vector<float> d_B(B, B + K * N);
    thrust::device_vector<float> d_C(M * N, 0.0f);

    // 使用Thrust实现矩阵乘法
    auto multiply_op = [=] __device__ (thrust::tuple<int, int> idx) -> float {
        int row = thrust::get<0>(idx);
        int col = thrust::get<1>(idx);
        float sum = 0.0f;

        for (int k = 0; k < K; k++) {
            sum += d_A[row * K + k] * d_B[k * N + col];
        }
        return sum;
    };

    // 生成索引对
    thrust::device_vector<int> rows(M * N);
    thrust::device_vector<int> cols(M * N);

    thrust::counting_iterator<int> first(0);
    thrust::counting_iterator<int> last(M * N);

    // 填充行索引
    for (int i = 0; i < M; i++) {
        thrust::fill(rows.begin() + i * N, rows.begin() + (i + 1) * N, i);
    }

    // 填充列索引
    for (int j = 0; j < N; j++) {
        thrust::fill(cols.begin() + j * M, cols.begin() + (j + 1) * M, j);
    }

    // 执行矩阵乘法
    thrust::transform(thrust::make_zip_iterator(thrust::make_tuple(rows.begin(), cols.begin())),
                     thrust::make_zip_iterator(thrust::make_tuple(rows.end(), cols.end())),
                     d_C.begin(), multiply_op);

    // 拷贝结果
    thrust::copy(d_C.begin(), d_C.end(), C);
}
```

**统计分析**：
```cpp
// 数据统计分析
struct statistics_result {
    float mean;
    float variance;
    float min_val;
    float max_val;
    int count;
};

statistics_result analyze_data_thrust(const std::vector<float>& data) {
    // 创建设备向量
    thrust::device_vector<float> d_data(data.begin(), data.end());

    statistics_result result;

    // 计算均值
    float sum = thrust::reduce(d_data.begin(), d_data.end());
    result.mean = sum / d_data.size();

    // 计算方差
    auto variance_op = [mean = result.mean] __device__ (float x) {
        float diff = x - mean;
        return diff * diff;
    };
    float variance_sum = thrust::transform_reduce(
        d_data.begin(), d_data.end(), variance_op, 0.0f, thrust::plus<float>()
    );
    result.variance = variance_sum / d_data.size();

    // 计算最值
    auto min_max = thrust::minmax_element(d_data.begin(), d_data.end());
    result.min_val = *min_max.first;
    result.max_val = *min_max.second;

    // 计算数量
    result.count = d_data.size();

    return result;
}
```

#### 13.2.9 Thrust调试和性能分析
**调试技巧**：
```cpp
// 使用thrust::host_vector进行调试
void debug_thrust_code() {
    // 在主机上测试逻辑
    thrust::host_vector<int> h_data = {5, 2, 8, 1, 9};
    thrust::sort(h_data.begin(), h_data.end());

    // 验证结果
    for (size_t i = 0; i < h_data.size(); i++) {
        std::cout << h_data[i] << " ";
    }
    std::cout << std::endl;

    // 然后在设备上运行
    thrust::device_vector<int> d_data = h_data;
    thrust::sort(d_data.begin(), d_data.end());

    // 拷贝回主机验证
    h_data = d_data;
}
```

**性能分析**：
```cpp
#include <chrono>

// 性能基准测试
template<typename Vector>
double benchmark_sort(Vector& data, const std::string& name) {
    auto start = std::chrono::high_resolution_clock::now();

    thrust::sort(data.begin(), data.end());

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

    std::cout << name << " sort time: " << duration.count() << " microseconds" << std::endl;
    return duration.count();
}

void performance_comparison() {
    size_t n = 1000000;
    std::vector<int> std_data(n);
    thrust::host_vector<int> thrust_data(n);
    thrust::device_vector<int> cuda_data(n);

    // 填充随机数据
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_int_distribution<> dis(1, 1000000);

    for (size_t i = 0; i < n; i++) {
        int val = dis(gen);
        std_data[i] = val;
        thrust_data[i] = val;
        cuda_data[i] = val;
    }

    // 性能比较
    benchmark_sort(std_data, "STL");
    benchmark_sort(thrust_data, "Thrust (Host)");
    benchmark_sort(cuda_data, "Thrust (CUDA)");
}
```

#### 13.2.10 Thrust最佳实践
**代码设计原则**：
```cpp
// 1. 选择合适的容器
void container_selection() {
    // 小数据集：使用host_vector
    thrust::host_vector<int> small_data(100);

    // 大数据集：使用device_vector
    thrust::device_vector<float> large_data(1000000);

    // 需要频繁主机设备传输：考虑内存映射
    thrust::device_vector<int> mapped_data(10000);
}

// 2. 选择合适的执行策略
void execution_policy_selection() {
    // CPU密集型：使用par
    thrust::host_vector<int> cpu_data(1000000);
    thrust::sort(thrust::par, cpu_data.begin(), cpu_data.end());

    // GPU密集型：使用cuda::par
    thrust::device_vector<int> gpu_data(1000000);
    thrust::sort(thrust::cuda::par, gpu_data.begin(), gpu_data.end());
}

// 3. 避免不必要的数据传输
void minimize_data_transfer() {
    // 好的做法：就地操作
    thrust::device_vector<int> data(1000);
    thrust::sort(data.begin(), data.end());  // 就地排序

    // 避免：频繁的主机设备传输
    // thrust::host_vector<int> h_data = d_data;  // 避免频繁拷贝
    // ... 处理 ...
    // d_data = h_data;  // 避免频繁拷贝
}

// 4. 使用合适的算法
void algorithm_selection() {
    // 排序：根据数据大小选择
    // 搜索：使用二分查找而不是线性搜索
    // 规约：使用thrust::reduce而不是手动循环
}
```

**性能优化策略**：
```cpp
// 1. 内存对齐
void memory_alignment() {
    // 使用thrust::device_vector自动处理对齐
    thrust::device_vector<float4> aligned_data(256);  // 16字节对齐

    // 避免：手动分配可能不对齐的内存
}

// 2. 减少kernel启动开销
void reduce_kernel_launch() {
    // 批量操作
    thrust::device_vector<float> data(1000000);

    // 好的做法：单次变换
    thrust::transform(data.begin(), data.end(), data.begin(),
                     [] __device__ (float x) { return x * x + 1.0f; });

    // 避免：多次小操作
    // thrust::transform(...); thrust::transform(...); thrust::transform(...);
}

// 3. 利用并行性
void maximize_parallelism() {
    // 大数据集使用GPU
    thrust::device_vector<int> large_data(10000000);
    thrust::sort(thrust::cuda::par, large_data.begin(), large_data.end());

    // 小数据集使用CPU
    thrust::host_vector<int> small_data(1000);
    thrust::sort(thrust::par, small_data.begin(), small_data.end());
}
```

**代码示例总结**：
```cpp
// 完整的Thrust应用示例：并行快速排序
#include <thrust/sort.h>
#include <thrust/partition.h>
#include <thrust/device_vector.h>

class parallel_quick_sort {
private:
    static const int THRESHOLD = 1000;

public:
    template<typename RandomAccessIterator>
    void operator()(RandomAccessIterator begin, RandomAccessIterator end) {
        if (end - begin <= THRESHOLD) {
            // 小数据集使用标准排序
            thrust::sort(begin, end);
        } else {
            // 大数据集递归分区
            auto pivot = select_pivot(begin, end);
            auto mid = thrust::partition(begin, end,
                                       [pivot] __device__ (typename RandomAccessIterator::value_type x) {
                                           return x < pivot;
                                       });
            // 递归排序左右两部分
            operator()(begin, mid);
            operator()(mid, end);
        }
    }

private:
    template<typename RandomAccessIterator>
    typename RandomAccessIterator::value_type
    select_pivot(RandomAccessIterator begin, RandomAccessIterator end) {
        // 简化的pivot选择
        return *(begin + (end - begin) / 2);
    }
};

// 使用示例
void quick_sort_example() {
    thrust::device_vector<int> data(1000000);

    // 填充随机数据
    thrust::generate(data.begin(), data.end(),
                   []() { return rand() % 1000000; });

    // 执行并行快速排序
    parallel_quick_sort sort_func;
    sort_func(data.begin(), data.end());
}

// 编译和运行
// nvcc -O3 -std=c++11 quick_sort_example.cu -o quick_sort
```

这个扩展为读者提供了全面的Thrust编程知识，从基础概念到高级优化技术，包含了大量实用的代码示例和最佳实践指导。

### 13.3 TBB (Threading Building Blocks)

Intel Threading Building Blocks (TBB) 是一个高性能的C++并行编程库，提供了高级抽象来简化多线程编程。TBB通过任务调度器和丰富的并行算法库，让开发者能够轻松构建可扩展的并行应用程序。

#### 13.3.1 TBB核心概念

**任务调度系统**：
TBB采用工作窃取（Work-Stealing）调度算法，自动平衡负载：

```cpp
#include <tbb/tbb.h>
#include <iostream>
#include <vector>

// TBB任务调度示例
void task_scheduling_example() {
    // 获取硬件线程数
    int num_threads = tbb::task_scheduler_init::default_num_threads();
    std::cout << "Available threads: " << num_threads << std::endl;

    // 初始化任务调度器
    tbb::task_scheduler_init init(num_threads);

    // 并行for_each示例
    std::vector<int> data(1000000);
    for (int i = 0; i < 1000000; i++) {
        data[i] = i;
    }

    // 使用parallel_for_each并行处理
    tbb::parallel_for_each(data.begin(), data.end(),
        [](int& value) {
            value = value * 2;
        }
    );

    std::cout << "First element after processing: " << data[0] << std::endl;
}
```

**任务粒度控制**：
```cpp
// 任务粒度控制示例
void granularity_control_example() {
    const int N = 1000000;
    std::vector<int> data(N);

    // 使用自定义粒度控制
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N, 1000),  // 每个任务处理1000个元素
        [&](const tbb::blocked_range<int>& range) {
            for (int i = range.begin(); i != range.end(); ++i) {
                data[i] = i * i;
            }
        }
    );
}
```

#### 13.3.2 核心并行算法

**parallel_for** - 并行循环：
```cpp
#include <tbb/parallel_for.h>
#include <tbb/blocked_range.h>

// 并行for示例：矩阵乘法
void parallel_matrix_multiplication() {
    const int N = 1000;
    std::vector<std::vector<double>> A(N, std::vector<double>(N, 1.0));
    std::vector<std::vector<double>> B(N, std::vector<double>(N, 2.0));
    std::vector<std::vector<double>> C(N, std::vector<double>(N, 0.0));

    // 并行计算矩阵乘法 C = A * B
    tbb::parallel_for(
        tbb::blocked_range2d<int>(0, N, 64, 0, N, 64),  // 64x64分块
        [&](const tbb::blocked_range2d<int>& r) {
            for (size_t i = r.rows().begin(); i != r.rows().end(); ++i) {
                for (size_t j = r.cols().begin(); j != r.cols().end(); ++j) {
                    double sum = 0.0;
                    for (size_t k = 0; k < N; ++k) {
                        sum += A[i][k] * B[k][j];
                    }
                    C[i][j] = sum;
                }
            }
        }
    );
}

// 简单parallel_for示例
void simple_parallel_for() {
    const int N = 1000000;
    std::vector<int> data(N);

    // 并行初始化数组
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N),
        [&](const tbb::blocked_range<int>& range) {
            for (int i = range.begin(); i < range.end(); ++i) {
                data[i] = i * i;
            }
        }
    );
}
```

**parallel_reduce** - 并行归约：
```cpp
#include <tbb/parallel_reduce.h>
#include <tbb/blocked_range.h>

// 并行求和示例
double parallel_sum_example() {
    const int N = 1000000;
    std::vector<double> data(N);

    // 初始化数据
    for (int i = 0; i < N; i++) {
        data[i] = i + 1;
    }

    // 并行求和
    double sum = tbb::parallel_reduce(
        tbb::blocked_range<int>(0, N),
        0.0,  // 初始值
        [&](const tbb::blocked_range<int>& r, double local_sum) {
            // 局部计算
            for (int i = r.begin(); i < r.end(); ++i) {
                local_sum += data[i];
            }
            return local_sum;
        },
        std::plus<double>()  // 合并函数
    );

    return sum;
}

// 并行最大值查找
double parallel_max_example() {
    const int N = 1000000;
    std::vector<double> data(N);

    // 初始化随机数据
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<> dis(0.0, 100.0);

    for (int i = 0; i < N; i++) {
        data[i] = dis(gen);
    }

    // 并行查找最大值
    double max_val = tbb::parallel_reduce(
        tbb::blocked_range<int>(0, N),
        -std::numeric_limits<double>::infinity(),
        [&](const tbb::blocked_range<int>& r, double local_max) {
            for (int i = r.begin(); i < r.end(); ++i) {
                local_max = std::max(local_max, data[i]);
            }
            return local_max;
        },
        [](double x, double y) { return std::max(x, y); }
    );

    return max_val;
}
```

**parallel_scan** - 并行前缀和：
```cpp
#include <tbb/parallel_scan.h>

// 并行前缀和示例
void parallel_prefix_sum() {
    const int N = 1000000;
    std::vector<int> input(N);
    std::vector<int> output(N);

    // 初始化输入数据
    for (int i = 0; i < N; i++) {
        input[i] = i + 1;
    }

    // 并行前缀和计算
    tbb::parallel_scan(
        tbb::blocked_range<int>(0, N),
        0,  // 初始值
        [&](const tbb::blocked_range<int>& r, int local_sum, bool is_final_scan) {
            // 前向扫描阶段
            for (int i = r.begin(); i < r.end(); ++i) {
                local_sum += input[i];
                if (is_final_scan) {
                    output[i] = local_sum;
                }
            }
            return local_sum;
        },
        std::plus<int>()
    );
}
```

**parallel_sort** - 并行排序：
```cpp
#include <tbb/parallel_sort.h>

// 并行排序示例
void parallel_sort_example() {
    const int N = 1000000;
    std::vector<int> data(N);

    // 生成随机数据
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_int_distribution<> dis(1, 1000000);

    for (int i = 0; i < N; i++) {
        data[i] = dis(gen);
    }

    // 并行排序
    tbb::parallel_sort(data.begin(), data.end());

    // 验证排序结果
    bool is_sorted = std::is_sorted(data.begin(), data.end());
    std::cout << "Array is sorted: " << (is_sorted ? "Yes" : "No") << std::endl;
}
```

#### 13.3.3 TBB容器

**concurrent_vector** - 并发向量：
```cpp
#include <tbb/concurrent_vector.h>

// 并发向量示例
void concurrent_vector_example() {
    tbb::concurrent_vector<int> vec;

    // 并行添加元素
    tbb::parallel_for(0, 1000000,
        [&](int i) {
            vec.push_back(i * 2);
        }
    );

    std::cout << "Vector size: " << vec.size() << std::endl;

    // 并行访问元素
    tbb::parallel_for(0, std::min(100, (int)vec.size()),
        [&](int i) {
            std::cout << "Element " << i << ": " << vec[i] << std::endl;
        }
    );
}
```

**concurrent_queue** - 并发队列：
```cpp
#include <tbb/concurrent_queue.h>
#include <thread>
#include <chrono>

// 生产者-消费者模式
void concurrent_queue_example() {
    tbb::concurrent_queue<int> queue;
    bool finished = false;

    // 生产者线程
    std::thread producer([&]() {
        for (int i = 0; i < 1000; i++) {
            queue.push(i);
            std::this_thread::sleep_for(std::chrono::milliseconds(1));
        }
        queue.push(-1);  // 结束标记
    });

    // 消费者线程
    std::thread consumer([&]() {
        int value;
        while (true) {
            if (queue.try_pop(value)) {
                if (value == -1) break;  // 结束标记
                std::cout << "Consumed: " << value << std::endl;
            } else {
                std::this_thread::sleep_for(std::chrono::milliseconds(10));
            }
        }
    });

    producer.join();
    consumer.join();
}
```

**concurrent_unordered_map** - 并发哈希表：
```cpp
#include <tbb/concurrent_unordered_map.h>

// 并发哈希表示例
void concurrent_map_example() {
    tbb::concurrent_unordered_map<std::string, int> word_count;

    // 并行统计单词频率
    std::vector<std::string> words = {
        "hello", "world", "hello", "tbb", "world", "parallel"
    };

    tbb::parallel_for_each(words.begin(), words.end(),
        [&](const std::string& word) {
            ++word_count[word];
        }
    );

    // 输出结果
    for (const auto& pair : word_count) {
        std::cout << pair.first << ": " << pair.second << std::endl;
    }
}
```

**concurrent_hash_map** - 并发哈希映射：
```cpp
#include <tbb/concurrent_hash_map.h>

// 并发哈希映射示例
void concurrent_hash_map_example() {
    tbb::concurrent_hash_map<int, std::string> cache;

    // 并行填充缓存
    tbb::parallel_for(0, 10000,
        [&](int i) {
            tbb::concurrent_hash_map<int, std::string>::accessor acc;
            cache.insert(acc, std::make_pair(i, "value_" + std::to_string(i)));
        }
    );

    // 并行查询
    tbb::parallel_for(0, 100,
        [&](int i) {
            tbb::concurrent_hash_map<int, std::string>::const_accessor acc;
            if (cache.find(acc, i)) {
                std::cout << "Found: " << i << " -> " << acc->second << std::endl;
            }
        }
    );
}
```

#### 13.3.4 高级并行模式

**任务流图 (Flow Graph)**：
```cpp
#include <tbb/flow_graph.h>

// 任务流图示例
void flow_graph_example() {
    using namespace tbb::flow;

    // 创建流图
    graph g;

    // 定义节点
    source_node<int> source(g, [&](int& value) -> bool {
        static int counter = 0;
        if (counter < 10) {
            value = counter++;
            return true;
        }
        return false;
    });

    function_node<int, int> square(g, 1, [](int x) {
        return x * x;
    });

    function_node<int, int> add_one(g, 1, [](int x) {
        return x + 1;
    });

    function_node<int> output(g, 1, [](int x) {
        std::cout << "Result: " << x << std::endl;
    });

    // 连接节点
    make_edge(source, square);
    make_edge(square, add_one);
    make_edge(add_one, output);

    // 执行流图
    g.wait_for_all();
}
```

**pipeline并行**：
```cpp
#include <tbb/pipeline.h>

// 管道并行示例
void pipeline_example() {
    using namespace tbb;

    // 定义过滤器
    filter<void, int> source_filter(
        filter::serial_in_order,
        [](flow_control& fc) -> int {
            static int counter = 0;
            if (counter < 1000000) {
                return counter++;
            }
            fc.stop();
            return -1;
        }
    );

    filter<int, int> square_filter(
        filter::parallel,
        [](int x) -> int {
            return x * x;
        }
    );

    filter<int, void> output_filter(
        filter::serial_in_order,
        [](int x) {
            // 处理结果
            std::cout << "Processed: " << x << std::endl;
        }
    );

    // 创建管道
    pipeline pipe;
    pipe.add_filter(source_filter);
    pipe.add_filter(square_filter);
    pipe.add_filter(output_filter);

    // 执行管道
    pipe.run(4);  // 使用4个线程

    pipe.clear();
}
```

**任务组 (Task Groups)**：
```cpp
#include <tbb/task_group.h>

// 任务组示例
void task_group_example() {
    tbb::task_group tg;

    // 添加任务
    tg.run([]() {
        std::cout << "Task 1 running" << std::endl;
        std::this_thread::sleep_for(std::chrono::seconds(1));
        std::cout << "Task 1 completed" << std::endl;
    });

    tg.run([]() {
        std::cout << "Task 2 running" << std::endl;
        std::this_thread::sleep_for(std::chrono::seconds(2));
        std::cout << "Task 2 completed" << std::endl;
    });

    tg.run([]() {
        std::cout << "Task 3 running" << std::endl;
        std::this_thread::sleep_for(std::chrono::seconds(1));
        std::cout << "Task 3 completed" << std::endl;
    });

    // 等待所有任务完成
    tg.wait();
}
```

#### 13.3.5 内存管理

**内存池 (Memory Pool)**：
```cpp
#include <tbb/scalable_allocator.h>
#include <tbb/tbb_allocator.h>

// 自定义内存分配器示例
template<typename T>
using scalable_vector = std::vector<T, tbb::scalable_allocator<T>>;

// 内存池使用示例
void memory_pool_example() {
    // 使用可扩展分配器的向量
    scalable_vector<int> scalable_vec;
    scalable_vec.reserve(1000000);

    // 并行填充
    tbb::parallel_for(0, 1000000,
        [&](int i) {
            scalable_vec.push_back(i);
        }
    );

    std::cout << "Vector size: " << scalable_vec.size() << std::endl;
}
```

**对象池 (Object Pool)**：
```cpp
#include <tbb/concurrent_queue.h>

// 对象池实现
template<typename T>
class object_pool {
private:
    tbb::concurrent_queue<T*> pool_;
    std::function<T*()> factory_;
    std::function<void(T*)> cleanup_;

public:
    object_pool(std::function<T*()> factory, std::function<void(T*)> cleanup = [](T*){})
        : factory_(factory), cleanup_(cleanup) {}

    T* acquire() {
        T* obj;
        if (pool_.try_pop(obj)) {
            return obj;
        }
        return factory_();
    }

    void release(T* obj) {
        cleanup_(obj);
        pool_.push(obj);
    }
};

// 对象池使用示例
void object_pool_example() {
    object_pool<std::vector<int>> pool(
        []() { return new std::vector<int>(1000); },
        [](std::vector<int>* v) { v->clear(); }
    );

    // 并行使用对象池
    tbb::parallel_for(0, 100,
        [&](int i) {
            auto vec = pool.acquire();
            // 使用向量...
            vec->push_back(i);
            pool.release(vec);
        }
    );
}
```

#### 13.3.6 性能优化技巧

**负载平衡**：
```cpp
// 动态负载平衡示例
void load_balancing_example() {
    const int N = 1000000;
    std::vector<int> data(N);

    // 使用自适应分区器
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N),
        [&](const tbb::blocked_range<int>& range) {
            for (int i = range.begin(); i < range.end(); ++i) {
                // 模拟不均匀的工作负载
                int work = i % 100;
                for (int j = 0; j < work; j++) {
                    data[i] += j;
                }
            }
        },
        tbb::auto_partitioner()  // 自动分区器
    );
}
```

**缓存友好的数据访问**：
```cpp
// 缓存友好的矩阵访问
void cache_friendly_example() {
    const int N = 1000;
    const int BLOCK_SIZE = 64;
    std::vector<std::vector<double>> matrix(N, std::vector<double>(N));

    // 分块处理以提高缓存性能
    tbb::parallel_for(
        tbb::blocked_range2d<int>(0, N, BLOCK_SIZE, 0, N, BLOCK_SIZE),
        [&](const tbb::blocked_range2d<int>& r) {
            for (size_t i = r.rows().begin(); i != r.rows().end(); ++i) {
                for (size_t j = r.cols().begin(); j != r.cols().end(); ++j) {
                    // 处理矩阵块
                    matrix[i][j] = i + j;
                }
            }
        }
    );
}
```

**减少锁竞争**：
```cpp
#include <tbb/concurrent_vector.h>

// 减少锁竞争的累加器
class concurrent_accumulator {
private:
    tbb::concurrent_vector<double> partial_sums_;
    std::atomic<int> counter_{0};

public:
    void add(double value) {
        int idx = counter_++;
        if (idx >= partial_sums_.size()) {
            partial_sums_.push_back(0.0);
        }
        partial_sums_[idx] += value;
    }

    double get_total() const {
        double total = 0.0;
        for (double sum : partial_sums_) {
            total += sum;
        }
        return total;
    }
};

void lock_free_accumulator_example() {
    concurrent_accumulator acc;

    // 并行累加
    tbb::parallel_for(0, 1000000,
        [&](int i) {
            acc.add(i * 0.1);
        }
    );

    std::cout << "Total: " << acc.get_total() << std::endl;
}
```

#### 13.3.7 TBB最佳实践

**1. 选择合适的并行算法**：
```cpp
// 根据数据大小和操作类型选择算法
template<typename Container>
void parallel_algorithm_selection(Container& data) {
    if (data.size() < 1000) {
        // 小数据集使用串行算法
        std::sort(data.begin(), data.end());
    } else if (data.size() < 100000) {
        // 中等数据集使用TBB parallel_sort
        tbb::parallel_sort(data.begin(), data.end());
    } else {
        // 大数据集使用自定义并行算法
        tbb::parallel_sort(data.begin(), data.end(),
            tbb::auto_partitioner());
    }
}
```

**2. 合理设置任务粒度**：
```cpp
// 根据任务复杂度调整粒度
void granularity_optimization() {
    const int N = 1000000;

    // 简单操作使用较粗粒度
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N, 1000),  // 每个任务1000个元素
        [&](const tbb::blocked_range<int>& range) {
            // 简单操作
            for (int i = range.begin(); i < range.end(); ++i) {
                // 简单计算
            }
        }
    );

    // 复杂操作使用较细粒度
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N, 100),  // 每个任务100个元素
        [&](const tbb::blocked_range<int>& range) {
            // 复杂计算
            for (int i = range.begin(); i < range.end(); ++i) {
                // 复杂计算
            }
        }
    );
}
```

**3. 避免数据竞争**：
```cpp
// 使用TBB容器避免数据竞争
void data_race_avoidance() {
    tbb::concurrent_vector<int> results;

    tbb::parallel_for(0, 1000000,
        [&](int i) {
            int result = expensive_computation(i);
            results.push_back(result);  // 线程安全
        }
    );

    // 或者使用局部变量减少竞争
    tbb::parallel_for(0, 1000000,
        [&](int i) {
            std::vector<int> local_results;
            for (int j = 0; j < 100; j++) {
                local_results.push_back(expensive_computation(i + j));
            }
            // 最后合并结果
        }
    );
}
```

**4. 内存访问模式优化**：
```cpp
// 优化内存访问模式
void memory_access_optimization() {
    const int N = 1000;
    std::vector<std::vector<double>> matrix(N, std::vector<double>(N));

    // 行优先访问（缓存友好）
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N),
        [&](const tbb::blocked_range<int>& range) {
            for (int i = range.begin(); i < range.end(); ++i) {
                for (int j = 0; j < N; ++j) {
                    matrix[i][j] = i + j;  // 行优先访问
                }
            }
        }
    );

    // 列优先访问（缓存不友好，但有时必要）
    tbb::parallel_for(
        tbb::blocked_range<int>(0, N),
        [&](const tbb::blocked_range<int>& range) {
            for (int j = range.begin(); j < range.end(); ++j) {
                for (int i = 0; i < N; ++i) {
                    matrix[i][j] = i + j;  // 列优先访问
                }
            }
        }
    );
}
```

#### 13.3.8 TBB与OpenMP对比

**性能对比示例**：
```cpp
#include <chrono>
#include <omp.h>

// OpenMP版本
double openmp_version(const std::vector<double>& data) {
    double sum = 0.0;
    #pragma omp parallel for reduction(+:sum)
    for (size_t i = 0; i < data.size(); i++) {
        sum += data[i] * data[i];
    }
    return sum;
}

// TBB版本
double tbb_version(const std::vector<double>& data) {
    return tbb::parallel_reduce(
        tbb::blocked_range<size_t>(0, data.size()),
        0.0,
        [&](const tbb::blocked_range<size_t>& r, double local_sum) {
            for (size_t i = r.begin(); i < r.end(); ++i) {
                local_sum += data[i] * data[i];
            }
            return local_sum;
        },
        std::plus<double>()
    );
}

// 性能测试
void performance_comparison() {
    const int N = 10000000;
    std::vector<double> data(N);

    // 初始化数据
    std::random_device rd;
    std::mt19937 gen(rd());
    std::uniform_real_distribution<> dis(0.0, 1.0);
    for (int i = 0; i < N; i++) {
        data[i] = dis(gen);
    }

    // 测试OpenMP
    auto start = std::chrono::high_resolution_clock::now();
    double result1 = openmp_version(data);
    auto end = std::chrono::high_resolution_clock::now();
    auto duration1 = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    // 测试TBB
    start = std::chrono::high_resolution_clock::now();
    double result2 = tbb_version(data);
    end = std::chrono::high_resolution_clock::now();
    auto duration2 = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "OpenMP result: " << result1 << ", time: " << duration1.count() << "ms" << std::endl;
    std::cout << "TBB result: " << result2 << ", time: " << duration2.count() << "ms" << std::endl;
}
```

**选择建议**：
- **TBB优势**：工作窃取调度、更灵活的任务模型、更好的负载平衡
- **OpenMP优势**：简单易用、编译器集成好、广泛支持
- **混合使用**：在某些场景下可以结合使用，发挥各自优势

#### 13.3.9 完整应用示例

**并行图像处理**：
```cpp
#include <tbb/parallel_for.h>
#include <tbb/blocked_range2d.h>

struct Image {
    int width, height;
    std::vector<std::vector<unsigned char>> data;

    Image(int w, int h) : width(w), height(h), data(h, std::vector<unsigned char>(w)) {}
};

// 并行图像模糊处理
void parallel_image_blur(const Image& input, Image& output, int radius = 3) {
    auto blur_pixel = [&](int x, int y) -> unsigned char {
        int sum = 0, count = 0;
        for (int dy = -radius; dy <= radius; dy++) {
            for (int dx = -radius; dx <= radius; dx++) {
                int nx = x + dx;
                int ny = y + dy;
                if (nx >= 0 && nx < input.width && ny >= 0 && ny < input.height) {
                    sum += input.data[ny][nx];
                    count++;
                }
            }
        }
        return static_cast<unsigned char>(sum / count);
    };

    tbb::parallel_for(
        tbb::blocked_range2d<int>(0, output.height, 32, 0, output.width, 32),
        [&](const tbb::blocked_range2d<int>& r) {
            for (size_t y = r.rows().begin(); y != r.rows().end(); ++y) {
                for (size_t x = r.cols().begin(); x != r.cols().end(); ++x) {
                    output.data[y][x] = blur_pixel(x, y);
                }
            }
        }
    );
}

// 并行图像处理主函数
void image_processing_example() {
    Image input(1000, 800);
    Image output(1000, 800);

    // 初始化输入图像（随机数据）
    for (int y = 0; y < input.height; y++) {
        for (int x = 0; x < input.width; x++) {
            input.data[y][x] = rand() % 256;
        }
    }

    // 并行模糊处理
    auto start = std::chrono::high_resolution_clock::now();
    parallel_image_blur(input, output, 5);
    auto end = std::chrono::high_resolution_clock::now();

    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    std::cout << "Image blur completed in " << duration.count() << "ms" << std::endl;
}
```

这个扩展后的TBB章节提供了：
- ✅ 详细的TBB核心概念和任务调度机制
- ✅ 完整的并行算法实现（parallel_for, parallel_reduce, parallel_scan等）
- ✅ TBB并发容器的使用方法和示例
- ✅ 高级并行模式（流图、管道、任务组）
- ✅ 内存管理和优化技巧
- ✅ 性能优化策略和最佳实践
- ✅ 与OpenMP的对比分析
- ✅ 完整的图像处理应用示例
- ✅ 实用的代码示例和性能测试

## 第14章 分布式计算框架

### 14.1 Apache Spark

Apache Spark是一个快速、通用的分布式计算系统，适用于大规模数据处理。它提供了高级API和优化引擎，支持批处理、流处理、机器学习和图计算等多种计算模式。

#### 14.1.1 Spark核心架构

**Spark生态系统**：
```
┌─────────────────────────────────────────────────────────────────┐
│                        Spark Applications                        │
├─────────────────────────────────────────────────────────────────┤
│        Spark SQL        │    MLlib    │     GraphX     │ Spark │
│                         │             │                  │  Streaming │
├─────────────────────────┼─────────────┼──────────────────┼─────────────┤
│                         │             │                  │             │
│      Spark Core         │             │                  │             │
│                         │             │                  │             │
├─────────────────────────┼─────────────┼──────────────────┼─────────────┤
│         Cluster Manager (Standalone, YARN, Mesos, Kubernetes)    │
├─────────────────────────────────────────────────────────────────┤
│                          Storage Layer                           │
│      HDFS      │    S3     │   Cassandra  │    HBase     │ Hive  │
└─────────────────────────────────────────────────────────────────┘
```

**核心组件**：
```python
# Spark架构示例
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf

# 配置Spark
conf = SparkConf() \
    .setAppName("SparkArchitectureExample") \
    .setMaster("local[*]") \
    .set("spark.executor.memory", "2g") \
    .set("spark.driver.memory", "1g") \
    .set("spark.sql.shuffle.partitions", "200")

# 创建SparkSession
spark = SparkSession.builder \
    .config(conf=conf) \
    .getOrCreate()

# 获取SparkContext
sc = spark.sparkContext

# 查看Spark配置
print(f"Master: {sc.master}")
print(f"Application Name: {sc.appName}")
print(f"Default Parallelism: {sc.defaultParallelism}")
print(f"Executor Memory: {sc.getConf().get('spark.executor.memory')}")
```

#### 14.1.2 RDD (Resilient Distributed Dataset)

**RDD基础操作**：
```python
from pyspark import SparkContext, SparkConf
import random

# 创建Spark配置和上下文
conf = SparkConf().setAppName("RDDExample").setMaster("local[*]")
sc = SparkContext(conf=conf)

# 1. 创建RDD
# 从集合创建
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd_from_list = sc.parallelize(data, numSlices=4)

# 从文件创建
rdd_from_file = sc.textFile("data.txt")

# 从其他RDD转换
rdd_from_rdd = rdd_from_list.map(lambda x: x * 2)

# 2. 转换操作 (Transformations)
# map: 一对一转换
squared_rdd = rdd_from_list.map(lambda x: x ** 2)

# filter: 过滤
even_rdd = rdd_from_list.filter(lambda x: x % 2 == 0)

# flatMap: 一对多转换
words_rdd = sc.parallelize(["hello world", "spark is great"])
words_flat = words_rdd.flatMap(lambda line: line.split(" "))

# mapPartitions: 对分区进行转换
def partition_transform(iterator):
    result = []
    for item in iterator:
        result.append(item * 2)
    return result

partition_rdd = rdd_from_list.mapPartitions(partition_transform)

# 3. 行动操作 (Actions)
# collect: 收集所有数据到Driver
result_list = squared_rdd.collect()
print(f"Collected result: {result_list}")

# count: 计数
count = rdd_from_list.count()
print(f"Count: {count}")

# reduce: 聚合
total = rdd_from_list.reduce(lambda a, b: a + b)
print(f"Sum: {total}")

# take: 获取前N个元素
first_three = rdd_from_list.take(3)
print(f"First three: {first_three}")

# saveAsTextFile: 保存到文件
squared_rdd.saveAsTextFile("output/squared")
```

**键值对RDD操作**：
```python
# 键值对RDD示例
pairs_rdd = sc.parallelize([(1, "a"), (2, "b"), (1, "c"), (3, "d"), (2, "e")])

# mapValues: 只对值进行转换
mapped_values = pairs_rdd.mapValues(lambda x: x.upper())

# reduceByKey: 按键聚合
summed_pairs = sc.parallelize([(1, 2), (1, 3), (2, 4), (2, 5)]).reduceByKey(lambda a, b: a + b)

# groupByKey: 按键分组
grouped_pairs = pairs_rdd.groupByKey()

# join: 内连接
rdd1 = sc.parallelize([(1, "a"), (2, "b"), (3, "c")])
rdd2 = sc.parallelize([(1, 10), (2, 20), (4, 40)])
joined = rdd1.join(rdd2)  # 结果: [(1, ('a', 10)), (2, ('b', 20))]

# leftOuterJoin: 左外连接
left_joined = rdd1.leftOuterJoin(rdd2)

# rightOuterJoin: 右外连接
right_joined = rdd1.rightOuterJoin(rdd2)

# fullOuterJoin: 全外连接
full_joined = rdd1.fullOuterJoin(rdd2)

# cogroup: 分组连接
cogrouped = rdd1.cogroup(rdd2)
```

**高级RDD操作**：
```python
# 累加器 (Accumulators)
counter = sc.accumulator(0)
rdd = sc.parallelize([1, 2, 3, 4, 5])

def count_and_print(x):
    global counter
    counter += 1
    print(f"Processing: {x}")

rdd.foreach(count_and_print)
print(f"Total processed: {counter.value}")

# 广播变量 (Broadcast Variables)
broadcast_data = sc.broadcast([1, 2, 3, 4, 5])

def process_with_broadcast(x):
    return x * sum(broadcast_data.value)

result = rdd.map(process_with_broadcast).collect()
print(f"Result with broadcast: {result}")

# 分区操作
# 获取分区数
num_partitions = rdd.getNumPartitions()
print(f"Number of partitions: {num_partitions}")

# 重新分区
repartitioned_rdd = rdd.repartition(8)  # 增加分区
coalesced_rdd = rdd.coalesce(2)  # 减少分区

# 自定义分区器
from pyspark import Partitioner

class CustomPartitioner(Partitioner):
    def __init__(self, num_partitions):
        self.num_partitions = num_partitions

    def numPartitions(self):
        return self.num_partitions

    def getPartition(self, key):
        return hash(key) % self.num_partitions

# 使用自定义分区器
custom_rdd = pairs_rdd.partitionBy(CustomPartitioner(3))
```

#### 14.1.3 DataFrame API

**DataFrame基础操作**：
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.functions import col, when, sum as spark_sum, avg, count, max as spark_max, min as spark_min

# 创建SparkSession
spark = SparkSession.builder \
    .appName("DataFrameExample") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

# 1. 创建DataFrame
# 从列表创建
data = [("Alice", 25, 50000.0), ("Bob", 30, 60000.0), ("Charlie", 35, 70000.0)]
columns = ["name", "age", "salary"]
df = spark.createDataFrame(data, columns)

# 从字典列表创建
data_dict = [
    {"name": "Alice", "age": 25, "salary": 50000.0},
    {"name": "Bob", "age": 30, "salary": 60000.0},
    {"name": "Charlie", "age": 35, "salary": 70000.0}
]
df_dict = spark.createDataFrame(data_dict)

# 从JSON文件创建
df_json = spark.read.json("employees.json")

# 从CSV文件创建
df_csv = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 2. DataFrame查询操作
# 选择列
df.select("name", "age").show()
df.select(col("name"), col("age")).show()

# 过滤数据
df.filter(col("age") > 30).show()
df.where("age > 30").show()

# 条件表达式
df.withColumn("age_group",
    when(col("age") < 30, "Young")
    .when(col("age") < 40, "Middle")
    .otherwise("Senior")
).show()

# 3. 聚合操作
# 基本聚合
df.agg(
    spark_sum("salary").alias("total_salary"),
    avg("salary").alias("avg_salary"),
    count("*").alias("count")
).show()

# 分组聚合
df.groupBy("age") \
    .agg(
        spark_sum("salary").alias("total_salary"),
        avg("salary").alias("avg_salary"),
        count("*").alias("count")
    ).show()

# 多级分组
df.groupBy("age", "name") \
    .agg(spark_sum("salary").alias("total_salary")) \
    .orderBy(col("age"), col("total_salary").desc()) \
    .show()

# 4. 排序和限制
df.orderBy(col("salary").desc()).show()
df.orderBy(["age", "salary"], ascending=[True, False]).show()
df.limit(2).show()

# 5. 窗口函数
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank

window_spec = Window.partitionBy("department").orderBy(col("salary").desc())

df.withColumn("row_number", row_number().over(window_spec)) \
  .withColumn("rank", rank().over(window_spec)) \
  .withColumn("dense_rank", dense_rank().over(window_spec)) \
  .show()
```

**DataFrame连接操作**：
```python
# 创建示例DataFrame
employees = spark.createDataFrame([
    (1, "Alice", 101),
    (2, "Bob", 102),
    (3, "Charlie", 101),
    (4, "David", 103)
], ["id", "name", "dept_id"])

departments = spark.createDataFrame([
    (101, "Engineering"),
    (102, "Sales"),
    (104, "Marketing")
], ["dept_id", "dept_name"])

# 内连接
inner_join = employees.join(departments, on="dept_id", how="inner")

# 左外连接
left_join = employees.join(departments, on="dept_id", how="left")

# 右外连接
right_join = employees.join(departments, on="dept_id", how="right")

# 全外连接
full_join = employees.join(departments, on="dept_id", how="outer")

# 交叉连接
cross_join = employees.crossJoin(departments)

# 多条件连接
employees.join(departments, (employees.dept_id == departments.dept_id) & (departments.dept_name != "Marketing"))

# 自连接
employees.alias("e1").join(
    employees.alias("e2"),
    col("e1.id") != col("e2.id")
).show()
```

#### 14.1.4 Spark SQL

**SQL查询**：
```python
# 创建临时视图
df.createOrReplaceTempView("employees")

# 执行SQL查询
result = spark.sql("""
    SELECT
        name,
        age,
        salary,
        CASE
            WHEN age < 30 THEN 'Young'
            WHEN age < 40 THEN 'Middle'
            ELSE 'Senior'
        END as age_group
    FROM employees
    WHERE salary > 50000
    ORDER BY salary DESC
""")

result.show()

# 复杂SQL查询示例
complex_query = spark.sql("""
    WITH dept_stats AS (
        SELECT
            dept_id,
            AVG(salary) as avg_salary,
            COUNT(*) as emp_count
        FROM employees
        GROUP BY dept_id
    )
    SELECT
        e.name,
        e.salary,
        ds.avg_salary,
        ds.emp_count,
        e.salary - ds.avg_salary as salary_diff
    FROM employees e
    JOIN dept_stats ds ON e.dept_id = ds.dept_id
    WHERE e.salary > ds.avg_salary
    ORDER BY salary_diff DESC
""")

complex_query.show()
```

**UDF (用户定义函数)**：
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, DoubleType

# 定义UDF函数
def categorize_salary(salary):
    if salary < 50000:
        return "Low"
    elif salary < 80000:
        return "Medium"
    else:
        return "High"

# 注册UDF
categorize_udf = udf(categorize_salary, StringType())

# 使用UDF
df_with_category = df.withColumn("salary_category", categorize_udf(col("salary")))
df_with_category.show()

# 带参数的UDF
def calculate_bonus(salary, bonus_rate):
    return salary * bonus_rate

bonus_udf = udf(lambda salary: calculate_bonus(salary, 0.1), DoubleType())
df_with_bonus = df.withColumn("bonus", bonus_udf(col("salary")))
df_with_bonus.show()

# Pandas UDF (向量化UDF)
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf(returnType=DoubleType())
def pandas_salary_increase(salary_series: pd.Series) -> pd.Series:
    return salary_series * 1.1  # 10%增长

df_with_increase = df.withColumn("new_salary", pandas_salary_increase(col("salary")))
df_with_increase.show()
```

#### 14.1.5 Spark Streaming

**基本流处理**：
```python
from pyspark.sql.functions import explode, split
from pyspark.sql.types import StructType, StructField, StringType

# 创建StreamingContext
spark = SparkSession.builder \
    .appName("StreamingExample") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
    .getOrCreate()

# 1. 从Socket读取数据流
lines = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# 2. 处理数据流
words = lines.select(
    explode(split(lines.value, " ")).alias("word")
)

word_counts = words.groupBy("word").count()

# 3. 启动流处理
query = word_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()

# 结构化流处理示例：单词计数
def streaming_word_count():
    # 定义schema
    schema = StructType([
        StructField("timestamp", StringType(), True),
        StructField("text", StringType(), True)
    ])

    # 从文件流读取
    lines = spark \
        .readStream \
        .format("text") \
        .schema(schema) \
        .load("/tmp/streaming_data/")

    # 处理数据
    words = lines.select(
        explode(split(col("text"), " ")).alias("word")
    ).filter(col("word").isNotNull())

    # 窗口聚合
    windowed_counts = words \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            window(col("timestamp"), "10 minutes", "5 minutes"),
            col("word")
        ) \
        .count() \
        .orderBy(col("window").desc())

    # 输出结果
    query = windowed_counts.writeStream \
        .outputMode("append") \
        .format("console") \
        .option("truncate", False) \
        .start()

    query.awaitTermination()
```

**窗口操作**：
```python
from pyspark.sql.functions import window, current_timestamp

# 滚动窗口 (Tumbling Window)
def tumbling_window_example():
    # 每5分钟计算一次
    windowed_data = df \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            window(col("timestamp"), "5 minutes"),
            col("category")
        ) \
        .agg(
            spark_sum("amount").alias("total_amount"),
            count("*").alias("transaction_count")
        )

# 滑动窗口 (Sliding Window)
def sliding_window_example():
    # 每2分钟计算一次过去10分钟的数据
    windowed_data = df \
        .withWatermark("timestamp", "15 minutes") \
        .groupBy(
            window(col("timestamp"), "10 minutes", "2 minutes"),
            col("product")
        ) \
        .agg(
            spark_sum("sales").alias("total_sales"),
            avg("price").alias("avg_price")
        )

# 会话窗口 (Session Window)
def session_window_example():
    # 用户会话分析
    session_data = df \
        .withWatermark("timestamp", "30 minutes") \
        .groupBy(
            col("user_id"),
            session_window(col("timestamp"), "10 minutes")
        ) \
        .agg(
            count("*").alias("session_count"),
            spark_sum("duration").alias("total_duration")
        )
```

#### 14.1.6 MLlib机器学习

**机器学习管道**：
```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler
from pyspark.ml.classification import RandomForestClassifier, LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# 1. 数据预处理
# 创建示例数据
data = spark.createDataFrame([
    (0, "male", 25, 50000, 1.0),
    (1, "female", 30, 60000, 0.0),
    (2, "male", 35, 70000, 1.0),
    (3, "female", 28, 55000, 0.0)
], ["id", "gender", "age", "income", "label"])

# 特征工程
# 字符串索引化
gender_indexer = StringIndexer(inputCol="gender", outputCol="gender_index")

# One-Hot编码
gender_encoder = OneHotEncoder(inputCol="gender_index", outputCol="gender_vec")

# 向量组装
assembler = VectorAssembler(
    inputCols=["gender_vec", "age", "income"],
    outputCol="features"
)

# 特征标准化
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

# 2. 模型训练
# 随机森林分类器
rf = RandomForestClassifier(
    labelCol="label",
    featuresCol="scaled_features",
    numTrees=100,
    maxDepth=5,
    seed=42
)

# 逻辑回归
lr = LogisticRegression(
    labelCol="label",
    featuresCol="scaled_features",
    maxIter=100,
    regParam=0.01
)

# 3. 创建管道
pipeline = Pipeline(stages=[gender_indexer, gender_encoder, assembler, scaler, rf])

# 4. 训练模型
model = pipeline.fit(data)

# 5. 模型评估
predictions = model.transform(data)
evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="accuracy"
)
accuracy = evaluator.evaluate(predictions)
print(f"Accuracy: {accuracy}")

# 6. 模型调优
param_grid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [50, 100, 200]) \
    .addGrid(rf.maxDepth, [3, 5, 7]) \
    .addGrid(rf.maxBins, [16, 32, 64]) \
    .build()

crossval = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=param_grid,
    evaluator=evaluator,
    numFolds=3
)

cv_model = crossval.fit(data)
best_model = cv_model.bestModel
```

**回归和聚类**：
```python
from pyspark.ml.regression import LinearRegression, RandomForestRegressor
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import RegressionEvaluator

# 线性回归
def linear_regression_example():
    # 创建回归数据
    regression_data = spark.createDataFrame([
        (1.0, 2.0, 3.0),
        (2.0, 4.0, 6.0),
        (3.0, 6.0, 9.0),
        (4.0, 8.0, 12.0)
    ], ["feature1", "feature2", "label"])

    assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
    lr = LinearRegression(featuresCol="features", labelCol="label")

    model = lr.fit(assembler.transform(regression_data))
    predictions = model.transform(assembler.transform(regression_data))

    evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    print(f"RMSE: {rmse}")

# K-means聚类
def kmeans_clustering_example():
    # 创建聚类数据
    clustering_data = spark.createDataFrame([
        (0.0, Vectors.dense([0.0, 0.0])),
        (1.0, Vectors.dense([1.0, 1.0])),
        (2.0, Vectors.dense([9.0, 8.0])),
        (3.0, Vectors.dense([8.0, 9.0]))
    ], ["id", "features"])

    kmeans = KMeans(k=2, seed=1)
    model = kmeans.fit(clustering_data)

    centers = model.clusterCenters()
    print(f"Cluster Centers: {centers}")

    predictions = model.transform(clustering_data)
    predictions.select("id", "prediction").show()
```

#### 14.1.7 Spark性能优化

**数据分区优化**：
```python
# 1. 合理设置分区数
# 读取数据时指定分区数
df = spark.read.parquet("large_dataset.parquet").repartition(200)

# 写入时控制分区数
df.coalesce(50).write.parquet("output/")

# 2. 数据倾斜处理
# 盐化技术处理数据倾斜
def handle_skew_with_salting():
    # 为倾斜键添加随机前缀
    df_with_salt = df.withColumn("salted_key", concat(col("key"), lit("_"), (rand() * 100).cast("int")))

    # 先按盐化键聚合
    salted_agg = df_with_salt.groupBy("salted_key").agg(sum("value").alias("partial_sum"))

    # 去掉盐化前缀，再次聚合
    final_agg = salted_agg.withColumn("key", split(col("salted_key"), "_").getItem(0)) \
                         .groupBy("key") \
                         .agg(sum("partial_sum").alias("total_sum"))

# 3. 广播连接优化
from pyspark.sql.functions import broadcast

# 广播小表
small_df = spark.read.parquet("small_table.parquet")
large_df = spark.read.parquet("large_table.parquet")

# 使用broadcast hint
result = large_df.join(broadcast(small_df), "key")

# 4. 缓存策略
# 缓存频繁访问的数据
df.cache()  # 存储在内存中
df.persist(StorageLevel.MEMORY_AND_DISK)  # 内存+磁盘

# 5. 数据格式优化
# 使用列式存储格式
df.write.format("parquet").mode("overwrite").save("optimized_data.parquet")

# 使用Snappy压缩
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")
```

**内存管理优化**：
```python
# 1. 调整Executor内存配置
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.executor.memoryFraction", "0.8")
spark.conf.set("spark.memory.fraction", "0.6")

# 2. 调整并行度
spark.conf.set("spark.sql.shuffle.partitions", "200")  # 默认200
spark.conf.set("spark.default.parallelism", "100")  # 默认CPU核心数

# 3. 优化序列化
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# 4. 调整GC参数
spark.conf.set("spark.executor.extraJavaOptions",
    "-XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication")
```

#### 14.1.8 Spark监控和调试

**性能监控**：
```python
# 1. 启用详细日志
spark.sparkContext.setLogLevel("INFO")

# 2. 查看执行计划
df.explain(mode="extended")

# 3. 监控指标收集
from pyspark.sql.functions import spark_partition_id

# 查看分区分布
df.withColumn("partition_id", spark_partition_id()).groupBy("partition_id").count().show()

# 4. 自定义指标
from pyspark.sql.functions import count, sum as spark_sum

# 记录数据质量指标
quality_metrics = df.select(
    count("*").alias("total_records"),
    count("column1").alias("non_null_records"),
    spark_sum(when(col("column1").isNull(), 1).otherwise(0)).alias("null_records")
)

quality_metrics.show()
```

**调试技巧**：
```python
# 1. 数据采样调试
sample_df = df.sample(fraction=0.1, seed=42)

# 2. 分区调试
def debug_partitions(df):
    return df.rdd.mapPartitionsWithIndex(
        lambda idx, iterator: [(idx, sum(1 for _ in iterator))]
    ).toDF(["partition_id", "record_count"])

# 3. 数据质量检查
def data_quality_check(df):
    total_count = df.count()

    # 检查空值
    null_checks = []
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        null_percentage = (null_count / total_count) * 100
        null_checks.append((col_name, null_count, null_percentage))

    return null_checks
```

#### 14.1.9 Spark与外部系统集成

**与Hadoop集成**：
```python
# 读取HDFS数据
hdfs_df = spark.read.parquet("hdfs://namenode:9000/data/large_dataset.parquet")

# 写入HDFS
df.write.parquet("hdfs://namenode:9000/output/result.parquet")

# 读取HBase数据
hbase_conf = {
    "hbase.zookeeper.quorum": "zk1,zk2,zk3",
    "hbase.mapreduce.inputtable": "my_table"
}

hbase_df = spark.read.format("org.apache.hadoop.hbase.spark") \
    .options(**hbase_conf) \
    .load()
```

**与数据库集成**：
```python
# 读取JDBC数据
jdbc_df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("dbtable", "employees") \
    .option("user", "username") \
    .option("password", "password") \
    .option("numPartitions", 4) \
    .option("lowerBound", 1) \
    .option("upperBound", 1000000) \
    .option("partitionColumn", "id") \
    .load()

# 写入JDBC
df.write.format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("dbtable", "output_table") \
    .option("user", "username") \
    .option("password", "password") \
    .mode("overwrite") \
    .save()
```

#### 14.1.10 Spark应用部署

**本地模式**：
```python
# 本地模式配置
spark = SparkSession.builder \
    .appName("LocalSparkApp") \
    .master("local[*]") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "1g") \
    .getOrCreate()
```

**集群模式**：
```bash
# 提交到YARN集群
spark-submit \
    --master yarn \
    --deploy-mode cluster \
    --executor-memory 4g \
    --executor-cores 2 \
    --num-executors 10 \
    --conf spark.sql.adaptive.enabled=true \
    my_spark_app.py

# 提交到Standalone集群
spark-submit \
    --master spark://master:7077 \
    --deploy-mode client \
    --executor-memory 4g \
    --total-executor-cores 20 \
    my_spark_app.py

# 提交到Kubernetes
spark-submit \
    --master k8s://https://k8s-api-server:6443 \
    --deploy-mode cluster \
    --conf spark.kubernetes.namespace=spark \
    --conf spark.kubernetes.driver.pod.name=my-driver \
    my_spark_app.py
```

#### 14.1.11 实际应用案例

**实时数据分析流水线**：
```python
def real_time_analytics_pipeline():
    # 1. 数据摄取
    raw_data = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "user_events") \
        .load()

    # 2. 数据清洗和转换
    cleaned_data = raw_data \
        .select(from_json(col("value").cast("string"), schema).alias("data")) \
        .select("data.*") \
        .filter(col("user_id").isNotNull()) \
        .withColumn("event_time", current_timestamp())

    # 3. 实时聚合
    windowed_stats = cleaned_data \
        .withWatermark("event_time", "10 minutes") \
        .groupBy(
            window(col("event_time"), "5 minutes", "1 minute"),
            col("event_type")
        ) \
        .agg(
            count("*").alias("event_count"),
            countDistinct("user_id").alias("unique_users"),
            avg("duration").alias("avg_duration")
        )

    # 4. 结果存储
    query = windowed_stats \
        .writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("checkpointLocation", "/tmp/checkpoint/real_time") \
        .option("path", "/tmp/output/real_time_stats") \
        .trigger(processingTime="1 minute") \
        .start()

    query.awaitTermination()

# 批处理ETL流水线
def batch_etl_pipeline():
    # 1. 数据提取
    raw_customers = spark.read.format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/source_db") \
        .option("dbtable", "customers") \
        .load()

    raw_orders = spark.read.format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/source_db") \
        .option("dbtable", "orders") \
        .load()

    # 2. 数据转换
    enriched_data = raw_customers \
        .join(raw_orders, "customer_id", "left") \
        .withColumn("order_date", to_date(col("order_date"))) \
        .filter(col("order_date") >= date_sub(current_date(), 365)) \
        .withColumn("customer_segment",
            when(col("total_orders") > 10, "VIP")
            .when(col("total_orders") > 5, "Regular")
            .otherwise("New"))

    # 3. 数据加载
    enriched_data.write \
        .format("parquet") \
        .mode("overwrite") \
        .partitionBy("customer_segment") \
        .save("/data/warehouse/customer_analytics")

    # 4. 生成报表
    summary_report = enriched_data \
        .groupBy("customer_segment") \
        .agg(
            count("*").alias("customer_count"),
            avg("total_spent").alias("avg_spent"),
            sum("total_orders").alias("total_orders")
        )

    summary_report.write \
        .format("jdbc") \
        .option("url", "jdbc:postgresql://localhost:5432/reporting_db") \
        .option("dbtable", "customer_summary") \
        .mode("overwrite") \
        .save()
```

这个扩展后的Apache Spark章节提供了：
- ✅ 详细的Spark架构和生态系统介绍
- ✅ 完整的RDD操作、DataFrame API、Spark SQL使用方法
- ✅ Spark Streaming实时处理和窗口操作
- ✅ MLlib机器学习管道和模型调优
- ✅ 性能优化策略和最佳实践
- ✅ 监控调试技巧和外部系统集成
- ✅ 部署配置和实际应用案例
- ✅ 实用的代码示例和最佳实践指导

### 14.2 Apache Flink

Apache Flink是一个分布式流处理框架，支持高吞吐量、低延迟的实时数据处理。它提供了统一的批处理和流处理API，支持事件时间处理、状态管理、容错机制等高级特性。

#### 14.2.1 Flink核心架构

**Flink架构组件**：
```
┌─────────────────────────────────────────────────────────────────┐
│                        Flink Applications                        │
├─────────────────────────────────────────────────────────────────┤
│  DataSet API  │  DataStream API  │  Table API  │  SQL API       │
├─────────────────────────────────────────────────────────────────┤
│                         Flink Runtime                           │
├─────────────────────────────────────────────────────────────────┤
│  JobManager  │  TaskManager  │  ResourceManager  │  Dispatcher  │
├─────────────────────────────────────────────────────────────────┤
│                    Deployment Layer                             │
│  Standalone  │  YARN  │  Mesos  │  Kubernetes  │  Docker        │
└─────────────────────────────────────────────────────────────────┘
```

**核心概念**：
```java
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.windowing.time.Time;

public class FlinkArchitectureExample {
    public static void main(String[] args) throws Exception {
        // 1. 创建执行环境
        // 批处理环境
        ExecutionEnvironment batchEnv = ExecutionEnvironment.getExecutionEnvironment();

        // 流处理环境
        StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();

        // 2. 配置环境
        streamEnv.setParallelism(4);  // 设置并行度
        streamEnv.enableCheckpointing(5000);  // 启用检查点，5秒间隔
        streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);  // 设置事件时间

        // 3. 查看配置信息
        System.out.println("Parallelism: " + streamEnv.getParallelism());
        System.out.println("Checkpoint interval: " + streamEnv.getCheckpointInterval());

        // 4. 执行环境
        streamEnv.execute("Flink Architecture Example");
    }
}
```

#### 14.2.2 DataStream API基础

**数据源和转换**：
```java
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.windowing.time.Time;

public class DataStreamBasics {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 1. 数据源 (Sources)
        // 从集合创建
        DataStream<String> collectionSource = env.fromElements("apple", "banana", "cherry", "date");

        // 从文件创建
        DataStream<String> fileSource = env.readTextFile("/path/to/input.txt");

        // 从Socket创建
        DataStream<String> socketSource = env.socketTextStream("localhost", 9999);

        // 从Kafka创建
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "flink-consumer-group");

        DataStream<String> kafkaSource = env.addSource(
            new FlinkKafkaConsumer<>(
                "input-topic",
                new SimpleStringSchema(),
                properties
            )
        );

        // 2. 基本转换操作 (Transformations)
        // Map转换：一对一转换
        DataStream<Integer> mapped = collectionSource.map(new MapFunction<String, Integer>() {
            @Override
            public Integer map(String value) throws Exception {
                return value.length();
            }
        });

        // Lambda表达式简化
        DataStream<Integer> mappedLambda = collectionSource.map(String::length);

        // Filter过滤：过滤偶数长度的字符串
        DataStream<String> filtered = collectionSource.filter(new FilterFunction<String>() {
            @Override
            public boolean filter(String value) throws Exception {
                return value.length() > 5;
            }
        });

        // Filter Lambda表达式
        DataStream<String> filteredLambda = collectionSource.filter(s -> s.length() > 5);

        // FlatMap：一对多转换
        DataStream<String> flatMapped = collectionSource.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public void flatMap(String value, Collector<String> out) throws Exception {
                String[] words = value.split(" ");
                for (String word : words) {
                    out.collect(word);
                }
            }
        });

        // 3. 键值转换
        // 将字符串转换为(key, value)对
        DataStream<Tuple2<String, Integer>> keyValueStream = collectionSource
            .map(new MapFunction<String, Tuple2<String, Integer>>() {
                @Override
                public Tuple2<String, Integer> map(String value) throws Exception {
                    return new Tuple2<>(value, 1);
                }
            });

        // 4. 输出操作 (Sinks)
        // 输出到控制台
        mapped.print();
        filtered.print("Filtered data:");

        // 输出到文件
        mapped.writeAsText("/path/to/output.txt");

        // 输出到Kafka
        mapped.map(String::valueOf).addSink(
            new FlinkKafkaProducer<>(
                "localhost:9092",
                "output-topic",
                new SimpleStringSchema()
            )
        );

        env.execute("DataStream Basics Example");
    }
}
```

**键控流操作**：
```java
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;
import org.apache.flink.streaming.api.windowing.time.Time;

public class KeyedStreamOperations {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 创建键控数据流
        DataStream<Tuple2<String, Integer>> dataStream = env.fromElements(
            new Tuple2<>("apple", 1),
            new Tuple2<>("banana", 2),
            new Tuple2<>("apple", 3),
            new Tuple2<>("cherry", 4),
            new Tuple2<>("banana", 5)
        );

        // 按key分组
        KeyedStream<Tuple2<String, Integer>, String> keyedStream = dataStream.keyBy(0);

        // 1. Reduce聚合
        DataStream<Tuple2<String, Integer>> reduced = keyedStream.reduce(new ReduceFunction<Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> reduce(Tuple2<String, Integer> value1, Tuple2<String, Integer> value2) throws Exception {
                return new Tuple2<>(value1.f0, value1.f1 + value2.f1);
            }
        });

        // 2. Sum聚合
        DataStream<Tuple2<String, Integer>> summed = keyedStream.sum(1);

        // 3. Min/Max聚合
        DataStream<Tuple2<String, Integer>> minValues = keyedStream.min(1);
        DataStream<Tuple2<String, Integer>> maxValues = keyedStream.max(1);

        // 4. 滚动聚合 (Rolling Aggregation)
        DataStream<Tuple2<String, Integer>> rollingSum = keyedStream.sum(1);
        DataStream<Tuple2<String, Integer>> rollingMin = keyedStream.minBy(1);
        DataStream<Tuple2<String, Integer>> rollingMax = keyedStream.maxBy(1);

        reduced.print("Reduced:");
        summed.print("Summed:");
        rollingSum.print("Rolling sum:");

        env.execute("KeyedStream Operations");
    }
}
```

#### 14.2.3 窗口操作

**时间窗口**：
```java
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.evictors.TimeEvictor;
import org.apache.flink.streaming.api.windowing.triggers.EventTimeTrigger;

public class WindowOperations {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 设置事件时间
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 创建示例数据流
        DataStream<Tuple2<String, Integer>> dataStream = env.fromElements(
            new Tuple2<>("sensor1", 10),
            new Tuple2<>("sensor1", 20),
            new Tuple2<>("sensor2", 15),
            new Tuple2<>("sensor1", 30),
            new Tuple2<>("sensor2", 25)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Tuple2<String, Integer>>forMonotonousTimestamps()
                .withTimestampAssigner((event, timestamp) -> System.currentTimeMillis())
        );

        KeyedStream<Tuple2<String, Integer>, String> keyedStream = dataStream.keyBy(0);

        // 1. 滚动窗口 (Tumbling Window)
        // 每5秒计算一次
        DataStream<Tuple2<String, Integer>> tumblingWindow = keyedStream
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .sum(1);

        // 2. 滑动窗口 (Sliding Window)
        // 每2秒计算一次过去10秒的数据
        DataStream<Tuple2<String, Integer>> slidingWindow = keyedStream
            .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(2)))
            .sum(1);

        // 3. 会话窗口 (Session Window)
        // 用户会话分析，30秒不活动后创建新会话
        DataStream<Tuple2<String, Integer>> sessionWindow = keyedStream
            .window(EventTimeSessionWindows.withGap(Time.minutes(5)))
            .sum(1);

        // 4. 自定义窗口
        // 使用自定义触发器和驱逐器
        DataStream<Tuple2<String, Integer>> customWindow = keyedStream
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .trigger(EventTimeTrigger.create())
            .evictor(TimeEvictor.of(Time.minutes(10)))
            .sum(1);

        tumblingWindow.print("Tumbling Window:");
        slidingWindow.print("Sliding Window:");
        sessionWindow.print("Session Window:");
        customWindow.print("Custom Window:");

        env.execute("Window Operations");
    }
}
```

**窗口函数**：
```java
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.streaming.api.functions.windowing.WindowFunction;
import org.apache.flink.util.Collector;

public class WindowFunctions {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        DataStream<Tuple2<String, Integer>> dataStream = env.fromElements(
            new Tuple2<>("sensor1", 10),
            new Tuple2<>("sensor1", 20),
            new Tuple2<>("sensor2", 15)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Tuple2<String, Integer>>forMonotonousTimestamps()
                .withTimestampAssigner((event, timestamp) -> System.currentTimeMillis())
        );

        KeyedStream<Tuple2<String, Integer>, String> keyedStream = dataStream.keyBy(0);

        // 1. WindowFunction
        DataStream<String> windowResult = keyedStream
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .apply(new WindowFunction<Tuple2<String, Integer>, String, String, TimeWindow>() {
                @Override
                public void apply(String key, TimeWindow window, Iterable<Tuple2<String, Integer>> values, Collector<String> out) throws Exception {
                    int sum = 0;
                    int count = 0;
                    for (Tuple2<String, Integer> value : values) {
                        sum += value.f1;
                        count++;
                    }
                    double average = (double) sum / count;
                    out.collect("Sensor: " + key + ", Average: " + average + ", Window: " + window);
                }
            });

        // 2. ProcessWindowFunction (更强大的窗口函数)
        DataStream<String> processWindowResult = keyedStream
            .window(TumblingEventTimeWindows.of(Time.seconds(5)))
            .process(new ProcessWindowFunction<Tuple2<String, Integer>, String, String, TimeWindow>() {
                @Override
                public void process(String key, Context context, Iterable<Tuple2<String, Integer>> elements, Collector<String> out) throws Exception {
                    long windowStart = context.window().getStart();
                    long windowEnd = context.window().getEnd();
                    long watermark = context.currentWatermark();

                    List<Integer> values = new ArrayList<>();
                    for (Tuple2<String, Integer> element : elements) {
                        values.add(element.f1);
                    }

                    out.collect("Key: " + key +
                        ", Window: [" + windowStart + ", " + windowEnd + ")" +
                        ", Watermark: " + watermark +
                        ", Values: " + values);
                }
            });

        windowResult.print("Window Function Result:");
        processWindowResult.print("Process Window Function Result:");

        env.execute("Window Functions");
    }
}
```

#### 14.2.4 事件时间和水位线

**事件时间处理**：
```java
import org.apache.flink.api.common.eventtime.*;
import org.apache.flink.streaming.api.windowing.time.Time;

public class EventTimeProcessing {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1. 基于时间戳的水位线生成
        DataStream<Event> events = env.fromElements(
            new Event("sensor1", 1000L, 25.5),
            new Event("sensor1", 2000L, 26.0),
            new Event("sensor2", 1500L, 24.0),
            new Event("sensor1", 3000L, 26.5)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forMonotonousTimestamps()  // 单调递增时间戳
                .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

        // 2. 基于最大延迟的水位线生成
        DataStream<Event> eventsWithLateness = env.fromElements(
            new Event("sensor1", 1000L, 25.5),
            new Event("sensor1", 2000L, 26.0)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))  // 最大延迟5秒
                .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

        // 3. 自定义水位线生成器
        DataStream<Event> customWatermarks = env.fromElements(
            new Event("sensor1", 1000L, 25.5),
            new Event("sensor1", 2000L, 26.0)
        ).assignTimestampsAndWatermarks(
            new WatermarkStrategy<Event>() {
                @Override
                public WatermarkGenerator<Event> createWatermarkGenerator(WatermarkGeneratorSupplier.Context context) {
                    return new CustomWatermarkGenerator();
                }

                @Override
                public TimestampAssigner<Event> createTimestampAssigner(TimestampAssignerSupplier.Context context) {
                    return (event, timestamp) -> event.getTimestamp();
                }
            }
        );

        // 4. 处理迟到数据
        KeyedStream<Event, String> keyedEvents = events.keyBy(Event::getSensorId);

        DataStream<String> lateDataHandling = keyedEvents
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .allowedLateness(Time.minutes(5))  // 允许迟到5分钟
            .sideOutputLateData(new OutputTag<Event>("late-events") {})  // 侧输出迟到数据
            .apply(new WindowFunction<Event, String, String, TimeWindow>() {
                @Override
                public void apply(String key, TimeWindow window, Iterable<Event> values, Collector<String> out) throws Exception {
                    out.collect("Window result for sensor: " + key);
                }
            });

        // 获取侧输出流
        DataStream<Event> lateEvents = lateDataHandling.getSideOutput(new OutputTag<Event>("late-events") {});
        lateEvents.print("Late events:");

        events.print("Events:");
        customWatermarks.print("Custom watermarks:");
        lateDataHandling.print("Window results:");

        env.execute("Event Time Processing");
    }

    // 自定义水位线生成器
    public static class CustomWatermarkGenerator implements WatermarkGenerator<Event> {
        private final long maxOutOfOrderness = 3500; // 3.5秒
        private long currentMaxTimestamp;

        @Override
        public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
            currentMaxTimestamp = Math.max(eventTimestamp, currentMaxTimestamp);
        }

        @Override
        public void onPeriodicEmit(WatermarkOutput output) {
            output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1));
        }
    }

    // 事件类
    public static class Event {
        private String sensorId;
        private long timestamp;
        private double value;

        public Event(String sensorId, long timestamp, double value) {
            this.sensorId = sensorId;
            this.timestamp = timestamp;
            this.value = value;
        }

        // Getters and setters
        public String getSensorId() { return sensorId; }
        public long getTimestamp() { return timestamp; }
        public double getValue() { return value; }
    }
}
```

#### 14.2.5 状态管理和容错

**状态管理**：
```java
import org.apache.flink.api.common.functions.RichFlatMapFunction;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.util.Collector;

public class StateManagement {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Tuple2<String, Integer>> dataStream = env.fromElements(
            new Tuple2<>("sensor1", 10),
            new Tuple2<>("sensor1", 20),
            new Tuple2<>("sensor1", 5),   // 重复数据
            new Tuple2<>("sensor2", 15),
            new Tuple2<>("sensor2", 15)   // 重复数据
        );

        // 使用状态去重
        DataStream<Tuple2<String, Integer>> deduplicated = dataStream
            .keyBy(0)
            .flatMap(new RichFlatMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
                private ValueState<Integer> lastValueState;

                @Override
                public void open(Configuration parameters) throws Exception {
                    // 定义状态描述符
                    ValueStateDescriptor<Integer> descriptor = new ValueStateDescriptor<>(
                        "lastValue",  // 状态名称
                        TypeInformation.of(Integer.class),  // 状态类型
                        0  // 默认值
                    );
                    lastValueState = getRuntimeContext().getState(descriptor);
                }

                @Override
                public void flatMap(Tuple2<String, Integer> value, Collector<Tuple2<String, Integer>> out) throws Exception {
                    Integer lastValue = lastValueState.value();

                    // 如果当前值与上次不同，则输出
                    if (value.f1 != lastValue) {
                        out.collect(value);
                        lastValueState.update(value.f1);
                    }
                }
            });

        deduplicated.print("Deduplicated data:");

        env.execute("State Management Example");
    }
}
```

**检查点和容错**：
```java
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.environment.CheckpointConfig;

public class FaultTolerance {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 1. 启用检查点
        env.enableCheckpointing(5000);  // 每5秒创建一个检查点

        // 2. 配置检查点参数
        CheckpointConfig config = env.getCheckpointConfig();

        // 设置检查点模式
        config.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);  // 精确一次语义

        // 设置检查点超时时间
        config.setCheckpointTimeout(60000);  // 60秒超时

        // 设置最大并发检查点数量
        config.setMaxConcurrentCheckpoints(1);  // 一次只允许一个检查点

        // 设置检查点之间的最小暂停时间
        config.setMinPauseBetweenCheckpoints(500);  // 最小暂停500毫秒

        // 设置检查点失败时是否让整个作业失败
        config.setFailOnCheckpointingErrors(false);

        // 设置外部化检查点
        config.enableExternalizedCheckpoints(
            CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
        );

        // 3. 配置重启策略
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
            3,  // 重试次数
            Time.of(10, TimeUnit.SECONDS)  // 重试间隔
        ));

        // 4. 配置状态后端
        // 内存状态后端（开发环境）
        env.setStateBackend(new MemoryStateBackend());

        // 文件系统状态后端（生产环境）
        // env.setStateBackend(new FsStateBackend("hdfs://namenode:port/flink/checkpoints"));

        // RocksDB状态后端（大状态应用）
        // env.setStateBackend(new RocksDBStateBackend("hdfs://namenode:port/flink/checkpoints"));

        // 创建数据流
        DataStream<String> stream = env.socketTextStream("localhost", 9999);
        stream.print();

        env.execute("Fault Tolerance Example");
    }
}
```

#### 14.2.6 Flink Table API和SQL

**Table API基础**：
```java
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.api.java.tuple.Tuple3;

public class TableAPIBasics {
    public static void main(String[] args) throws Exception {
        // 1. 创建表环境
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
            .inStreamingMode()  // 流处理模式
            .build();

        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(StreamExecutionEnvironment.getExecutionEnvironment(), settings);

        // 2. 创建数据流
        DataStream<Tuple3<String, Integer, String>> dataStream = tableEnv
            .getStreamExecutionEnvironment()
            .fromElements(
                Tuple3.of("Alice", 25, "Engineering"),
                Tuple3.of("Bob", 30, "Sales"),
                Tuple3.of("Charlie", 35, "Engineering")
            );

        // 3. 将数据流注册为表
        tableEnv.createTemporaryView("employees",
            dataStream,
            Schema.newBuilder()
                .column("name", DataTypes.STRING())
                .column("age", DataTypes.INT())
                .column("department", DataTypes.STRING())
                .build()
        );

        // 4. Table API查询
        Table tableApiQuery = tableEnv.from("employees")
            .filter($("age").isGreaterOrEqual(30))
            .select($("name"), $("department"));

        // 5. SQL查询
        Table sqlQuery = tableEnv.sqlQuery(
            "SELECT name, department FROM employees WHERE age >= 30"
        );

        // 6. 聚合查询
        Table aggregationQuery = tableEnv.from("employees")
            .groupBy($("department"))
            .select($("department"), $("age").avg().as("avg_age"));

        // 7. 将结果转换回数据流
        DataStream<Row> resultStream = tableEnv.toDataStream(tableApiQuery);

        // 8. 输出结果
        tableEnv.toAppendStream(tableApiQuery, Row.class).print("Table API Result:");
        tableEnv.toAppendStream(sqlQuery, Row.class).print("SQL Result:");
        tableEnv.toRetractStream(aggregationQuery, Row.class).print("Aggregation Result:");

        tableEnv.execute("Table API Basics");
    }
}
```

**复杂SQL查询**：
```java
public class ComplexSQLQueries {
    public static void main(String[] args) throws Exception {
        EnvironmentSettings settings = EnvironmentSettings.newInstance()
            .inStreamingMode()
            .build();

        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(
            StreamExecutionEnvironment.getExecutionEnvironment(),
            settings
        );

        // 创建订单表
        String ordersDDL = "CREATE TABLE orders (" +
            "order_id BIGINT," +
            "user_id BIGINT," +
            "product STRING," +
            "price DECIMAL(10, 2)," +
            "order_time TIMESTAMP(3)," +
            "WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND" +
            ") WITH (" +
            "'connector' = 'kafka'," +
            "'topic' = 'orders'," +
            "'properties.bootstrap.servers' = 'localhost:9092'," +
            "'format' = 'json'" +
            ")";

        // 创建用户表
        String usersDDL = "CREATE TABLE users (" +
            "user_id BIGINT," +
            "user_name STRING," +
            "city STRING," +
            "registration_time TIMESTAMP(3)" +
            ") WITH (" +
            "'connector' = 'jdbc'," +
            "'url' = 'jdbc:mysql://localhost:3306/ecommerce'," +
            "'table-name' = 'users'," +
            "'username' = 'root'," +
            "'password' = 'password'," +
            "'format' = 'json'" +
            ")";

        tableEnv.executeSql(ordersDDL);
        tableEnv.executeSql(usersDDL);

        // 1. 基本聚合查询
        String basicAggregation = "SELECT " +
            "product," +
            "COUNT(*) as order_count," +
            "SUM(price) as total_revenue," +
            "AVG(price) as avg_price" +
            "FROM orders " +
            "GROUP BY product";

        // 2. 窗口聚合查询
        String windowAggregation = "SELECT " +
            "TUMBLE_START(order_time, INTERVAL '1' HOUR) as window_start," +
            "TUMBLE_END(order_time, INTERVAL '1' HOUR) as window_end," +
            "COUNT(*) as hourly_orders," +
            "SUM(price) as hourly_revenue" +
            "FROM orders " +
            "GROUP BY TUMBLE(order_time, INTERVAL '1' HOUR)";

        // 3. 连接查询
        String joinQuery = "SELECT " +
            "u.user_name," +
            "u.city," +
            "COUNT(o.order_id) as order_count," +
            "SUM(o.price) as total_spent" +
            "FROM users u " +
            "JOIN orders o ON u.user_id = o.user_id " +
            "GROUP BY u.user_name, u.city";

        // 4. 窗口连接查询
        String windowJoinQuery = "SELECT " +
            "u.user_name," +
            "COUNT(o.order_id) as recent_orders" +
            "FROM users u " +
            "JOIN orders o " +
            "ON u.user_id = o.user_id " +
            "AND o.order_time BETWEEN u.registration_time AND u.registration_time + INTERVAL '1' DAY " +
            "GROUP BY u.user_name";

        // 5. 窗口Top-N查询
        String topNQuery = "SELECT " +
            "product," +
            "order_count," +
            "avg_price" +
            "FROM (" +
            "  SELECT " +
            "    product," +
            "    COUNT(*) as order_count," +
            "    AVG(price) as avg_price," +
            "    ROW_NUMBER() OVER (PARTITION BY TUMBLE_START(order_time, INTERVAL '1' HOUR) ORDER BY COUNT(*) DESC) as row_num" +
            "  FROM orders " +
            "  GROUP BY " +
            "    product," +
            "    TUMBLE(order_time, INTERVAL '1' HOUR)" +
            ") WHERE row_num <= 3";

        tableEnv.sqlQuery(basicAggregation).execute().print();
        tableEnv.sqlQuery(windowAggregation).execute().print();
        tableEnv.sqlQuery(joinQuery).execute().print();
        tableEnv.sqlQuery(windowJoinQuery).execute().print();
        tableEnv.sqlQuery(topNQuery).execute().print();
    }
}
```

#### 14.2.7 Flink CEP (复杂事件处理)

**CEP基础**：
```java
import org.apache.flink.cep.CEP;
import org.apache.flink.cep.PatternStream;
import org.apache.flink.cep.functions.PatternProcessFunction;
import org.apache.flink.cep.pattern.Pattern;
import org.apache.flink.cep.pattern.conditions.SimpleCondition;
import org.apache.flink.util.Collector;

public class CEPExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1. 创建事件流
        DataStream<Event> events = env.fromElements(
            new Event("user1", "login", 1000L),
            new Event("user1", "browse", 2000L),
            new Event("user1", "add_to_cart", 3000L),
            new Event("user1", "purchase", 4000L),
            new Event("user2", "login", 5000L),
            new Event("user2", "browse", 6000L),
            new Event("user2", "logout", 7000L)  // 没有购买
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forMonotonousTimestamps()
                .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

        // 2. 定义模式
        Pattern<Event, ?> purchasePattern = Pattern.<Event>begin("login")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("login");
                }
            })
            .next("browse")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("browse");
                }
            })
            .next("add_to_cart")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("add_to_cart");
                }
            })
            .next("purchase")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("purchase");
                }
            })
            .within(Time.minutes(10));  // 整个模式在10分钟内完成

        // 3. 应用模式到数据流
        PatternStream<Event> patternStream = CEP.pattern(events.keyBy(Event::getUserId), purchasePattern);

        // 4. 处理匹配的模式
        DataStream<String> patternResults = patternStream.process(new PatternProcessFunction<Event, String>() {
            @Override
            public void processMatch(Map<String, List<Event>> match, Context ctx, Collector<String> out) throws Exception {
                Event loginEvent = match.get("login").get(0);
                Event purchaseEvent = match.get("purchase").get(0);

                out.collect("用户 " + loginEvent.getUserId() +
                    " 在 " + (purchaseEvent.getTimestamp() - loginEvent.getTimestamp()) + " 毫秒内完成了购买流程");
            }
        });

        // 5. 检测异常模式
        Pattern<Event, ?> abnormalPattern = Pattern.<Event>begin("login")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("login");
                }
            })
            .next("browse")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("browse");
                }
            })
            .next("logout")
            .where(new SimpleCondition<Event>() {
                @Override
                public boolean filter(Event event) throws Exception {
                    return event.getAction().equals("logout");
                }
            })
            .notNext("purchase")  // 不包含购买事件
            .within(Time.minutes(5));

        PatternStream<Event> abnormalPatternStream = CEP.pattern(events.keyBy(Event::getUserId), abnormalPattern);

        DataStream<String> abnormalResults = abnormalPatternStream.process(new PatternProcessFunction<Event, String>() {
            @Override
            public void processMatch(Map<String, List<Event>> match, Context ctx, Collector<String> out) throws Exception {
                Event loginEvent = match.get("login").get(0);
                Event logoutEvent = match.get("logout").get(0);

                out.collect("异常用户行为: " + loginEvent.getUserId() +
                    " 登录后没有购买就退出，耗时: " + (logoutEvent.getTimestamp() - loginEvent.getTimestamp()) + " 毫秒");
            }
        });

        patternResults.print("正常购买流程:");
        abnormalResults.print("异常行为检测:");

        env.execute("CEP Example");
    }

    // 事件类
    public static class Event {
        private String userId;
        private String action;
        private long timestamp;

        public Event(String userId, String action, long timestamp) {
            this.userId = userId;
            this.action = action;
            this.timestamp = timestamp;
        }

        // Getters
        public String getUserId() { return userId; }
        public String getAction() { return action; }
        public long getTimestamp() { return timestamp; }
    }
}
```

**高级CEP模式**：
```java
// 复杂模式示例：检测价格波动模式
public class AdvancedCEP {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        DataStream<StockPrice> prices = env.fromElements(
            new StockPrice("AAPL", 150.0, 1000L),
            new StockPrice("AAPL", 152.0, 2000L),  // 上涨
            new StockPrice("AAPL", 151.0, 3000L),  // 下跌
            new StockPrice("AAPL", 155.0, 4000L),  // 上涨
            new StockPrice("AAPL", 153.0, 5000L),  // 下跌
            new StockPrice("AAPL", 158.0, 6000L)   // 上涨
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<StockPrice>forMonotonousTimestamps()
                .withTimestampAssigner((price, timestamp) -> price.getTimestamp())
        );

        // 检测W型价格模式（两次上涨-下跌-上涨）
        Pattern<StockPrice, ?> wPattern = Pattern.<StockPrice>begin("start")
            .where(new SimpleCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice price) throws Exception {
                    return true;  // 任意起始价格
                }
            })
            .next("peak1")
            .where(new SimpleCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice current) throws Exception {
                    // 需要找到一个峰值
                    return true;
                }
            })
            .next("trough")
            .where(new SimpleCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice current) throws Exception {
                    // 需要找到一个谷值
                    return true;
                }
            })
            .next("peak2")
            .where(new SimpleCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice current) throws Exception {
                    // 第二个峰值
                    return true;
                }
            })
            .within(Time.minutes(30));

        // 使用迭代条件检查价格模式
        Pattern<StockPrice, ?> iterativePattern = Pattern.<StockPrice>begin("start")
            .where(new IterativeCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice current, Context<StockPrice> ctx) throws Exception {
                    // 检查是否为序列中的第一个元素
                    return ctx.getEventsForPattern("start").isEmpty();
                }
            })
            .oneOrMore()  // 可以匹配一次或多次
            .where(new IterativeCondition<StockPrice>() {
                @Override
                public boolean filter(StockPrice current, Context<StockPrice> ctx) throws Exception {
                    // 检查价格是否持续上涨
                    if (ctx.getEventsForPattern("start").isEmpty()) {
                        return true;
                    }
                    StockPrice last = ctx.getEventsForPattern("start").get(0);
                    return current.getPrice() > last.getPrice();
                }
            })
            .consecutive()  // 连续匹配
            .within(Time.minutes(10));

        env.execute("Advanced CEP Example");
    }

    public static class StockPrice {
        private String symbol;
        private double price;
        private long timestamp;

        public StockPrice(String symbol, double price, long timestamp) {
            this.symbol = symbol;
            this.price = price;
            this.timestamp = timestamp;
        }

        // Getters
        public String getSymbol() { return symbol; }
        public double getPrice() { return price; }
        public long getTimestamp() { return timestamp; }
    }
}
```

#### 14.2.8 Flink与外部系统集成

**Kafka集成**：
```java
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;
import org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.producer.ProducerRecord;

public class KafkaIntegration {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "flink-consumer-group");
        properties.setProperty("auto.offset.reset", "earliest");

        // 1. Kafka消费者
        FlinkKafkaConsumer<String> kafkaConsumer = new FlinkKafkaConsumer<>(
            "input-topic",
            new SimpleStringSchema(),
            properties
        );

        // 设置起始位置
        kafkaConsumer.setStartFromEarliest();  // 从最早开始
        // kafkaConsumer.setStartFromLatest();  // 从最新开始
        // kafkaConsumer.setStartFromTimestamp(123456789L);  // 从指定时间戳开始

        DataStream<String> kafkaStream = env.addSource(kafkaConsumer);

        // 2. Kafka生产者
        FlinkKafkaProducer<String> kafkaProducer = new FlinkKafkaProducer<>(
            "output-topic",
            new SimpleStringSchema(),
            properties
        );

        // 设置生产者配置
        kafkaProducer.setWriteTimestampToKafka(true);  // 写入时间戳到Kafka

        // 3. 自定义序列化
        KafkaSerializationSchema<String> customSerializer = new KafkaSerializationSchema<String>() {
            @Override
            public ProducerRecord<byte[], byte[]> serialize(String element, @Nullable Long timestamp) {
                return new ProducerRecord<>(
                    "output-topic",
                    element.getBytes(StandardCharsets.UTF_8),
                    element.getBytes(StandardCharsets.UTF_8)
                );
            }
        };

        FlinkKafkaProducer<String> customProducer = new FlinkKafkaProducer<>(
            "output-topic",
            customSerializer,
            properties
        );

        // 4. 处理数据流
        kafkaStream
            .map(String::toUpperCase)
            .addSink(kafkaProducer);

        env.execute("Kafka Integration Example");
    }
}
```

**数据库集成**：
```java
import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
import org.apache.flink.connector.jdbc.JdbcSink;
import org.apache.flink.connector.jdbc.JdbcSource;
import org.apache.flink.connector.jdbc.catalog.JdbcCatalog;

public class DatabaseIntegration {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 1. JDBC连接配置
        JdbcConnectionOptions connectionOptions = new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
            .withUrl("jdbc:mysql://localhost:3306/mydb")
            .withDriverName("com.mysql.cj.jdbc.Driver")
            .withUsername("username")
            .withPassword("password")
            .build();

        // 2. JDBC源（读取数据）
        JdbcSource<String> jdbcSource = JdbcSource.<String>builder()
            .setDriverName("com.mysql.cj.jdbc.Driver")
            .setDbUrl("jdbc:mysql://localhost:3306/mydb")
            .setUsername("username")
            .setPassword("password")
            .setQuery("SELECT id, name, value FROM sensor_data WHERE timestamp > ?")
            .setRowConverter(result -> result.getString("name") + ": " + result.getDouble("value"))
            .build();

        DataStream<String> dbStream = env.fromSource(jdbcSource, WatermarkStrategy.noWatermarks(), "jdbc-source");

        // 3. JDBC接收器（写入数据）
        JdbcSink<String> jdbcSink = JdbcSink.sink(
            "INSERT INTO processed_data (id, processed_value, timestamp) VALUES (?, ?, ?)",
            (statement, value) -> {
                statement.setInt(1, value.hashCode());
                statement.setString(2, value);
                statement.setLong(3, System.currentTimeMillis());
            },
            connectionOptions
        );

        // 4. 使用JDBC目录
        JdbcCatalog catalog = new JdbcCatalog(
            "my_catalog",
            "username",
            "password",
            "jdbc:mysql://localhost:3306/mydb",
            "com.mysql.cj.jdbc.Driver"
        );

        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        tableEnv.registerCatalog("my_catalog", catalog);
        tableEnv.useCatalog("my_catalog");

        // 5. 从数据库表创建表
        String createTableSQL = "CREATE TABLE sensor_data (" +
            "id INT," +
            "name STRING," +
            "value DOUBLE," +
            "timestamp TIMESTAMP(3)," +
            "WATERMARK FOR timestamp AS timestamp - INTERVAL '5' SECOND" +
            ") WITH (" +
            "'connector' = 'jdbc'," +
            "'url' = 'jdbc:mysql://localhost:3306/mydb'," +
            "'table-name' = 'sensor_data'," +
            "'username' = 'username'," +
            "'password' = 'password'" +
            ")";

        tableEnv.executeSql(createTableSQL);

        // 6. 查询和处理
        Table result = tableEnv.sqlQuery(
            "SELECT name, AVG(value) as avg_value, COUNT(*) as count " +
            "FROM sensor_data " +
            "WHERE timestamp > CURRENT_TIMESTAMP - INTERVAL '1' HOUR " +
            "GROUP BY name"
        );

        result.execute().print();

        env.execute("Database Integration Example");
    }
}
```

#### 14.2.9 Flink性能优化

**性能调优**：
```java
public class FlinkPerformanceOptimization {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 1. 设置合适的并行度
        env.setParallelism(8);  // 根据集群资源设置

        // 2. 配置内存管理
        Configuration config = new Configuration();
        config.setString(TaskManagerOptions.MEMORY_PROCESS_SIZE, "2g");
        config.setString(TaskManagerOptions.MEMORY_NETWORK_MEMORY_FRACTION, "0.1");
        config.setString(TaskManagerOptions.MEMORY_MANAGED_MEMORY_FRACTION, "0.4");

        env.getConfig().setGlobalJobParameters(config);

        // 3. 优化序列化
        env.getConfig().enableForceAvro();  // 使用Avro序列化
        env.getConfig().enableObjectReuse();  // 重用对象减少GC

        // 4. 调整检查点配置
        env.enableCheckpointing(10000);  // 10秒检查点间隔
        env.getCheckpointConfig().setCheckpointTimeout(60000);
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(5000);

        // 5. 优化网络缓冲区
        env.setBufferTimeout(100);  // 减少缓冲区等待时间

        // 6. 使用广播状态优化
        DataStream<String> broadcastStream = env.fromElements("config1", "config2");
        MapStateDescriptor<String, String> broadcastStateDescriptor =
            new MapStateDescriptor<>("broadcast-state", String.class, String.class);

        DataStream<String> dataStream = env.fromElements("data1", "data2", "data3");

        DataStream<String> result = dataStream
            .connect(broadcastStream.broadcast(broadcastStateDescriptor))
            .process(new BroadcastProcessFunction<String, String, String>() {
                @Override
                public void processElement(String value, ReadOnlyContext ctx, Collector<String> out) throws Exception {
                    ReadOnlyBroadcastState<String, String> state = ctx.getBroadcastState(broadcastStateDescriptor);
                    // 使用广播状态
                    out.collect(value);
                }

                @Override
                public void processBroadcastElement(String value, Context ctx, Collector<String> out) throws Exception {
                    BroadcastState<String, String> state = ctx.getBroadcastState(broadcastStateDescriptor);
                    state.put("key", value);
                }
            });

        // 7. 优化窗口操作
        DataStream<Event> events = env.fromElements(
            new Event("sensor1", 1000L, 25.5),
            new Event("sensor1", 2000L, 26.0)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

        // 使用增量聚合减少状态大小
        KeyedStream<Event, String> keyedEvents = events.keyBy(Event::getSensorId);

        DataStream<Double> incrementalAgg = keyedEvents
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            .reduce(new ReduceFunction<Event>() {
                @Override
                public Event reduce(Event v1, Event v2) throws Exception {
                    return new Event(v1.sensorId, v1.timestamp, v1.value + v2.value);
                }
            })
            .map(event -> event.value);

        // 8. 使用分区优化
        DataStream<Event> partitionedEvents = events
            .partitionCustom(new Partitioner<String>() {
                @Override
                public int partition(String key, int numPartitions) {
                    return Math.abs(key.hashCode()) % numPartitions;
                }
            }, Event::getSensorId);

        result.print("Broadcast result:");
        incrementalAgg.print("Incremental aggregation:");
        partitionedEvents.print("Partitioned events:");

        env.execute("Performance Optimization Example");
    }
}
```

#### 14.2.10 Flink部署和监控

**部署配置**：
```yaml
# flink-conf.yaml
# 作业管理器配置
jobmanager.rpc.address: localhost
jobmanager.rpc.port: 6123
jobmanager.heap.size: 1024m

# 任务管理器配置
taskmanager.numberOfTaskSlots: 4
taskmanager.memory.process.size: 2048m
taskmanager.memory.flink.size: 1536m

# 检查点配置
execution.checkpointing.interval: 10000
execution.checkpointing.mode: EXACTLY_ONCE
execution.checkpointing.timeout: 600000
execution.checkpointing.min-pause: 500
execution.checkpointing.max-concurrent-checkpoints: 1
execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION

# 状态后端配置
state.backend: rocksdb
state.checkpoints.dir: hdfs://namenode:port/flink/checkpoints
state.savepoints.dir: hdfs://namenode:port/flink/savepoints

# 网络配置
taskmanager.network.memory.fraction: 0.15
taskmanager.network.memory.min: 64mb
taskmanager.network.memory.max: 1gb

# 并行度配置
parallelism.default: 4

# 重启策略
restart-strategy: fixed-delay
restart-strategy.fixed-delay.attempts: 3
restart-strategy.fixed-delay.delay: 10 s
```

**监控和指标**：
```java
import org.apache.flink.metrics.Counter;
import org.apache.flink.metrics.Gauge;
import org.apache.flink.metrics.Histogram;
import org.apache.flink.metrics.Meter;
import org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogram;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.util.Collector;

public class FlinkMonitoring {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Event> events = env.fromElements(
            new Event("sensor1", 1000L, 25.5),
            new Event("sensor1", 2000L, 26.0)
        ).assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

        // 自定义监控
        DataStream<String> monitoredEvents = events
            .keyBy(Event::getSensorId)
            .process(new KeyedProcessFunction<String, Event, String>() {
                private Counter eventCounter;
                private Histogram processingTimeHistogram;
                private Meter throughputMeter;
                private Gauge<Double> currentTemperatureGauge;

                @Override
                public void open(Configuration parameters) throws Exception {
                    // 注册计数器
                    eventCounter = getRuntimeContext()
                        .getMetricGroup()
                        .counter("events_processed");

                    // 注册直方图
                    processingTimeHistogram = getRuntimeContext()
                        .getMetricGroup()
                        .histogram("processing_time", new DescriptiveStatisticsHistogram(1000));

                    // 注册计量器
                    throughputMeter = getRuntimeContext()
                        .getMetricGroup()
                        .meter("throughput", new MeterView(60));

                    // 注册仪表
                    currentTemperatureGauge = getRuntimeContext()
                        .getMetricGroup()
                        .gauge("current_temperature", () -> currentTemperature);
                }

                private double currentTemperature = 0.0;

                @Override
                public void processElement(Event value, Context ctx, Collector<String> out) throws Exception {
                    long startTime = System.currentTimeMillis();

                    // 处理逻辑
                    currentTemperature = value.getValue();
                    String result = "Sensor: " + value.getSensorId() + ", Temperature: " + value.getValue();

                    long processingTime = System.currentTimeMillis() - startTime;

                    // 更新指标
                    eventCounter.inc();
                    processingTimeHistogram.update(processingTime);
                    throughputMeter.markEvent();
                    // currentTemperatureGauge自动更新

                    out.collect(result);
                }
            });

        monitoredEvents.print("Monitored events:");

        env.execute("Flink Monitoring Example");
    }
}
```

#### 14.2.11 实际应用案例

**实时欺诈检测系统**：
```java
public class RealTimeFraudDetection {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        // 1. 交易数据流
        DataStream<Transaction> transactions = env
            .addSource(new FlinkKafkaConsumer<>(
                "transactions",
                new TransactionDeserializer(),
                createKafkaProps()
            ))
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.<Transaction>forBoundedOutOfOrderness(Duration.ofMinutes(5))
                    .withTimestampAssigner((tx, timestamp) -> tx.getTimestamp())
            );

        // 2. 用户行为模式检测
        Pattern<Transaction, ?> fraudPattern = Pattern.<Transaction>begin("small_tx")
            .where(new SimpleCondition<Transaction>() {
                @Override
                public boolean filter(Transaction tx) throws Exception {
                    return tx.getAmount() < 100;  // 小额交易
                }
            })
            .next("large_tx")
            .where(new SimpleCondition<Transaction>() {
                @Override
                public boolean filter(Transaction tx) throws Exception {
                    return tx.getAmount() > 1000;  // 大额交易
                }
            })
            .within(Time.minutes(10));  // 10分钟内

        PatternStream<Transaction> patternStream = CEP.pattern(
            transactions.keyBy(Transaction::getUserId),
            fraudPattern
        );

        // 3. 检测可疑交易
        DataStream<FraudAlert> fraudAlerts = patternStream.process(new PatternProcessFunction<Transaction, FraudAlert>() {
            @Override
            public void processMatch(Map<String, List<Transaction>> match, Context ctx, Collector<FraudAlert> out) throws Exception {
                Transaction smallTx = match.get("small_tx").get(0);
                Transaction largeTx = match.get("large_tx").get(0);

                FraudAlert alert = new FraudAlert(
                    smallTx.getUserId(),
                    "Suspicious transaction pattern detected",
                    smallTx.getAmount(),
                    largeTx.getAmount(),
                    smallTx.getTimestamp()
                );

                out.collect(alert);
            }
        });

        // 4. 地理位置异常检测
        DataStream<FraudAlert> geoAlerts = transactions
            .keyBy(Transaction::getUserId)
            .process(new KeyedProcessFunction<String, Transaction, FraudAlert>() {
                private ValueState<String> lastLocationState;

                @Override
                public void open(Configuration parameters) throws Exception {
                    lastLocationState = getRuntimeContext().getState(
                        new ValueStateDescriptor<>("last_location", String.class)
                    );
                }

                @Override
                public void processElement(Transaction tx, Context ctx, Collector<FraudAlert> out) throws Exception {
                    String lastLocation = lastLocationState.value();

                    if (lastLocation != null && !lastLocation.equals(tx.getLocation())) {
                        // 计算时间差
                        long timeDiff = tx.getTimestamp() - ctx.timerService().currentProcessingTime();
                        // 假设地理位置变化太快
                        if (timeDiff < 3600000) {  // 1小时内
                            FraudAlert alert = new FraudAlert(
                                tx.getUserId(),
                                "Geographic anomaly detected",
                                tx.getAmount(),
                                0.0,
                                tx.getTimestamp()
                            );
                            out.collect(alert);
                        }
                    }

                    lastLocationState.update(tx.getLocation());
                }
            });

        // 5. 输出警报
        fraudAlerts.addSink(new FlinkKafkaProducer<>(
            "fraud-alerts",
            new FraudAlertSerializer(),
            createKafkaProps()
        ));

        geoAlerts.addSink(new FlinkKafkaProducer<>(
            "geo-alerts",
            new FraudAlertSerializer(),
            createKafkaProps()
        ));

        env.execute("Real-Time Fraud Detection");
    }

    // 数据类
    public static class Transaction {
        private String userId;
        private double amount;
        private String location;
        private long timestamp;

        // Constructors, getters, setters
    }

    public static class FraudAlert {
        private String userId;
        private String description;
        private double amount;
        private double threshold;
        private long timestamp;

        public FraudAlert(String userId, String description, double amount, double threshold, long timestamp) {
            this.userId = userId;
            this.description = description;
            this.amount = amount;
            this.threshold = threshold;
            this.timestamp = timestamp;
        }
        // Getters and setters
    }
}
```

这个扩展后的Apache Flink章节提供了：
- ✅ 详细的Flink架构和核心概念介绍
- ✅ 完整的DataStream API使用方法和示例
- ✅ 窗口操作、事件时间处理、状态管理
- ✅ Table API和SQL查询
- ✅ CEP (复杂事件处理)高级特性
- ✅ 与外部系统的集成方法
- ✅ 性能优化策略和最佳实践
- ✅ 部署配置和监控技巧
- ✅ 实际应用案例（实时欺诈检测）
- ✅ 实用的代码示例和最佳实践指导

### 14.3 Dask

Dask是一个灵活的并行计算库，为Python生态系统提供并行和分布式计算能力。它通过扩展Pandas、NumPy和Scikit-learn等库，让开发者能够轻松处理大规模数据集，而无需大幅修改现有代码。

#### 14.3.1 Dask核心架构

**Dask架构组件**：
```
┌─────────────────────────────────────────────────────────────────┐
│                      Dask Collections                          │
├─────────────────────────────────────────────────────────────────┤
│  Dask Array  │  Dask DataFrame  │  Dask Bag  │  Dask Delayed   │
├─────────────────────────────────────────────────────────────────┤
│                      Dask Schedulers                           │
│  Single Machine  │  Distributed  │  LocalCluster  │  Kubernetes │
├─────────────────────────────────────────────────────────────────┤
│                    Task Graph Engine                           │
├─────────────────────────────────────────────────────────────────┤
│                    Python Objects                              │
└─────────────────────────────────────────────────────────────────┘
```

**核心概念**：
```python
import dask
import dask.array as da
import dask.dataframe as dd
from dask.distributed import Client, LocalCluster
import numpy as np
import pandas as pd

# 1. 创建Dask客户端
def setup_dask_cluster():
    # 本地集群
    cluster = LocalCluster(
        n_workers=4,
        threads_per_worker=2,
        memory_limit='2GB'
    )
    client = Client(cluster)

    # 或者连接到远程集群
    # client = Client('scheduler-address:8786')

    print(f"Dask Dashboard: {client.dashboard_link}")
    return client

# 2. 配置Dask
def configure_dask():
    # 设置默认分区大小
    dask.config.set({
        'array.chunk-size': '128MB',
        'dataframe.shuffle.method': 'tasks',
        'distributed.worker.memory.target': 0.8,
        'distributed.worker.memory.spill': 0.9,
        'distributed.worker.memory.pause': 0.95
    })

# 3. 查看Dask配置
def check_dask_config():
    print("Dask Configuration:")
    for key, value in dask.config.config.items():
        if 'distributed' in key:
            print(f"  {key}: {value}")

# 4. 基本概念演示
def dask_concepts_demo():
    # 创建Dask数组
    x = da.random.random((10000, 10000), chunks=(1000, 1000))
    print(f"Dask Array shape: {x.shape}")
    print(f"Chunks: {x.chunks}")
    print(f"Number of chunks: {x.npartitions}")

    # 创建Dask DataFrame
    df = dd.from_pandas(pd.DataFrame({
        'x': np.random.randn(1000),
        'y': np.random.randn(1000),
        'z': np.random.randint(0, 10, 1000)
    }), npartitions=4)
    print(f"Dask DataFrame partitions: {df.npartitions}")

    return x, df
```

#### 14.3.2 Dask Array

**并行数组操作**：
```python
import dask.array as da
import numpy as np

def dask_array_operations():
    # 1. 创建Dask数组
    # 从NumPy数组创建
    np_array = np.random.random((5000, 5000))
    dask_array = da.from_array(np_array, chunks=(1000, 1000))

    # 从随机数创建
    random_array = da.random.random((10000, 10000), chunks=(2000, 2000))

    # 从文件创建
    # array_from_file = da.from_npy_stack('path/to/npy/files')

    # 2. 数组操作
    # 基本数学运算
    x = da.random.random((10000, 10000), chunks=(2000, 2000))
    y = da.random.random((10000, 10000), chunks=(2000, 2000))

    # 元素级运算
    z = x + y
    w = x * y
    result = da.sin(x) + da.cos(y)

    # 线性代数运算
    matrix_product = x @ y  # 矩阵乘法
    dot_product = da.dot(x, y)  # 点积

    # 统计运算
    mean_val = x.mean()
    std_val = x.std()
    sum_val = x.sum()

    # 聚合运算
    max_val = x.max(axis=0)
    min_val = x.min(axis=1)
    argmax_val = x.argmax(axis=0)

    # 3. 数组重排
    # 转置
    transposed = x.T

    # 重塑
    reshaped = x.reshape(-1, 1000)

    # 堆叠
    stacked = da.stack([x, y], axis=0)

    # 连接
    concatenated = da.concatenate([x, y], axis=0)

    # 4. 索引和切片
    # 基本切片
    sliced = x[100:500, 200:600]

    # 高级索引
    indices = da.arange(100)
    indexed = x[indices, indices]

    # 条件索引
    mask = x > 0.5
    filtered = x[mask]

    # 5. 线性代数高级操作
    # Cholesky分解
    A = da.random.random((2000, 2000), chunks=(500, 500))
    A = A @ A.T  # 确保正定
    L = da.linalg.cholesky(A)

    # 特征值分解
    eigenvalues, eigenvectors = da.linalg.eigh(A)

    # SVD分解
    U, s, Vt = da.linalg.svd(A)

    # 6. 傅里叶变换
    fft_result = da.fft.fft2(x)
    ifft_result = da.fft.ifft2(fft_result)

    return {
        'array': x,
        'operations': [mean_val, std_val, sum_val],
        'linear_algebra': [L, eigenvalues, U],
        'fft': [fft_result, ifft_result]
    }

# 性能比较示例
def performance_comparison():
    import time

    # NumPy操作
    np_x = np.random.random((5000, 5000))
    np_y = np.random.random((5000, 5000))

    start_time = time.time()
    np_result = np.dot(np_x, np_y)
    np_time = time.time() - start_time
    print(f"NumPy time: {np_time:.2f} seconds")

    # Dask操作
    da_x = da.from_array(np_x, chunks=(1000, 1000))
    da_y = da.from_array(np_y, chunks=(1000, 1000))

    start_time = time.time()
    da_result = da.dot(da_x, da_y).compute()
    da_time = time.time() - start_time
    print(f"Dask time: {da_time:.2f} seconds")

    # 验证结果一致性
    print(f"Results match: {np.allclose(np_result, da_result)}")

def lazy_evaluation_demo():
    """演示Dask的懒执行特性"""
    x = da.random.random((10000, 10000), chunks=(2000, 2000))

    # 创建计算图，但不执行
    y = x + 1
    z = y * 2
    result = z.mean()

    print("计算图创建完成，但尚未执行")
    print(f"Result type: {type(result)}")

    # 触发计算
    actual_result = result.compute()
    print(f"实际计算结果: {actual_result}")

    # 查看计算图
    print("计算图:")
    result.visualize(filename='dask_graph.png', format='png')
```

#### 14.3.3 Dask DataFrame

**并行数据处理**：
```python
import dask.dataframe as dd
import pandas as pd
import numpy as np

def dask_dataframe_operations():
    # 1. 创建Dask DataFrame
    # 从Pandas DataFrame创建
    pdf = pd.DataFrame({
        'id': range(100000),
        'name': [f'user_{i}' for i in range(100000)],
        'age': np.random.randint(18, 80, 100000),
        'salary': np.random.normal(50000, 20000, 100000),
        'department': np.random.choice(['IT', 'Sales', 'Marketing', 'HR'], 100000)
    })
    ddf = dd.from_pandas(pdf, npartitions=8)

    # 从CSV文件创建
    # ddf = dd.read_csv('large_dataset.csv', blocksize="25MB")

    # 从Parquet文件创建
    # ddf = dd.read_parquet('data.parquet')

    # 2. 基本操作
    # 查看数据
    print(f"Shape: {ddf.shape}")
    print(f"Columns: {ddf.columns}")
    print(f"Partitions: {ddf.npartitions}")

    # 数据类型
    print(f"Data types:\n{ddf.dtypes}")

    # 描述性统计
    print(f"Describe:\n{ddf.describe().compute()}")

    # 3. 数据选择和过滤
    # 选择列
    selected_cols = ddf[['name', 'age', 'salary']]

    # 过滤行
    filtered = ddf[ddf.age > 30]
    filtered_dept = ddf[ddf.department == 'IT']

    # 复杂条件过滤
    complex_filter = ddf[(ddf.age > 25) & (ddf.salary > 40000)]

    # 4. 数据转换
    # 添加新列
    ddf['age_group'] = ddf.age.apply(
        lambda x: 'Young' if x < 30 else 'Middle' if x < 50 else 'Senior',
        meta=('age_group', 'object')
    )

    # 数值转换
    ddf['salary_k'] = ddf.salary / 1000

    # 字符串操作
    ddf['name_upper'] = ddf.name.str.upper()

    # 5. 分组聚合操作
    # 基本聚合
    dept_stats = ddf.groupby('department').agg({
        'salary': ['mean', 'std', 'min', 'max'],
        'age': 'mean'
    })

    # 自定义聚合函数
    def custom_agg(series):
        return series.quantile(0.75) - series.quantile(0.25)

    custom_result = ddf.groupby('department')['salary'].apply(custom_agg, meta=('salary_iqr', 'float64'))

    # 多级分组
    multi_group = ddf.groupby(['department', 'age_group']).agg({
        'salary': 'mean',
        'id': 'count'
    })

    # 6. 排序和索引
    # 按值排序
    sorted_df = ddf.sort_values('salary', ascending=False)

    # 设置索引
    indexed_df = ddf.set_index('id')

    # 按索引排序
    sorted_index = indexed_df.sort_index()

    # 7. 合并与连接
    # 创建另一个DataFrame用于合并
    dept_info = dd.from_pandas(pd.DataFrame({
        'department': ['IT', 'Sales', 'Marketing', 'HR'],
        'budget': [1000000, 500000, 300000, 200000],
        'manager': ['Alice', 'Bob', 'Charlie', 'David']
    }), npartitions=2)

    # 内连接
    joined = ddf.merge(dept_info, on='department', how='inner')

    # 左连接
    left_joined = ddf.merge(dept_info, on='department', how='left')

    # 8. 时间序列操作
    # 创建时间序列数据
    dates = pd.date_range('2023-01-01', periods=100000, freq='H')
    ts_data = dd.from_pandas(pd.DataFrame({
        'timestamp': dates,
        'value': np.random.randn(100000),
        'category': np.random.choice(['A', 'B', 'C'], 100000)
    }), npartitions=8)

    # 设置时间索引
    ts_data = ts_data.set_index('timestamp')

    # 时间序列聚合
    daily_avg = ts_data.resample('D').mean()
    monthly_sum = ts_data.resample('M').sum()

    # 滚动窗口
    rolling_mean = ts_data['value'].rolling(window=24).mean()

    return {
        'basic_ops': ddf.head(),
        'grouped': dept_stats.compute(),
        'joined': joined.head(),
        'timeseries': daily_avg.compute()
    }

def data_cleaning_example():
    """数据清洗示例"""
    # 创建包含缺失值的数据
    data = {
        'id': range(10000),
        'name': [f'user_{i}' for i in range(10000)],
        'age': np.random.choice([20, 25, 30, np.nan, 35], 10000),
        'salary': np.random.normal(50000, 20000, 10000),
        'department': np.random.choice(['IT', 'Sales', 'Marketing', None, 'HR'], 10000)
    }

    ddf = dd.from_pandas(pd.DataFrame(data), npartitions=4)

    # 1. 处理缺失值
    # 检查缺失值
    missing_stats = ddf.isnull().sum().compute()
    print(f"Missing values:\n{missing_stats}")

    # 填充缺失值
    ddf_filled = ddf.fillna({
        'age': ddf['age'].mean().compute(),
        'department': 'Unknown'
    })

    # 删除缺失值
    ddf_clean = ddf.dropna()

    # 2. 处理异常值
    # 使用IQR方法检测异常值
    Q1 = ddf['salary'].quantile(0.25).compute()
    Q3 = ddf['salary'].quantile(0.75).compute()
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # 过滤异常值
    ddf_filtered = ddf[(ddf['salary'] >= lower_bound) & (ddf['salary'] <= upper_bound)]

    # 3. 数据类型转换
    ddf['id'] = ddf['id'].astype('int32')
    ddf['salary'] = ddf['salary'].round(2)

    # 4. 去重
    ddf_deduped = ddf.drop_duplicates()

    return ddf_filled, ddf_filtered, ddf_deduped
```

#### 14.3.4 Dask Bag

**并行集合处理**：
```python
import dask.bag as db
import json
import os

def dask_bag_operations():
    # 1. 创建Dask Bag
    # 从Python列表创建
    data = ['apple', 'banana', 'cherry', 'date', 'elderberry']
    bag = db.from_sequence(data, npartitions=2)

    # 从文件创建
    # bag = db.read_text('large_text_file.txt')

    # 从JSON文件创建
    # bag = db.read_text('data.json').map(json.loads)

    # 2. 基本转换操作
    # Map操作
    lengths = bag.map(len)
    uppercased = bag.map(str.upper)

    # Filter操作
    long_words = bag.filter(lambda x: len(x) > 5)
    starts_with_a = bag.filter(lambda x: x.startswith('a'))

    # FlatMap操作
    words = db.from_sequence(['hello world', 'foo bar', 'test data'])
    all_words = words.flatMap(lambda line: line.split())

    # 3. 聚合操作
    # Count操作
    total_count = bag.count()
    word_counts = bag.frequencies()

    # Reduce操作
    total_length = lengths.reduction(sum, sum)
    max_length = lengths.reduction(max, max)

    # GroupBy操作
    grouped = bag.groupby(lambda x: x[0])  # 按首字母分组
    grouped_counts = grouped.count()

    # 4. 文本处理示例
    def process_text(text):
        """处理文本数据"""
        words = text.lower().split()
        return [(word, 1) for word in words if len(word) > 2]

    # 创建示例文本数据
    texts = [
        "Hello world this is a test",
        "Dask makes parallel computing easy",
        "Python is great for data analysis",
        "Machine learning is fascinating"
    ]

    text_bag = db.from_sequence(texts)
    word_pairs = text_bag.flatMap(process_text)
    word_frequencies = word_pairs.groupby(lambda x: x[0]).map(lambda x: (x[0], sum(v for _, v in x[1])))

    # 5. JSON数据处理
    def process_json_record(record):
        """处理JSON记录"""
        try:
            data = json.loads(record)
            return {
                'user_id': data.get('user_id'),
                'action': data.get('action'),
                'timestamp': data.get('timestamp')
            }
        except:
            return None

    # 模拟JSON日志数据
    log_data = [
        '{"user_id": 1, "action": "login", "timestamp": "2023-01-01T10:00:00"}',
        '{"user_id": 2, "action": "logout", "timestamp": "2023-01-01T10:05:00"}',
        '{"user_id": 1, "action": "purchase", "timestamp": "2023-01-01T10:10:00"}',
        '{"user_id": 3, "action": "login", "timestamp": "2023-01-01T10:15:00"}'
    ]

    log_bag = db.from_sequence(log_data)
    processed_logs = log_bag.map(process_json_record).filter(lambda x: x is not None)

    # 6. 文件处理示例
    def process_file(filename):
        """处理单个文件"""
        try:
            with open(filename, 'r') as f:
                content = f.read()
                lines = content.split('\n')
                return {
                    'filename': filename,
                    'line_count': len(lines),
                    'word_count': sum(len(line.split()) for line in lines),
                    'char_count': len(content)
                }
        except Exception as e:
            return {'filename': filename, 'error': str(e)}

    # 处理多个文件
    # file_list = ['/path/to/file1.txt', '/path/to/file2.txt', '/path/to/file3.txt']
    # file_bag = db.from_sequence(file_list)
    # file_stats = file_bag.map(process_file)

    return {
        'basic_ops': word_frequencies.compute(),
        'text_processing': word_frequencies.compute(),
        'json_processing': processed_logs.compute()
    }

def word_count_example():
    """Word Count示例"""
    # 创建示例文本数据
    documents = [
        "The quick brown fox jumps over the lazy dog",
        "Hello world this is a test document",
        "Dask is great for parallel computing",
        "Python makes data analysis easy and fun"
    ]

    # 创建Dask Bag
    text_bag = db.from_sequence(documents)

    # 实现Word Count
    word_counts = (text_bag
                  .map(str.lower)  # 转换为小写
                  .map(lambda line: line.split())  # 分词
                  .flatten()  # 展平
                  .frequencies())  # 计算频率

    result = word_counts.compute()
    print("Word frequencies:", result)

    # 按频率排序
    sorted_words = word_counts.topk(10, key=lambda x: x[1])
    print("Top 10 words:", sorted_words.compute())
```

#### 14.3.5 Dask Delayed

**延迟计算**：
```python
from dask import delayed
import time
import numpy as np

def dask_delayed_example():
    """Dask Delayed基本示例"""
    @delayed
    def add(x, y):
        """延迟的加法函数"""
        time.sleep(0.1)  # 模拟计算时间
        return x + y

    @delayed
    def multiply(x, y):
        """延迟的乘法函数"""
        time.sleep(0.1)
        return x * y

    @delayed
    def power(x, n):
        """延迟的幂函数"""
        time.sleep(0.1)
        return x ** n

    # 创建延迟计算图
    a = delayed(5)
    b = delayed(3)
    c = add(a, b)  # 延迟计算 a + b
    d = multiply(c, 2)  # 延迟计算 (a + b) * 2
    e = power(d, 2)  # 延迟计算 ((a + b) * 2) ^ 2

    # 查看计算图
    print("计算图:")
    e.visualize(filename='delayed_graph.png', format='png')

    # 执行计算
    result = e.compute()
    print(f"计算结果: {result}")

    # 比较与直接计算的差异
    direct_result = ((5 + 3) * 2) ** 2
    print(f"直接计算结果: {direct_result}")
    print(f"结果一致: {result == direct_result}")

def parallel_simulation_example():
    """并行模拟示例"""
    @delayed
    def simulate_particle(position, velocity, time_step):
        """模拟单个粒子的运动"""
        new_position = position + velocity * time_step
        return new_position

    @delayed
    def calculate_force(p1, p2, distance):
        """计算两个粒子间的力"""
        if distance == 0:
            return 0
        return 1.0 / (distance ** 2)

    def n_body_simulation():
        """N体模拟"""
        num_particles = 100
        positions = np.random.random(num_particles)
        velocities = np.random.random(num_particles) * 0.1
        time_steps = 10

        for step in range(time_steps):
            # 计算新的位置
            new_positions = []
            for i in range(num_particles):
                # 计算所有其他粒子对当前粒子的作用力
                forces = []
                for j in range(num_particles):
                    if i != j:
                        distance = abs(positions[i] - positions[j])
                        force = calculate_force(positions[i], positions[j], distance)
                        forces.append(force)

                # 合力
                total_force = sum(forces)

                # 更新速度和位置
                new_velocity = velocities[i] + total_force * 0.01
                new_pos = simulate_particle(positions[i], new_velocity, 0.1)

                new_positions.append(new_pos)
                velocities[i] = new_velocity

            positions[:] = new_positions

        return positions

    # 使用延迟计算
    delayed_positions = delayed(n_body_simulation)()
    result = delayed_positions.compute()
    print(f"模拟结果: {result[:10]}...")  # 显示前10个粒子的位置

def machine_learning_pipeline():
    """机器学习管道示例"""
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error

    @delayed
    def load_data(n_samples=10000, n_features=10):
        """加载数据"""
        X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42)
        return X, y

    @delayed
    def split_data(X, y, test_size=0.2):
        """分割数据"""
        return train_test_split(X, y, test_size=test_size, random_state=42)

    @delayed
    def train_model(X_train, y_train):
        """训练模型"""
        model = LinearRegression()
        model.fit(X_train, y_train)
        return model

    @delayed
    def evaluate_model(model, X_test, y_test):
        """评估模型"""
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        return mse

    # 构建机器学习管道
    X, y = load_data()
    X_train, X_test, y_train, y_test = split_data(X, y)
    model = train_model(X_train, y_train)
    mse = evaluate_model(model, X_test, y_test)

    # 执行管道
    result = mse.compute()
    print(f"模型MSE: {result}")

def optimization_example():
    """优化问题示例"""
    @delayed
    def evaluate_solution(solution):
        """评估解决方案"""
        # 简单的优化目标函数：最小化 x^2 + y^2
        x, y = solution
        return x**2 + y**2

    def grid_search_optimization():
        """网格搜索优化"""
        solutions = []
        for x in np.linspace(-5, 5, 20):
            for y in np.linspace(-5, 5, 20):
                solutions.append((x, y))

        # 并行评估所有解决方案
        evaluations = [evaluate_solution(sol) for sol in solutions]

        # 找到最优解
        min_eval = min(evaluations)
        best_solution = solutions[evaluations.index(min_eval)]

        return best_solution, min_eval

    result = grid_search_optimization()
    best_sol, min_val = result.compute()
    print(f"最优解: {best_sol}, 最优值: {min_val}")
```

#### 14.3.6 Dask Distributed

**分布式计算**：
```python
from dask.distributed import Client, LocalCluster, as_completed
import dask.dataframe as dd
import pandas as pd
import numpy as np
from dask import delayed
import time

def setup_distributed_cluster():
    """设置分布式集群"""
    # 本地集群
    cluster = LocalCluster(
        n_workers=4,
        threads_per_worker=2,
        memory_limit='2GB',
        processes=True
    )
    client = Client(cluster)

    print(f"Dask Dashboard: {client.dashboard_link}")
    print(f"Workers: {len(client.scheduler_info()['workers'])}")

    return client, cluster

def distributed_computing_example(client):
    """分布式计算示例"""
    # 1. 分布式数据处理
    def process_chunk(chunk_id):
        """处理数据块"""
        # 模拟数据处理
        data = np.random.randn(10000, 10)
        result = np.sum(data ** 2, axis=1)
        return result.mean()

    # 提交多个任务到集群
    futures = []
    for i in range(20):
        future = client.submit(process_chunk, i)
        futures.append(future)

    # 收集结果
    results = client.gather(futures)
    print(f"分布式处理结果: {results[:5]}...")

    # 2. 实时监控
    def monitor_progress():
        """监控任务进度"""
        while True:
            status = client.processing()
            if not status:
                break
            print(f"正在处理的任务: {len(status)}")
            time.sleep(1)

    # 3. 异步处理
    def async_processing():
        """异步处理示例"""
        @delayed
        def expensive_computation(n):
            """耗时计算"""
            time.sleep(2)
            return sum(i**2 for i in range(n))

        # 提交多个异步任务
        tasks = [expensive_computation(i * 1000) for i in range(10)]

        # 使用as_completed获取结果
        futures = client.compute(tasks)
        for future, result in as_completed(futures, with_results=True):
            print(f"任务完成，结果: {result}")

    # 4. 数据分片处理
    def shard_processing_example():
        """数据分片处理"""
        # 创建大型DataFrame
        df = dd.from_pandas(pd.DataFrame({
            'x': np.random.randn(1000000),
            'y': np.random.randn(1000000),
            'category': np.random.choice(['A', 'B', 'C'], 1000000)
        }), npartitions=100)

        # 分片处理
        def process_partition(partition):
            """处理数据分区"""
            return partition.groupby('category').agg({'x': 'mean', 'y': 'std'})

        results = df.map_partitions(process_partition).compute()
        return results

    return shard_processing_example()

def load_balancing_example(client):
    """负载均衡示例"""
    def heavy_task(task_id):
        """重任务"""
        import time
        time.sleep(np.random.uniform(1, 3))
        return f"Task {task_id} completed"

    def light_task(task_id):
        """轻任务"""
        time.sleep(0.1)
        return f"Light task {task_id} completed"

    # 提交混合任务
    heavy_futures = [client.submit(heavy_task, i) for i in range(20)]
    light_futures = [client.submit(light_task, i) for i in range(50)]

    all_futures = heavy_futures + light_futures

    # 使用as_completed实现负载均衡
    completed_count = 0
    for future in as_completed(all_futures):
        result = future.result()
        completed_count += 1
        print(f"完成: {result} (总进度: {completed_count}/{len(all_futures)})")

def fault_tolerance_example(client):
    """容错处理示例"""
    @delayed
    def unreliable_task(task_id):
        """可能失败的任务"""
        import random
        if random.random() < 0.2:  # 20%概率失败
            raise Exception(f"Task {task_id} failed")
        return f"Task {task_id} succeeded"

    # 提交任务
    tasks = [unreliable_task(i) for i in range(20)]
    futures = client.compute(tasks)

    # 处理失败的任务
    successful_results = []
    failed_tasks = []

    for future in as_completed(futures):
        try:
            result = future.result()
            successful_results.append(result)
        except Exception as e:
            failed_tasks.append(str(e))
            print(f"任务失败: {e}")

    print(f"成功任务: {len(successful_results)}")
    print(f"失败任务: {len(failed_tasks)}")

    # 重试失败的任务
    if failed_tasks:
        retry_tasks = [unreliable_task(int(task.split()[-2])) for task in failed_tasks]
        retry_futures = client.compute(retry_tasks)

        for future in as_completed(retry_futures):
            try:
                result = future.result()
                successful_results.append(result)
                print(f"重试成功: {result}")
            except Exception as e:
                print(f"重试失败: {e}")

def resource_management_example(client):
    """资源管理示例"""
    # 1. 内存管理
    def memory_intensive_task():
        """内存密集型任务"""
        # 创建大数组
        data = np.random.randn(10000, 10000)
        result = np.sum(data)
        return result

    # 2. CPU密集型任务
    def cpu_intensive_task():
        """CPU密集型任务"""
        total = 0
        for i in range(1000000):
            total += i ** 2
        return total

    # 3. I/O密集型任务
    def io_intensive_task():
        """I/O密集型任务"""
        import time
        time.sleep(1)  # 模拟I/O等待
        return "I/O task completed"

    # 提交不同类型的任务
    memory_futures = [client.submit(memory_intensive_task) for _ in range(5)]
    cpu_futures = [client.submit(cpu_intensive_task) for _ in range(10)]
    io_futures = [client.submit(io_intensive_task) for _ in range(20)]

    # 监控资源使用
    def monitor_resources():
        """监控资源使用情况"""
        while True:
            workers = client.scheduler_info()['workers']
            total_memory = sum(w['memory_limit'] for w in workers.values())
            used_memory = sum(w['memory'] for w in workers.values())
            total_cpu = sum(w['nthreads'] for w in workers.values())
            used_cpu = sum(w['cpu'] for w in workers.values())

            print(f"内存使用: {used_memory/total_memory*100:.1f}%")
            print(f"CPU使用: {used_cpu/total_cpu*100:.1f}%")

            if not any(future.status == 'pending' for future in memory_futures + cpu_futures + io_futures):
                break
            time.sleep(2)

    # 执行任务
    all_futures = memory_futures + cpu_futures + io_futures

    # 启动资源监控
    import threading
    monitor_thread = threading.Thread(target=monitor_resources)
    monitor_thread.start()

    # 等待所有任务完成
    results = client.gather(all_futures)
    monitor_thread.join()

    return results
```

#### 14.3.7 Dask性能优化

**性能调优**：
```python
import dask
import dask.array as da
import dask.dataframe as dd
import dask.bag as db
from dask.distributed import Client
import numpy as np
import pandas as pd

def chunk_size_optimization():
    """分块大小优化"""
    # 1. 数组分块优化
    def optimize_array_chunks():
        """优化数组分块"""
        # 不同的分块大小
        shapes = [(10000, 10000), (50000, 5000), (1000, 100000)]
        chunk_sizes = ['64MB', '128MB', '256MB', '512MB']

        for shape in shapes:
            print(f"\nShape: {shape}")
            for chunk_size in chunk_sizes:
                try:
                    x = da.random.random(shape, chunks=chunk_size)
                    # 执行一些操作
                    result = (x + 1).sum().compute()
                    print(f"  Chunk size: {chunk_size}, Partitions: {x.npartitions}")
                except Exception as e:
                    print(f"  Chunk size: {chunk_size}, Error: {e}")

    # 2. DataFrame分块优化
    def optimize_dataframe_partitions():
        """优化DataFrame分区"""
        # 创建测试数据
        data = pd.DataFrame({
            'x': np.random.randn(1000000),
            'y': np.random.randn(1000000),
            'z': np.random.choice(range(100), 1000000)
        })

        partition_counts = [10, 50, 100, 200, 500]

        for npartitions in partition_counts:
            ddf = dd.from_pandas(data, npartitions=npartitions)

            # 执行聚合操作
            start_time = time.time()
            result = ddf.groupby('z').agg({'x': 'mean', 'y': 'std'}).compute()
            end_time = time.time()

            print(f"Partitions: {npartitions}, Time: {end_time - start_time:.2f}s")

    return optimize_array_chunks(), optimize_dataframe_partitions()

def memory_management_optimization():
    """内存管理优化"""
    # 1. 配置内存参数
    dask.config.set({
        'distributed.worker.memory.target': 0.6,  # 目标内存使用率
        'distributed.worker.memory.spill': 0.7,   # 开始溢出到磁盘的内存使用率
        'distributed.worker.memory.pause': 0.8,   # 暂停任务的内存使用率
    })

    # 2. 使用适当的dtype
    def optimize_dtypes():
        """优化数据类型"""
        # 创建数据
        data = {
            'int_col': np.random.randint(0, 100, 1000000),
            'float_col': np.random.randn(1000000),
            'category_col': np.random.choice(['A', 'B', 'C'], 1000000)
        }

        # 优化前
        df_before = dd.from_pandas(pd.DataFrame(data), npartitions=10)
        print(f"Original memory usage: {df_before.memory_usage(deep=True).sum().compute()}")

        # 优化后
        optimized_data = {
            'int_col': data['int_col'].astype('int8'),  # 从int64优化为int8
            'float_col': data['float_col'].astype('float32'),  # 从float64优化为float32
            'category_col': data['category_col'].astype('category')  # 转换为category类型
        }

        df_after = dd.from_pandas(pd.DataFrame(optimized_data), npartitions=10)
        print(f"Optimized memory usage: {df_after.memory_usage(deep=True).sum().compute()}")

    # 3. 及时释放内存
    def memory_cleanup():
        """内存清理"""
        import gc

        # 创建大数组
        x = da.random.random((10000, 10000), chunks=(2000, 2000))
        y = x + 1
        z = y * 2
        result = z.sum()

        # 计算结果
        actual_result = result.compute()

        # 清理中间变量
        del x, y, z
        gc.collect()

        return actual_result

    return optimize_dtypes(), memory_cleanup()

def computation_optimization():
    """计算优化"""
    # 1. 使用适当的聚合函数
    def optimize_aggregations():
        """优化聚合操作"""
        # 创建测试数据
        data = dd.from_pandas(pd.DataFrame({
            'group': np.random.choice(range(100), 1000000),
            'value': np.random.randn(1000000)
        }), npartitions=50)

        # 方法1: 直接聚合
        start_time = time.time()
        result1 = data.groupby('group')['value'].mean().compute()
        time1 = time.time() - start_time
        print(f"Direct aggregation time: {time1:.2f}s")

        # 方法2: 使用reduction优化
        start_time = time.time()
        result2 = data.groupby('group')['value'].reduction(
            chunk=lambda x: x.sum(),
            aggregate=lambda x: x.sum(),
            meta=('value', 'f8')
        ).compute()
        time2 = time.time() - start_time
        print(f"Optimized aggregation time: {time2:.2f}s")

        return result1, result2

    # 2. 避免不必要的计算
    def avoid_redundant_computations():
        """避免冗余计算"""
        # 创建数据
        x = da.random.random((5000, 5000), chunks=(1000, 1000))
        y = da.random.random((5000, 5000), chunks=(1000, 1000))

        # 不好的做法：多次compute
        # result1 = (x + y).mean().compute()
        # result2 = (x * y).sum().compute()

        # 好的做法：一次性compute
        mean_val = (x + y).mean()
        sum_val = (x * y).sum()
        result1, result2 = dask.compute(mean_val, sum_val)

        return result1, result2

    # 3. 使用caching
    def use_caching():
        """使用缓存"""
        from dask.cache import Cache

        # 设置内存缓存
        cache = Cache(2e9)  # 2GB缓存
        cache.register()

        # 创建计算图
        x = da.random.random((1000, 1000), chunks=(500, 500))
        y = x + 1
        z = y * 2

        # 第一次计算
        result1 = z.sum().compute()

        # 第二次计算（会使用缓存）
        result2 = z.sum().compute()

        return result1, result2

    return optimize_aggregations(), avoid_redundant_computations(), use_caching()

def parallelization_optimization():
    """并行化优化"""
    # 1. 选择合适的调度器
    def choose_scheduler():
        """选择调度器"""
        # 单机调度器
        x = da.random.random((5000, 5000), chunks=(1000, 1000))

        # 默认调度器（多线程）
        result1 = x.sum().compute(scheduler='threads')

        # 单线程调度器
        result2 = x.sum().compute(scheduler='single-threaded')

        # 进程调度器
        result3 = x.sum().compute(scheduler='processes')

        return result1, result2, result3

    # 2. 优化任务图
    def optimize_task_graph():
        """优化任务图"""
        x = da.random.random((10000, 10000), chunks=(2000, 2000))

        # 复杂计算
        y = x + 1
        z = y * 2
        w = z ** 2
        result = w.sum()

        # 优化前的任务图
        print("Original task graph:")
        result.visualize(filename='original_graph.png', format='png')

        # 使用rechunk优化
        x_optimized = x.rechunk((5000, 5000))  # 更大的块
        y_opt = x_optimized + 1
        z_opt = y_opt * 2
        w_opt = z_opt ** 2
        result_opt = w_opt.sum()

        print("Optimized task graph:")
        result_opt.visualize(filename='optimized_graph.png', format='png')

        return result.compute(), result_opt.compute()

    # 3. 批量提交任务
    def batch_task_submission():
        """批量提交任务"""
        @delayed
        def compute_task(i):
            """计算任务"""
            return sum(j**2 for j in range(i * 1000, (i + 1) * 1000))

        # 批量创建任务
        tasks = [compute_task(i) for i in range(100)]

        # 批量提交
        results = dask.compute(*tasks)
        return results

    return choose_scheduler(), optimize_task_graph(), batch_task_submission()

def io_optimization():
    """I/O优化"""
    # 1. 使用适当的文件格式
    def choose_file_format():
        """选择文件格式"""
        # 创建测试数据
        data = pd.DataFrame({
            'x': np.random.randn(1000000),
            'y': np.random.randn(1000000),
            'z': np.random.choice(range(100), 1000000)
        })

        # CSV格式
        start_time = time.time()
        data.to_csv('test.csv', index=False)
        ddf_csv = dd.read_csv('test.csv')
        csv_time = time.time() - start_time
        print(f"CSV I/O time: {csv_time:.2f}s")

        # Parquet格式
        start_time = time.time()
        data.to_parquet('test.parquet')
        ddf_parquet = dd.read_parquet('test.parquet')
        parquet_time = time.time() - start_time
        print(f"Parquet I/O time: {parquet_time:.2f}s")

        # HDF5格式
        start_time = time.time()
        data.to_hdf('test.hdf5', key='data')
        ddf_hdf5 = dd.read_hdf('test.hdf5', key='data')
        hdf5_time = time.time() - start_time
        print(f"HDF5 I/O time: {hdf5_time:.2f}s")

        return {
            'csv': (csv_time, ddf_csv.shape[0].compute()),
            'parquet': (parquet_time, ddf_parquet.shape[0].compute()),
            'hdf5': (hdf5_time, ddf_hdf5.shape[0].compute())
        }

    # 2. 并行I/O
    def parallel_io():
        """并行I/O操作"""
        # 创建多个文件
        for i in range(10):
            chunk = pd.DataFrame({
                'x': np.random.randn(100000),
                'y': np.random.randn(100000),
                'file_id': i
            })
            chunk.to_csv(f'data_chunk_{i}.csv', index=False)

        # 并行读取
        start_time = time.time()
        ddf = dd.read_csv('data_chunk_*.csv')
        result = ddf.groupby('file_id').agg({'x': 'mean', 'y': 'std'}).compute()
        parallel_time = time.time() - start_time
        print(f"Parallel I/O time: {parallel_time:.2f}s")

        # 串行读取对比
        start_time = time.time()
        dfs = []
        for i in range(10):
            df = pd.read_csv(f'data_chunk_{i}.csv')
            dfs.append(df)
        combined_df = pd.concat(dfs)
        result_serial = combined_df.groupby('file_id').agg({'x': 'mean', 'y': 'std'})
        serial_time = time.time() - start_time
        print(f"Serial I/O time: {serial_time:.2f}s")

        return result, result_serial

    return choose_file_format(), parallel_io()
```

#### 14.3.8 Dask实际应用案例

**大数据分析流水线**：
```python
import dask.dataframe as dd
import dask.array as da
import pandas as pd
import numpy as np
from dask.distributed import Client
import time
from datetime import datetime, timedelta

class DataAnalysisPipeline:
    """大数据分析流水线示例"""

    def __init__(self, client):
        self.client = client

    def load_and_preprocess_data(self, file_pattern):
        """加载和预处理数据"""
        # 加载数据
        df = dd.read_csv(file_pattern, blocksize="64MB")

        # 数据清洗
        df = df.dropna()
        df = df[df['price'] > 0]
        df = df[df['quantity'] > 0]

        # 数据类型优化
        df['date'] = dd.to_datetime(df['date'])
        df['category'] = df['category'].astype('category')
        df['price'] = df['price'].astype('float32')

        # 添加衍生特征
        df['total_value'] = df['price'] * df['quantity']
        df['month'] = df['date'].dt.month
        df['day_of_week'] = df['date'].dt.dayofweek

        return df

    def sales_analysis(self, df):
        """销售分析"""
        # 按产品类别的销售分析
        category_sales = df.groupby('category').agg({
            'total_value': ['sum', 'mean', 'std'],
            'quantity': 'sum',
            'transaction_id': 'count'
        }).rename(columns={'transaction_id': 'transaction_count'})

        # 按时间的销售趋势
        time_sales = df.groupby(['date', 'category']).agg({
            'total_value': 'sum',
            'quantity': 'sum'
        }).reset_index()

        # 按门店的销售分析
        store_sales = df.groupby('store_id').agg({
            'total_value': ['sum', 'mean'],
            'quantity': 'sum',
            'transaction_id': 'nunique'
        })

        return category_sales, time_sales, store_sales

    def customer_analysis(self, df):
        """客户分析"""
        # 客户购买行为
        customer_behavior = df.groupby('customer_id').agg({
            'total_value': ['sum', 'mean', 'count'],
            'date': ['min', 'max'],
            'category': 'nunique'
        })

        # 客户生命周期价值
        customer_ltv = customer_behavior['total_value']['sum']
        customer_recency = (customer_behavior['date']['max'] - customer_behavior['date']['min']).dt.days

        # 客户分群
        def categorize_customer(row):
            if row['total_value']['sum'] > 10000 and row['total_value']['count'] > 50:
                return 'VIP'
            elif row['total_value']['sum'] > 5000:
                return 'High Value'
            elif row['total_value']['count'] > 20:
                return 'Frequent'
            else:
                return 'Regular'

        customer_segments = customer_behavior.apply(categorize_customer, axis=1)
        customer_behavior['segment'] = customer_segments

        return customer_behavior, customer_segments

    def inventory_analysis(self, df):
        """库存分析"""
        # 产品销售速度
        product_velocity = df.groupby('product_id').agg({
            'quantity': ['sum', 'mean'],
            'date': ['min', 'max']
        })

        # 库存周转率计算
        product_velocity['days_active'] = (
            product_velocity['date']['max'] - product_velocity['date']['min']
        ).dt.days + 1

        product_velocity['velocity'] = (
            product_velocity['quantity']['sum'] / product_velocity['days_active']
        )

        # 滞销产品识别
        slow_moving = product_velocity[
            product_velocity['velocity'] < product_velocity['velocity'].quantile(0.1)
        ]

        return product_velocity, slow_moving

    def run_complete_analysis(self, data_path):
        """运行完整的分析流水线"""
        print("开始数据分析流水线...")

        # 1. 数据加载和预处理
        start_time = time.time()
        df = self.load_and_preprocess_data(data_path)
        preprocess_time = time.time() - start_time
        print(f"数据预处理完成，耗时: {preprocess_time:.2f}秒")

        # 2. 销售分析
        start_time = time.time()
        category_sales, time_sales, store_sales = self.sales_analysis(df)
        sales_time = time.time() - start_time
        print(f"销售分析完成，耗时: {sales_time:.2f}秒")

        # 3. 客户分析
        start_time = time.time()
        customer_behavior, customer_segments = self.customer_analysis(df)
        customer_time = time.time() - start_time
        print(f"客户分析完成，耗时: {customer_time:.2f}秒")

        # 4. 库存分析
        start_time = time.time()
        product_velocity, slow_moving = self.inventory_analysis(df)
        inventory_time = time.time() - start_time
        print(f"库存分析完成，耗时: {inventory_time:.2f}秒")

        # 5. 计算结果
        results = {
            'category_sales': category_sales.compute(),
            'time_sales': time_sales.compute(),
            'store_sales': store_sales.compute(),
            'customer_behavior': customer_behavior.compute(),
            'customer_segments': customer_segments.compute(),
            'product_velocity': product_velocity.compute(),
            'slow_moving': slow_moving.compute()
        }

        total_time = time.time() - start_time
        print(f"分析流水线总耗时: {total_time:.2f}秒")

        return results

def machine_learning_pipeline_example():
    """机器学习流水线示例"""
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error, r2_score
    from dask_ml.model_selection import train_test_split as dask_train_test_split
    from dask_ml.ensemble import RandomForestRegressor as DaskRandomForestRegressor

    def create_large_dataset():
        """创建大型数据集"""
        n_samples = 1000000
        n_features = 20

        X = da.random.random((n_samples, n_features), chunks=(100000, n_features))
        y = da.random.random(n_samples, chunks=(100000,))

        # 添加一些真实的信号
        X_signal = X[:, :5] * 2 + 0.5
        y = X_signal.sum(axis=1) + da.random.random(n_samples, chunks=(100000,)) * 0.1

        return X, y

    def distributed_ml_pipeline():
        """分布式机器学习流水线"""
        # 1. 数据准备
        X, y = create_large_dataset()

        # 2. 数据分割
        X_train, X_test, y_train, y_test = dask_train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # 3. 模型训练
        model = DaskRandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )

        start_time = time.time()
        model.fit(X_train, y_train)
        training_time = time.time() - start_time
        print(f"模型训练耗时: {training_time:.2f}秒")

        # 4. 模型预测
        start_time = time.time()
        y_pred = model.predict(X_test)
        prediction_time = time.time() - start_time
        print(f"模型预测耗时: {prediction_time:.2f}秒")

        # 5. 模型评估
        mse = mean_squared_error(y_test.compute(), y_pred.compute())
        r2 = r2_score(y_test.compute(), y_pred.compute())

        print(f"模型性能:")
        print(f"  MSE: {mse:.4f}")
        print(f"  R²: {r2:.4f}")

        return model, mse, r2

    return distributed_ml_pipeline()

def real_time_processing_example():
    """实时处理示例"""
    import asyncio
    from dask.distributed import as_completed

    class RealTimeProcessor:
        """实时数据处理器"""

        def __init__(self, client):
            self.client = client
            self.buffer = []
            self.batch_size = 1000
            self.processing_interval = 5  # 秒

        async def process_stream(self, data_stream):
            """处理数据流"""
            while True:
                # 收集数据
                batch_data = []
                for _ in range(self.batch_size):
                    try:
                        data = await asyncio.wait_for(
                            data_stream.get(), timeout=self.processing_interval
                        )
                        batch_data.append(data)
                    except asyncio.TimeoutError:
                        break

                if batch_data:
                    # 异步处理批次
                    future = self.client.submit(self.process_batch, batch_data)
                    result = await future.result()
                    print(f"批次处理完成，结果: {result}")

        def process_batch(self, batch_data):
            """处理单个批次"""
            # 模拟数据处理
            import time
            time.sleep(1)  # 模拟处理时间

            # 计算统计信息
            values = [item['value'] for item in batch_data]
            stats = {
                'count': len(values),
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }

            return stats

    # 模拟数据流
    async def data_generator():
        """数据生成器"""
        while True:
            yield {
                'timestamp': time.time(),
                'value': np.random.randn(),
                'source': f'sensor_{np.random.randint(1, 10)}'
            }
            await asyncio.sleep(0.1)

    async def run_real_time_processing():
        """运行实时处理"""
        client = Client()
        processor = RealTimeProcessor(client)

        # 启动数据生成器
        data_stream = asyncio.Queue()
        generator_task = asyncio.create_task(
            data_generator_to_queue(data_generator(), data_stream)
        )

        # 启动处理器
        processor_task = asyncio.create_task(
            processor.process_stream(data_stream)
        )

        # 运行一段时间
        await asyncio.sleep(30)

        # 清理
        generator_task.cancel()
        processor_task.cancel()

    async def data_generator_to_queue(generator, queue):
        """将数据生成器连接到队列"""
        async for data in generator:
            await queue.put(data)

    return run_real_time_processing
```

这个扩展后的Dask章节提供了：
- ✅ 详细的Dask架构和核心概念介绍
- ✅ 完整的Dask Array、DataFrame、Bag、Delayed使用方法
- ✅ 分布式计算和集群管理
- ✅ 性能优化策略和最佳实践
- ✅ 实际应用案例（数据分析流水线、机器学习、实时处理）
- ✅ 实用的代码示例和最佳实践指导

## 第15章 性能分析工具

### 15.1 通用分析工具
- **gprof**：GNU性能分析器
- **perf**：Linux性能分析工具
- **Valgrind**：内存和性能分析

### 15.2 MPI分析工具
- **TAU**：Tuning and Analysis Utilities
- **Vampir**：可视化性能分析
- **mpiP**：轻量级MPI分析器
- **Intel Trace Analyzer**：Intel MPI分析工具

### 15.3 GPU分析工具
- **Nsight Systems**：NVIDIA系统级分析
- **Nsight Compute**：NVIDIA内核级分析
- **nvprof**：NVIDIA命令行分析器
- **CUDA-GDB**：CUDA调试器

### 15.4 代码示例：性能分析
```c
// 使用TAU进行性能分析
#include <profile/tau.h>

TAU_MAIN(argc, argv)

int main(int argc, char** argv) {
    TAU_PROFILE_TIMER(total_timer, "main", "", TAU_USER);

    TAU_PROFILE_START(total_timer);

    // 应用代码
    TAU_PROFILE_TIMER(compute_timer, "compute", "", TAU_USER);
    TAU_PROFILE_START(compute_timer);

    // 计算密集型代码
    for (int i = 0; i < n; i++) {
        // 计算...
    }

    TAU_PROFILE_STOP(compute_timer);

    TAU_PROFILE_STOP(total_timer);
    return 0;
}
```

## 第16章 构建和部署工具

### 16.1 构建系统
- **CMake**：跨平台构建系统
- **Make**：传统构建工具
- **autotools**：GNU构建系统

### 16.2 包管理
- **Conda**：Python包管理
- **Spack**：HPC软件包管理
- **EasyBuild**：科学软件构建

### 16.3 容器化
```dockerfile
# Dockerfile示例
FROM nvidia/cuda:11.0-devel-ubuntu20.04

# 安装MPI
RUN apt-get update && apt-get install -y \
    mpich \
    && rm -rf /var/lib/apt/lists/*

# 编译MPI程序
COPY mpi_example.c /app/
WORKDIR /app
RUN mpicc -o mpi_example mpi_example.c

CMD ["mpiexec", "-n", "4", "./mpi_example"]
```

### 16.4 作业调度系统
- **SLURM**：集群作业调度
- **PBS/Torque**：传统作业调度
- **LSF**：IBM作业调度系统

```bash
# SLURM作业脚本示例
#!/bin/bash
#SBATCH --job-name=mpi_job
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=16
#SBATCH --time=01:00:00
#SBATCH --output=output_%j.log

module load mpi/openmpi-x86_64
mpirun -np 64 ./my_mpi_program
```

## 实用技巧

### MPI调试技巧
```c
// 添加调试信息
#ifdef DEBUG
    printf("Rank %d: Processing chunk %d\n", rank, chunk_id);
    fflush(stdout);
#endif

// 检查MPI错误
int error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);
if (error != MPI_SUCCESS) {
    printf("MPI error occurred\n");
    MPI_Abort(MPI_COMM_WORLD, error);
}
```

### OpenMP性能调优
```c
// 设置线程数
export OMP_NUM_THREADS=8

// 调整调度策略
#pragma omp parallel for schedule(dynamic, 100)

// 使用NUMA感知
#pragma omp parallel num_threads(16)
{
    #pragma omp for schedule(static)
    for (int i = 0; i < n; i++) {
        // 工作...
    }
}
```

### CUDA内存优化
```cuda
// 使用pinned memory提高传输速度
float *h_data;
cudaMallocHost(&h_data, size);

// 使用异步传输
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);

// 合理使用共享内存
__shared__ float cache[BLOCK_SIZE];
```