# 第二章 并行计算模型

## 2.1 SIMD（单指令多数据）

### 2.1.1 概念定义

SIMD（Single Instruction, Multiple Data）是一种并行计算模型，其中所有处理单元同时执行相同的指令，但处理不同的数据元素。这种模型特别适合处理规则的数据并行操作。

### 2.1.2 架构特点

**统一的指令流控制多个数据流**：
- 所有处理单元在同一时钟周期执行相同的指令
- 每个处理单元处理不同的数据元素
- 高度同步的执行模式

**高度同步的执行模式**：
- 所有处理单元步调一致
- 适合规则的计算模式
- 避免了复杂的同步机制

**适合规则的数据并行操作**：
- 数组运算
- 矩阵计算
- 图像处理

### 2.1.3 SIMD指令集架构

#### x86架构的SIMD指令集

**MMX（MultiMedia eXtensions）**：
```assembly
# MMX指令示例
PADDW mm0, mm1    # 16位整数加法
PMULLW mm0, mm1   # 16位整数乘法
```

**SSE（Streaming SIMD Extensions）**：
```assembly
# SSE指令示例
ADDPS xmm0, xmm1  # 32位浮点数加法（4个并行）
MULPS xmm0, xmm1  # 32位浮点数乘法（4个并行）
```

**AVX（Advanced Vector Extensions）**：
```assembly
# AVX指令示例
VADDPS ymm0, ymm1, ymm2  # 256位向量加法（8个32位浮点数）
VMULPS ymm0, ymm1, ymm2  # 256位向量乘法（8个32位浮点数）
```

#### ARM架构的SIMD指令集

**NEON**：
```assembly
# NEON指令示例
VADD.I32 q0, q1, q2    # 32位整数向量加法
VMUL.I32 q0, q1, q2    # 32位整数向量乘法
```

### 2.1.4 SIMD编程实践

#### C/C++中的SIMD编程

**使用内联汇编**：
```cpp
#include <immintrin.h>

// AVX向量加法
__m256 simd_add(__m256 a, __m256 b) {
    return _mm256_add_ps(a, b);
}

// 使用示例
float array1[8] = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f};
float array2[8] = {0.5f, 1.5f, 2.5f, 3.5f, 4.5f, 5.5f, 6.5f, 7.5f};
float result[8];

__m256 vec1 = _mm256_loadu_ps(array1);
__m256 vec2 = _mm256_loadu_ps(array2);
__m256 vec_result = simd_add(vec1, vec2);
_mm256_storeu_ps(result, vec_result);
```

**自动向量化**：
```cpp
// 编译器自动向量化
void vector_add(float* a, float* b, float* c, int n) {
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];  // 编译器可能自动向量化
    }
}

// 使用编译器指令提示向量化
#pragma omp simd
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
```

### 2.1.5 SIMD的优缺点分析

**优势**：
- ✅ **高吞吐量**：一次指令处理多个数据元素
- ✅ **内存带宽利用率高**：批量加载和存储数据
- ✅ **功耗效率高**：相比多个标量操作更节能
- ✅ **实现简单**：编程模型相对简单

**局限性**：
- ❌ **灵活性差**：不支持分支和条件执行
- ❌ **数据依赖性要求高**：需要规则的数据访问模式
- ❌ **内存对齐要求**：通常需要数据对齐
- ❌ **调试困难**：向量化代码调试复杂

### 2.1.6 SIMD应用场景

#### 图像处理
```python
# Python中的SIMD风格图像处理
import numpy as np

def brighten_image_simd(image, brightness):
    """使用SIMD思想进行图像亮度调整"""
    # 将亮度值广播到整个数组
    brightness_array = np.full_like(image, brightness)
    return np.clip(image + brightness_array, 0, 255)

# 实际应用中会使用OpenCV等库的SIMD优化函数
import cv2
image = cv2.imread('image.jpg')
brightened = cv2.add(image, np.array([50.0]))  # SIMD优化的加法
```

#### 科学计算
```cpp
// 矩阵向量乘法的SIMD实现
void matrix_vector_multiply_simd(float* matrix, float* vector,
                               float* result, int rows, int cols) {
    for (int i = 0; i < rows; i++) {
        __m256 sum = _mm256_setzero_ps();

        // 处理8个元素的块
        for (int j = 0; j < cols; j += 8) {
            __m256 mat_row = _mm256_loadu_ps(&matrix[i * cols + j]);
            __m256 vec_col = _mm256_loadu_ps(&vector[j]);
            sum = _mm256_add_ps(sum, _mm256_mul_ps(mat_row, vec_col));
        }

        // 水平求和
        __m128 sum_low = _mm256_extractf128_ps(sum, 0);
        __m128 sum_high = _mm256_extractf128_ps(sum, 1);
        __m128 final_sum = _mm_add_ps(sum_low, sum_high);
        final_sum = _mm_hadd_ps(final_sum, final_sum);
        final_sum = _mm_hadd_ps(final_sum, final_sum);

        result[i] = _mm_cvtss_f32(final_sum);
    }
}
```

## 2.2 MIMD（多指令多数据）

### 2.2.1 概念定义

MIMD（Multiple Instruction, Multiple Data）是一种并行计算模型，其中每个处理单元可以独立执行不同的指令，处理不同的数据。这种模型提供了最大的灵活性，但也带来了复杂的同步和通信问题。

### 2.2.2 架构分类

#### SMP（对称多处理器）

**特点**：
- 所有处理器共享同一内存空间
- 处理器具有相同的访问权限
- 统一的内存地址空间

**优势**：
- 编程简单，共享内存模型
- 数据共享方便
- 适合紧密耦合的应用

**劣势**：
- 内存带宽限制
- 缓存一致性开销
- 可扩展性有限

**实现示例**：
```cpp
// SMP环境下的OpenMP并行
#include <omp.h>

void parallel_smp_example(int* data, int size) {
    #pragma omp parallel for
    for (int i = 0; i < size; i++) {
        data[i] = data[i] * 2 + 1;  // 每个线程执行不同指令
    }
}
```

#### Cluster（计算机集群）

**特点**：
- 每个节点有独立的内存
- 通过网络互连
- 分布式内存架构

**优势**：
- 可扩展性好
- 成本相对较低
- 容错能力强

**劣势**：
- 通信开销大
- 编程复杂
- 调试困难

**实现示例**：
```cpp
// 集群环境下的MPI并行
#include <mpi.h>

void parallel_cluster_example() {
    int rank, size;
    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 每个进程执行不同代码
    if (rank == 0) {
        // 主进程
        int data = 42;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
    } else {
        // 工作进程
        int data;
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        data = data * 2;
        MPI_Send(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }

    MPI_Finalize();
}
```

#### NUMA（非统一内存访问）

**特点**：
- 内存访问时间不一致
- 靠近处理器的内存访问更快
- 现代多核系统的典型架构

**优化策略**：
```cpp
// NUMA感知的内存分配
#include <numa.h>
#include <numaif.h>

void numa_optimized_allocation() {
    // 获取NUMA节点信息
    int num_nodes = numa_num_task_nodes();
    struct bitmask* mask = numa_allocate_nodemask();

    // 在特定节点分配内存
    numa_bitmask_clearall(mask);
    numa_bitmask_setbit(mask, 0);  // 选择节点0

    void* memory = numa_alloc_onnode(size, 0);

    // 绑定线程到特定节点
    numa_bitmask_clearall(mask);
    numa_bitmask_setbit(mask, 0);
    numa_bind(mask);

    numa_free_nodemask(mask);
    numa_free(memory, size);
}
```

### 2.2.3 MIMD编程模型

#### 消息传递模型
```cpp
// MPI消息传递示例
void mpi_message_passing_example() {
    MPI_Init(NULL, NULL);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // 点对点通信
    if (rank == 0) {
        int data = 100;
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
    } else if (rank == 1) {
        int data;
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Received: %d\n", data);
    }

    // 集体通信
    int global_sum;
    MPI_Allreduce(&rank, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    MPI_Finalize();
}
```

#### 共享内存模型
```cpp
// Pthreads共享内存示例
#include <pthread.h>

typedef struct {
    int* data;
    int start;
    int end;
    int result;
} thread_data;

void* thread_function(void* arg) {
    thread_data* data = (thread_data*)arg;
    int sum = 0;

    for (int i = data->start; i < data->end; i++) {
        sum += data->data[i];
    }

    data->result = sum;
    return NULL;
}

void pthreads_shared_memory_example() {
    const int size = 1000;
    int data[size];
    pthread_t threads[4];
    thread_data thread_data_array[4];

    // 初始化数据
    for (int i = 0; i < size; i++) {
        data[i] = i;
    }

    // 创建线程
    for (int i = 0; i < 4; i++) {
        thread_data_array[i].data = data;
        thread_data_array[i].start = i * (size / 4);
        thread_data_array[i].end = (i + 1) * (size / 4);
        pthread_create(&threads[i], NULL, thread_function, &thread_data_array[i]);
    }

    // 等待线程完成
    int total_sum = 0;
    for (int i = 0; i < 4; i++) {
        pthread_join(threads[i], NULL);
        total_sum += thread_data_array[i].result;
    }

    printf("Total sum: %d\n", total_sum);
}
```

### 2.2.4 MIMD的同步机制

#### 互斥锁（Mutex）
```cpp
#include <pthread.h>

pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
int shared_counter = 0;

void* counter_thread(void* arg) {
    for (int i = 0; i < 1000000; i++) {
        pthread_mutex_lock(&mutex);
        shared_counter++;
        pthread_mutex_unlock(&mutex);
    }
    return NULL;
}
```

#### 条件变量（Condition Variables）
```cpp
pthread_mutex_t buffer_mutex = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t buffer_not_full = PTHREAD_COND_INITIALIZER;
pthread_cond_t buffer_not_empty = PTHREAD_COND_INITIALIZER;
int buffer[100];
int count = 0;

void* producer(void* arg) {
    for (int i = 0; i < 1000; i++) {
        pthread_mutex_lock(&buffer_mutex);
        while (count == 100) {
            pthread_cond_wait(&buffer_not_full, &buffer_mutex);
        }
        buffer[count++] = i;
        pthread_cond_signal(&buffer_not_empty);
        pthread_mutex_unlock(&buffer_mutex);
    }
    return NULL;
}

void* consumer(void* arg) {
    for (int i = 0; i < 1000; i++) {
        pthread_mutex_lock(&buffer_mutex);
        while (count == 0) {
            pthread_cond_wait(&buffer_not_empty, &buffer_mutex);
        }
        int item = buffer[--count];
        pthread_cond_signal(&buffer_not_full);
        pthread_mutex_unlock(&buffer_mutex);
        // 处理item
    }
    return NULL;
}
```

## 2.3 共享内存模型

### 2.3.1 概念定义

共享内存模型是一种并行计算架构，其中所有处理器共享同一内存空间。处理器通过读写共享内存中的变量来进行通信和同步。

### 2.3.2 架构特点

**统一内存空间**：
- 所有处理器访问同一地址空间
- 内存地址在所有处理器间一致
- 简化了数据共享

**通信机制**：
- 通过共享变量进行通信
- 不需要显式的消息传递
- 通信开销相对较小

**同步需求**：
- 需要同步机制防止竞态条件
- 实现互斥和顺序控制
- 保证数据一致性

### 2.3.3 内存一致性模型

#### 严格一致性（Strict Consistency）
- 所有处理器看到的内存更新顺序完全一致
- 理论模型，实际硬件难以实现

#### 顺序一致性（Sequential Consistency）
- 所有处理器的操作按照某个全局顺序执行
- 每个处理器的操作保持程序顺序

#### 释放一致性（Release Consistency）
- 区分获取（acquire）和释放（release）操作
- 更宽松的一致性模型，性能更好

### 2.3.4 共享内存编程

#### OpenMP共享内存编程
```cpp
#include <omp.h>
#include <iostream>

void openmp_shared_memory_example() {
    const int size = 1000000;
    int data[size];
    int sum = 0;

    // 初始化数据
    #pragma omp parallel for
    for (int i = 0; i < size; i++) {
        data[i] = i;
    }

    // 并行计算总和
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < size; i++) {
        sum += data[i];
    }

    std::cout << "Sum: " << sum << std::endl;
}
```

#### C++11线程库
```cpp
#include <thread>
#include <mutex>
#include <vector>
#include <iostream>

std::mutex mtx;
int shared_data = 0;

void worker_thread(int id, int iterations) {
    for (int i = 0; i < iterations; i++) {
        std::lock_guard<std::mutex> lock(mtx);
        shared_data += id;
        std::cout << "Thread " << id << ": shared_data = " << shared_data << std::endl;
    }
}

void cpp11_threads_example() {
    std::vector<std::thread> threads;

    // 创建多个线程
    for (int i = 1; i <= 4; i++) {
        threads.emplace_back(worker_thread, i, 100);
    }

    // 等待所有线程完成
    for (auto& t : threads) {
        t.join();
    }
}
```

### 2.3.5 共享内存的挑战

#### 竞态条件
```cpp
// 危险的竞态条件示例
int counter = 0;

void unsafe_increment() {
    int temp = counter;  // 读取
    temp = temp + 1;     // 修改
    counter = temp;      // 写回
    // 在多线程环境下，这个操作不是原子的
}
```

**解决方案**：
```cpp
#include <atomic>

std::atomic<int> safe_counter(0);

void safe_increment() {
    safe_counter.fetch_add(1);  // 原子操作
}
```

#### 死锁
```cpp
std::mutex mutex1, mutex2;

void deadlock_example() {
    std::thread t1([](){
        std::lock_guard<std::mutex> lock1(mutex1);
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        std::lock_guard<std::mutex> lock2(mutex2);  // 可能死锁
    });

    std::thread t2([](){
        std::lock_guard<std::mutex> lock2(mutex2);
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        std::lock_guard<std::mutex> lock1(mutex1);  // 可能死锁
    });

    t1.join();
    t2.join();
}
```

**死锁预防**：
```cpp
// 使用std::lock避免死锁
void deadlock_free_example() {
    std::thread t1([](){
        std::lock(mutex1, mutex2);
        std::lock_guard<std::mutex> lock1(mutex1, std::adopt_lock);
        std::lock_guard<std::mutex> lock2(mutex2, std::adopt_lock);
        // 安全操作
    });

    std::thread t2([](){
        std::lock(mutex1, mutex2);
        std::lock_guard<std::mutex> lock1(mutex1, std::adopt_lock);
        std::lock_guard<std::mutex> lock2(mutex2, std::adopt_lock);
        // 安全操作
    });

    t1.join();
    t2.join();
}
```

## 2.4 分布式内存模型

### 2.4.1 概念定义

分布式内存模型是一种并行计算架构，其中每个处理器有独立的内存空间，处理器之间通过消息传递进行通信。

### 2.4.2 架构特点

**独立内存空间**：
- 每个处理器有自己独立的内存
- 内存地址空间不共享
- 需要显式的数据传输

**消息传递通信**：
- 通过发送和接收消息进行通信
- 通信开销相对较大
- 适合松耦合的应用

**可扩展性**：
- 理论上可以无限扩展
- 适合大规模并行系统
- 容错能力强

### 2.4.3 消息传递接口（MPI）

#### MPI基本概念
```cpp
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    // 初始化MPI
    MPI_Init(&argc, &argv);

    // 获取进程信息
    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    std::cout << "Hello from process " << world_rank
              << " of " << world_size << std::endl;

    // 结束MPI
    MPI_Finalize();
    return 0;
}
```

#### 点对点通信
```cpp
void mpi_point_to_point_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        int data = 42;
        // 阻塞发送
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
        std::cout << "Process 0 sent data: " << data << std::endl;
    } else if (rank == 1) {
        int data;
        // 阻塞接收
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        std::cout << "Process 1 received data: " << data << std::endl;
    }
}
```

#### 非阻塞通信
```cpp
void mpi_nonblocking_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    MPI_Request request;
    MPI_Status status;

    if (rank == 0) {
        int data = 100;
        MPI_Isend(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);
        // 可以进行其他计算
        MPI_Wait(&request, &status);
        std::cout << "Process 0 sent data: " << data << std::endl;
    } else if (rank == 1) {
        int data;
        MPI_Irecv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);
        // 可以进行其他计算
        MPI_Wait(&request, &status);
        std::cout << "Process 1 received data: " << data << std::endl;
    }
}
```

#### 集体通信
```cpp
void mpi_collective_example() {
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int local_data = rank * 10;
    int global_sum;

    // 全局求和
    MPI_Allreduce(&local_data, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
    std::cout << "Process " << rank << ": local_data=" << local_data
              << ", global_sum=" << global_sum << std::endl;

    // 广播
    int broadcast_data;
    if (rank == 0) {
        broadcast_data = 999;
    }
    MPI_Bcast(&broadcast_data, 1, MPI_INT, 0, MPI_COMM_WORLD);
    std::cout << "Process " << rank << ": broadcast_data=" << broadcast_data << std::endl;
}
```

### 2.4.4 分布式共享内存（DSM）

#### 概念
分布式共享内存是一种抽象，它让分布式内存系统看起来像共享内存系统。

#### 实现方式
```cpp
// 伪代码示例：DSM的基本思想
class DistributedSharedMemory {
private:
    std::map<void*, MemoryRegion> memory_map;
    NetworkManager network;

public:
    void* allocate_shared(size_t size) {
        void* local_addr = malloc(size);
        MemoryRegion region = {local_addr, size, current_node_id};
        memory_map[local_addr] = region;
        return local_addr;
    }

    void write(void* addr, const void* data, size_t size) {
        if (is_local(addr)) {
            memcpy(addr, data, size);
        } else {
            // 通过网络发送数据
            MemoryRegion& region = memory_map[addr];
            network.send(region.node_id, addr, data, size);
        }
    }

    void read(void* addr, void* buffer, size_t size) {
        if (is_local(addr)) {
            memcpy(buffer, addr, size);
        } else {
            // 通过网络读取数据
            MemoryRegion& region = memory_map[addr];
            network.receive(region.node_id, addr, buffer, size);
        }
    }
};
```

### 2.4.5 分布式内存模型的优缺点

**优势**：
- ✅ **可扩展性好**：可以扩展到数千个处理器
- ✅ **容错能力强**：单个节点故障不影响整个系统
- ✅ **成本效益高**：可以使用普通计算机组成集群
- ✅ **灵活性高**：适合各种类型的应用

**劣势**：
- ❌ **通信开销大**：网络通信比内存访问慢
- ❌ **编程复杂**：需要显式管理数据分布和通信
- ❌ **调试困难**：分布式系统的调试比单机系统复杂
- ❌ **负载均衡难**：需要仔细设计算法确保负载均衡

## 2.5 混合并行模型

### 2.5.1 多级并行

现代高性能计算系统通常采用多级并行架构：

```cpp
// 混合OpenMP + MPI示例
#include <mpi.h>
#include <omp.h>

void hybrid_parallel_example() {
    MPI_Init(NULL, NULL);

    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // MPI进程间并行
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        int num_threads = omp_get_num_threads();

        printf("Process %d, Thread %d of %d\n",
               world_rank, thread_id, num_threads);

        // 每个线程执行不同的计算
        for (int i = thread_id; i < 1000; i += num_threads) {
            // 计算任务
        }
    }

    MPI_Finalize();
}
```

### 2.5.2 异构计算

结合CPU和GPU的异构计算架构：

```cpp
// CPU + GPU混合计算示例
#include <cuda_runtime.h>

void heterogeneous_computing_example() {
    // CPU部分
    #pragma omp parallel for
    for (int i = 0; i < 1000; i++) {
        // CPU计算
    }

    // GPU部分
    float* d_data;
    cudaMalloc(&d_data, size * sizeof(float));

    // 启动CUDA kernel
    kernel_function<<<blocks, threads>>>(d_data, size);

    cudaDeviceSynchronize();
    cudaFree(d_data);
}
```

## 2.6 本章小结

本章介绍了四种主要的并行计算模型：

1. **SIMD模型**：适合规则的数据并行操作，提供高吞吐量但灵活性有限
2. **MIMD模型**：提供最大的灵活性，包括SMP、Cluster和NUMA等架构
3. **共享内存模型**：通过共享变量通信，编程相对简单但需要处理同步问题
4. **分布式内存模型**：通过消息传递通信，可扩展性好但编程复杂

每种模型都有其适用的场景和优缺点。在实际应用中，通常会根据具体需求选择合适的模型，或者结合多种模型形成混合并行架构。

理解这些并行计算模型是设计和实现高效并行算法的基础。在后续章节中，我们将深入探讨并行算法的设计和优化技术。

## 练习题

1. **简答题**：比较SIMD和MIMD模型的优缺点。
2. **分析题**：分析共享内存模型中的竞态条件问题，并提出解决方案。
3. **编程题**：实现一个基于MPI的分布式矩阵乘法程序。
4. **设计题**：设计一个混合OpenMP和MPI的并行程序架构。
5. **讨论题**：讨论在现代多核处理器上，哪种并行模型最适合科学计算应用。