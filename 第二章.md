# 第二章 并行计算模型

并行计算模型是并行算法设计与硬件架构之间的桥梁。本章将基于Flynn分类法，深入探讨两种主流的并行计算模型：SIMD（单指令多数据）和MIMD（多指令多数据）。同时，我们将从编程模型的角度，详细分析共享内存模型和分布式内存模型，并探讨混合并行计算架构。

## 2.1 SIMD（单指令多数据）模型

### 2.1.1 核心概念

**SIMD (Single Instruction, Multiple Data)** 是一种数据并行模式。在这种模型中，控制单元向所有处理单元广播同一条指令，但每个处理单元操作不同的数据元素。

> **核心特征**：
> *   **锁步执行 (Lock-step)**：所有活动的处理单元在同一时钟周期执行相同的指令。
> *   **数据并行**：特别适合处理结构规则、计算密集型的数组或矩阵运算。

### 2.1.2 硬件架构与指令集

现代处理器通过扩展指令集来支持SIMD。

#### 1. x86架构 (Intel/AMD)
*   **MMX**：早期的64位SIMD扩展。
*   **SSE (Streaming SIMD Extensions)**：引入128位寄存器 (XMM)，支持4个单精度浮点数并行计算。
*   **AVX (Advanced Vector Extensions)**：扩展至256位 (YMM) 和 512位 (ZMM) 寄存器，大幅提升浮点计算能力。

```assembly
; AVX指令示例：8个单精度浮点数加法
vaddps ymm0, ymm1, ymm2  ; ymm0 = ymm1 + ymm2 (同时执行8个加法)
```

#### 2. ARM架构
*   **NEON**：广泛用于移动设备的128位SIMD架构。
*   **SVE (Scalable Vector Extension)**：允许向量长度可变，适应高性能计算需求（如富岳超算）。

### 2.1.3 SIMD编程实践

利用SIMD主要有三种方式：编译器自动向量化、使用编译器指令、以及手写内联函数（Intrinsics）。

#### 方式一：编译器自动向量化
现代编译器（如GCC, Clang, ICC）在开启优化选项（如 `-O3`, `-march=native`）时，会自动尝试向量化循环。

```cpp
void vector_add(float* a, float* b, float* c, int n) {
    // 编译器可能会将此循环转化为SIMD指令
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}
```

#### 方式二：使用OpenMP指令
显式告诉编译器进行向量化。

```cpp
void vector_add_omp(float* a, float* b, float* c, int n) {
    #pragma omp simd
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}
```

#### 方式三：SIMD Intrinsics (AVX2示例)
直接调用底层指令的C接口，性能最高但可移植性最差。

```cpp
#include <immintrin.h>

void avx2_add(float* a, float* b, float* c, int n) {
    for (int i = 0; i < n; i += 8) {
        // 加载数据到256位寄存器
        __m256 va = _mm256_loadu_ps(&a[i]);
        __m256 vb = _mm256_loadu_ps(&b[i]);
        
        // 执行并行加法
        __m256 vc = _mm256_add_ps(va, vb);
        
        // 存储结果
        _mm256_storeu_ps(&c[i], vc);
    }
}
```

### 2.1.4 优缺点分析

| 维度 | 优势 (Pros) | 局限 (Cons) |
| :--- | :--- | :--- |
| **性能** | 高吞吐量，单指令处理多数据，能效比高。 | 遇到分支跳转（if-else）时效率骤降（分支发散）。 |
| **内存** | 访存模式规则，利用率高。 | 对内存对齐有严格要求，非对齐访问可能导致性能损失或异常。 |
| **编程** | 逻辑相对简单，无需处理复杂的线程同步。 | 代码可移植性差（针对特定指令集），调试困难。 |

---

## 2.2 MIMD硬件架构分类

**MIMD (Multiple Instruction, Multiple Data)** 是最通用的并行模型，每个处理器可以独立执行不同的指令流。根据内存组织方式，MIMD硬件主要分为三类：

### 2.2.1 SMP (对称多处理器)
*   **特点**：所有处理器平等地连接到共享内存（UMA - Uniform Memory Access）。
*   **适用性**：桌面多核CPU、小型服务器。
*   **瓶颈**：随着处理器数量增加，内存总线成为瓶颈，扩展性有限。

### 2.2.2 NUMA (非统一内存访问)
*   **特点**：内存被分散到不同的节点。处理器访问本地节点内存很快，访问远程节点内存较慢。
*   **现状**：现代双路/四路服务器普遍采用NUMA架构。
*   **优化**：编程时需注意"数据局部性"，尽量让线程处理本地内存数据。

### 2.2.3 Cluster (集群)
*   **特点**：由独立计算机（节点）通过网络互连组成。每个节点有独立的内存和操作系统。
*   **优势**：极高的可扩展性，构建了当今最强大的超级计算机。

---

## 2.3 共享内存编程模型

基于SMP和NUMA硬件，**共享内存模型**允许线程通过读写共享变量进行交互。

### 2.3.1 核心机制
*   **通信**：隐式通信。线程A写入变量 `x`，线程B读取变量 `x`。
*   **同步**：必须使用锁、信号量或原子操作来协调访问，防止数据竞争。

### 2.3.2 常见工具：OpenMP
OpenMP是一种基于指令的API，是共享内存编程的事实标准。

```cpp
#include <omp.h>
#include <vector>

void openmp_example() {
    std::vector<int> data(1000, 1);
    int sum = 0;

    // 并行区域：自动将循环分配给多个线程
    // reduction(+:sum) 处理了归约操作的竞争问题
    #pragma omp parallel for reduction(+:sum)
    for (int i = 0; i < 1000; i++) {
        sum += data[i];
    }
}
```

### 2.3.3 常见挑战：竞态条件与死锁

#### 竞态条件 (Race Condition)
当多个线程同时修改同一变量且缺乏同步时发生。

```cpp
// 错误示例
int counter = 0;
void unsafe_inc() { counter++; } // ++操作不是原子的

// 修正：使用C++11原子类型
#include <atomic>
std::atomic<int> safe_counter(0);
void safe_inc() { safe_counter++; }
```

#### 死锁 (Deadlock)
两个线程互相等待对方释放资源。

*   **预防策略**：保持一致的加锁顺序；使用 `std::lock` 同时获取多个锁。

---

## 2.4 分布式内存编程模型

基于Cluster硬件，**分布式内存模型**要求显式地在进程间传递消息。

### 2.4.1 核心机制
*   **独立地址空间**：进程A无法直接访问进程B的变量。
*   **消息传递**：必须通过 `Send` 和 `Receive` 操作交换数据。
*   **标准**：MPI (Message Passing Interface) 是该领域的绝对标准。

### 2.4.2 MPI编程范式

#### 点对点通信 (Point-to-Point)
```cpp
#include <mpi.h>

void mpi_p2p_example() {
    int rank, size, data;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    if (rank == 0) {
        data = 42;
        // 发送数据给进程1
        MPI_Send(&data, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
    } else if (rank == 1) {
        // 从进程0接收数据
        MPI_Recv(&data, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
}
```

#### 集体通信 (Collective)
涉及通信域内所有进程的操作，如广播、归约。

```cpp
void mpi_collective_example() {
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    int local_val = rank;
    int global_sum = 0;

    // 所有进程的local_val相加，结果存入所有进程的global_sum
    MPI_Allreduce(&local_val, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
}
```

### 2.4.3 优缺点分析

| 特性 | 说明 |
| :--- | :--- |
| **可扩展性** | **极佳**。随节点增加，性能可线性增长（理想情况下）。 |
| **编程难度** | **高**。程序员需显式管理数据划分和通信逻辑。 |
| **通信开销** | 网络延迟远高于内存访问，需优化通信频率和数据量。 |

---

## 2.5 混合并行模型 (Hybrid Parallelism)

为了利用现代超级计算机的层次化结构（节点间分布式 + 节点内多核 + 加速器），混合编程成为主流。

### 2.5.1 MPI + OpenMP
*   **宏观**：节点间使用MPI通信。
*   **微观**：节点内使用OpenMP利用多核共享内存。
*   **优势**：减少了MPI进程数量，降低了通信拓扑的复杂度，节省内存。

### 2.5.2 CPU + Accelerator (GPU)
*   **架构**：CPU负责逻辑控制和IO，GPU负责大规模数据并行计算。
*   **编程**：CUDA (NVIDIA), ROCm (AMD), SYCL (跨平台)。

```cpp
// 异构计算概念示例
void hybrid_gpu_compute() {
    // 1. CPU准备数据
    prepare_data(host_array);
    
    // 2. 数据传输 CPU -> GPU
    cudaMemcpy(device_array, host_array, ...);
    
    // 3. GPU并行计算
    gpu_kernel<<<blocks, threads>>>(device_array);
    
    // 4. 结果回传 GPU -> CPU
    cudaMemcpy(host_array, device_array, ...);
}
```

## 2.6 本章小结与模型对比

| 特性 | SIMD | 共享内存 (OpenMP) | 分布式内存 (MPI) |
| :--- | :--- | :--- | :--- |
| **并行粒度** | 数据级（极细） | 线程级（中等） | 进程级（粗） |
| **地址空间** | 寄存器级 | 全局共享 | 局部独立 |
| **通信方式** | 寄存器/内存广播 | 读写共享变量 | 显式消息传递 |
| **同步机制** | 锁步指令 (隐式) | 锁、屏障、原子操作 | 消息阻塞、屏障 |
| **典型硬件** | Vector Units, GPU | SMP, NUMA Multi-core | Clusters, Supercomputers |

### 练习题

1.  **概念辨析**：在NUMA架构下编写OpenMP程序，为什么需要关注"首次接触策略" (First Touch Policy)？
2.  **代码改写**：将一个简单的矩阵向量乘法 (Matrix-Vector Multiplication) 分别用 OpenMP 和 MPI 实现，并比较其代码结构的差异。
3.  **性能分析**：为什么在某些情况下，MPI程序的性能反而优于等效的共享内存程序？（提示：考虑缓存一致性流量和NUMA效应）。