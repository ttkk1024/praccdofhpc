# 第十章：MPI 编程

## 目录
- [10.1 MPI 基础概念](#101-mpi-基础概念)
- [10.2 MPI 环境设置](#102-mpi-环境设置)
  - [10.2.1 安装 MPI](#1021-安装-mpi)
  - [10.2.2 编译与运行](#1022-编译与运行)
- [10.3 MPI 程序结构](#103-mpi-程序结构)
- [10.4 点对点通信 (Point-to-Point)](#104-点对点通信-point-to-point)
  - [10.4.1 阻塞通信](#1041-阻塞通信)
  - [10.4.2 非阻塞通信](#1042-非阻塞通信)
- [10.5 集体通信 (Collective)](#105-集体通信-collective)
  - [10.5.1 广播 (Broadcast)](#1051-广播-broadcast)
  - [10.5.2 散发与收集 (Scatter & Gather)](#1052-散发与收集-scatter--gather)
  - [10.5.3 规约 (Reduce)](#1053-规约-reduce)
- [10.6 MPI 数据类型](#106-mpi-数据类型)
- [10.7 进程拓扑与分组](#107-进程拓扑与分组)
- [10.8 MPI 性能优化策略](#108-mpi-性能优化策略)
- [10.9 MPI 在生物信息学中的应用](#109-mpi-在生物信息学中的应用)

---

## 10.1 MPI 基础概念

**消息传递接口 (Message Passing Interface, MPI)** 是分布式内存并行计算的事实标准。与共享内存模型（如 OpenMP）不同，MPI 进程拥有独立的内存空间，必须通过显式的消息传递来交换数据。

**核心术语**：
- **进程 (Process)**：并行执行的基本单元。通常一个物理核心对应一个 MPI 进程。
- **通信器 (Communicator)**：定义了一组可以互相通信的进程集合。默认的通信器是 `MPI_COMM_WORLD`，包含所有启动的进程。
- **排名 (Rank)**：进程在通信器中的唯一整数标识（从 0 到 N-1）。Rank 0 通常作为主进程（Master）负责协调。
- **消息 (Message)**：包含数据本体（Buffer）、数据类型（Datatype）和信封信息（Source, Destination, Tag, Communicator）。

---

## 10.2 MPI 环境设置

### 10.2.1 安装 MPI

在高性能计算（HPC）集群中，MPI 通常已经预装（如通过 module 加载）。在个人开发环境中，可以通过以下方式安装。

#### 1. 系统包管理器 (推荐)

**Ubuntu/Debian**:
```bash
sudo apt-get update
sudo apt-get install openmpi-bin openmpi-common libopenmpi-dev
```

**CentOS/RHEL**:
```bash
sudo yum install openmpi openmpi-devel
module load mpi/openmpi-x86_64  # 加载环境变量
```

**macOS (Homebrew)**:
```bash
brew install open-mpi
```

#### 2. Conda 环境 (生物信息学常用)

```bash
conda install -c conda-forge openmpi mpi4py
```

### 10.2.2 编译与运行

MPI 提供了封装好的编译器包装器（Wrapper Compilers），自动处理头文件和库路径。

**编译**：
```bash
# C 程序
mpicc -o hello hello.c

# C++ 程序
mpicxx -o hello hello.cpp

# Fortran 程序
mpif90 -o hello hello.f90
```

**运行**：
使用 `mpirun` 或 `mpiexec` 启动并行程序。

```bash
# 在本地启动 4 个进程
mpirun -np 4 ./hello

# 使用主机文件 (Hostfile) 在多节点运行
# hosts.txt 内容:
# node01 slots=16
# node02 slots=16
mpirun -np 32 --hostfile hosts.txt ./hello
```

---

## 10.3 MPI 程序结构

一个标准的 MPI 程序包含初始化、核心计算逻辑和终结化三个部分。

```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // 1. 初始化 MPI 环境
    // 必须在调用任何其他 MPI 函数之前调用
    MPI_Init(&argc, &argv);

    // 2. 获取进程信息
    int world_size, world_rank;
    // 获取总进程数
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    // 获取当前进程的 ID (Rank)
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // 3. 获取处理器名称 (主机名)
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    // 4. 并行逻辑
    printf("Hello from processor %s, rank %d out of %d processors\n",
           processor_name, world_rank, world_size);

    // 5. 终结 MPI 环境
    // 清理资源，之后不能再调用 MPI 函数
    MPI_Finalize();
    return 0;
}
```

---

## 10.4 点对点通信 (Point-to-Point)

点对点通信涉及两个特定进程：发送者（Sender）和接收者（Receiver）。

### 10.4.1 阻塞通信

**阻塞 (Blocking)** 意味着函数调用在操作完成之前不会返回。
- `MPI_Send`：直到发送缓冲区可以安全重用（数据已复制到系统缓冲区或已发送）才返回。
- `MPI_Recv`：直到数据完全接收到应用缓冲区才返回。

```c
// 原型
int MPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm);
int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status);

// 示例：Rank 0 发送数据给 Rank 1

int number;
if (world_rank == 0) {
    number = 42;
    // 发送给 rank 1，标签为 0
    MPI_Send(&number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
    printf("Process 0 sent number %d to process 1\n", number);
} else if (world_rank == 1) {
    // 从 rank 0 接收，标签为 0
    MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    printf("Process 1 received number %d from process 0\n", number);
}
```

### 10.4.2 非阻塞通信

**非阻塞 (Non-blocking)** 调用会立即返回一个请求句柄（Request Handle），允许计算和通信重叠（Overlapping）。必须显式调用等待函数来确保操作完成。

- `MPI_Isend` / `MPI_Irecv`：发起操作。
- `MPI_Wait` / `MPI_Test`：等待或检查操作完成。

**示例：非阻塞环形通信**

```c
int token = 100;
MPI_Request request;
MPI_Status status;

int right_neighbor = (world_rank + 1) % world_size;
int left_neighbor = (world_rank - 1 + world_size) % world_size;

// 1. 发起非阻塞接收
MPI_Irecv(&token, 1, MPI_INT, left_neighbor, 0, MPI_COMM_WORLD, &request);

// 2. 发起阻塞发送 (或非阻塞发送)
MPI_Send(&token, 1, MPI_INT, right_neighbor, 0, MPI_COMM_WORLD);

// 3. 在等待接收的同时，可以做一些不依赖 token 的计算
// do_some_computation();

// 4. 等待接收完成
MPI_Wait(&request, &status);
```

---

## 10.5 集体通信 (Collective)

集体通信必须由通信器中的**所有**进程调用。它们通常是阻塞的。

### 10.5.1 广播 (Broadcast)

将数据从根进程（Root）复制到所有其他进程。

```c
// 原型
int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm);

// 示例
int data;
if (world_rank == 0) data = 100;
// 所有进程都必须调用 Bcast
MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);
// 现在所有进程的 data 都是 100
```

### 10.5.2 散发与收集 (Scatter & Gather)

- **Scatter**：将根进程的一个大数组切分，分发给所有进程。
- **Gather**：将所有进程的数据收集到根进程的一个大数组中。

```c
// 示例：分发数组处理后收集
#define ARRAY_SIZE 100

int local_size = ARRAY_SIZE / world_size;
float *local_buf = (float*)malloc(sizeof(float) * local_size);
float *global_buf = NULL;

if (world_rank == 0) {
    global_buf = (float*)malloc(sizeof(float) * ARRAY_SIZE);
    // 初始化 global_buf ...
}

// 1. 分发数据
MPI_Scatter(global_buf, local_size, MPI_FLOAT, 
            local_buf, local_size, MPI_FLOAT, 
            0, MPI_COMM_WORLD);

// 2. 本地计算
for(int i=0; i<local_size; i++) {
    local_buf[i] = sqrt(local_buf[i]);
}

// 3. 收集结果
MPI_Gather(local_buf, local_size, MPI_FLOAT, 
           global_buf, local_size, MPI_FLOAT, 
           0, MPI_COMM_WORLD);

if (world_rank == 0) free(global_buf);
free(local_buf);
```

### 10.5.3 规约 (Reduce)

将所有进程的数据进行某种运算（求和、最大值、逻辑与等），结果存放在根进程。

```c
// 原型
int MPI_Reduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);

// 常见操作(MPI_Op): MPI_SUM, MPI_MAX, MPI_MIN, MPI_PROD

// 示例：并行求PI
double local_pi, global_pi;
// ... 计算 local_pi ...

// 汇总结果到 rank 0
MPI_Reduce(&local_pi, &global_pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

// 如果所有进程都需要结果，使用 MPI_Allreduce
MPI_Allreduce(&local_pi, &global_pi, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
```

---

## 10.6 MPI 数据类型

MPI 通信需要指定数据类型以处理不同架构间的大小和字节序差异。

**基本类型**：
- `MPI_CHAR`, `MPI_INT`, `MPI_FLOAT`, `MPI_DOUBLE` 等。

**自定义结构体 (Derived Datatype)**：
如果需要发送复杂的 C 结构体，必须定义对应的 MPI 类型。

```c
typedef struct {
    int id;
    double x, y, z;
} Particle;

// 创建 MPI 结构体类型
int blocklengths[2] = {1, 3}; // id(1个int), coords(3个double)
MPI_Datatype types[2] = {MPI_INT, MPI_DOUBLE};
MPI_Aint offsets[2];

offsets[0] = offsetof(Particle, id);
offsets[1] = offsetof(Particle, x); // 假设 x,y,z 是连续的

MPI_Datatype mpi_particle_type;
MPI_Type_create_struct(2, blocklengths, offsets, types, &mpi_particle_type);
MPI_Type_commit(&mpi_particle_type);

// 使用
MPI_Send(&p, 1, mpi_particle_type, dest, tag, MPI_COMM_WORLD);

// 清理
MPI_Type_free(&mpi_particle_type);
```

---

## 10.7 进程拓扑与分组

MPI 允许将线性 Rank 映射为虚拟拓扑（如 2D/3D 网格），这在物理模拟中非常有用。

```c
// 创建 2D 笛卡尔拓扑
int dims[2] = {4, 4}; // 4x4 网格
int periods[2] = {0, 0}; // 非周期边界
int reorder = 1; // 允许重排 Rank 以优化物理布局
MPI_Comm comm_cart;

MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &comm_cart);

// 获取邻居 Rank
int up, down;
MPI_Cart_shift(comm_cart, 0, 1, &up, &down); // 维度 0，位移 1
```

---

## 10.8 MPI 性能优化策略

1.  **通信聚合 (Aggregation)**：将许多小消息合并为一个大消息发送，以减少延迟（Latency）开销。
2.  **重叠通信与计算 (Overlapping)**：使用 `MPI_Isend`/`MPI_Irecv`，在数据传输的同时利用 CPU 进行计算。
3.  **避免全局同步**：尽量减少 `MPI_Barrier` 的使用，大多数集体通信自带隐式同步。
4.  **利用共享内存**：在同一节点内的进程间通信（Intra-node），MPI 实现通常会自动优化为共享内存拷贝，但混合编程（MPI+OpenMP）往往能进一步减少内存占用。
5.  **数据局部性**：利用虚拟拓扑（Cartesian Topology）确保通信主要发生在逻辑邻居之间。

---

## 10.9 MPI 在生物信息学中的应用

### 10.9.1 案例：并行序列搜索 (简易 BLAST)

这是一个典型的主从模式（Master-Slave）应用。主进程负责读取 Query 序列并分发数据库片段，从进程执行比对。

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

// 模拟序列比对函数
int align_sequence(const char* query, const char* subject) {
    // 实际应用中会是 Smith-Waterman 或 BLAST 算法
    // 这里仅做长度模拟
    return abs((int)strlen(query) - (int)strlen(subject)); 
}

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    char query[100];
    if (rank == 0) {
        strcpy(query, "ATCGGCTA"); // 主进程读取 Query
    }

    // 1. 广播 Query 序列给所有 Worker
    // 先广播长度（简单起见省略），直接广播固定缓冲区
    MPI_Bcast(query, 100, MPI_CHAR, 0, MPI_COMM_WORLD);

    // 模拟数据库：假设总共有 1000 条序列，每个进程处理一部分
    int total_sequences = 1000;
    int seqs_per_process = total_sequences / size;
    
    // 实际应用中，Worker 可能会根据 Rank 并行读取文件的不同偏移量(MPI-IO)
    int start_idx = rank * seqs_per_process;
    int end_idx = start_idx + seqs_per_process;
    
    int local_best_score = -1;
    
    // 2. 并行处理
    for (int i = start_idx; i < end_idx; i++) {
        // char* subject = load_sequence_from_db(i);
        char subject[] = "ATCG..."; // 模拟数据
        int score = align_sequence(query, subject);
        if (score > local_best_score) {
            local_best_score = score;
        }
    }

    // 3. 规约所有进程的最佳分数
    int global_best_score;
    MPI_Reduce(&local_best_score, &global_best_score, 1, MPI_INT, 
               MPI_MAX, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Global best alignment score: %d\n", global_best_score);
    }

    MPI_Finalize();
    return 0;
}
```

### 10.9.2 案例：K-mer 计数 (基因组组装预处理)

使用 `MPI_Alltoall` 进行 K-mer 混洗（Shuffling），将相同的 K-mer 发送到同一个 Rank 进行聚合计数。

**逻辑流程**：
1.  **本地计数**：每个进程读取部分 Reads，生成 K-mer。
2.  **哈希分桶**：根据 `hash(kmer) % world_size` 决定 K-mer 的归属进程。
3.  **全局交换**：使用 `MPI_Alltoallv` 交换 K-mer 数据。
4.  **聚合**：接收归属于自己的 K-mer，进行最终计数。

这种模式是 MapReduce 在 HPC 环境下的典型实现，能够高效处理 TB 级的基因组数据。

```
