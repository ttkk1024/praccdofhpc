# 第七章 基因组分析

## 7.1 序列比对并行化

### 7.1.1 BLAST并行化

#### BLAST算法概述

BLAST（Basic Local Alignment Search Tool）是生物信息学中最常用的序列比对工具，用于在数据库中搜索相似的序列。

**BLAST算法步骤**：
1. **种子生成**：识别查询序列中的短片段（k-mers）
2. **数据库搜索**：在数据库中查找匹配的种子
3. **扩展比对**：从种子位置向两端扩展比对
4. **打分评估**：计算比对得分和统计显著性

#### 并行BLAST搜索实现

```python
# 并行BLAST搜索实现
import multiprocessing
import subprocess
import os
import tempfile
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import pandas as pd
from Bio import SeqIO
import numpy as np

class ParallelBLAST:
    def __init__(self, database_path, num_processes=None, evalue_threshold=1e-5):
        """
        初始化并行BLAST搜索器

        Args:
            database_path: BLAST数据库路径
            num_processes: 并行进程数
            evalue_threshold: E值阈值
        """
        self.database_path = database_path
        self.num_processes = num_processes or multiprocessing.cpu_count()
        self.evalue_threshold = evalue_threshold
        self.temp_dir = tempfile.mkdtemp()

    def split_query_file(self, query_file, chunk_size=1000):
        """将查询文件分割为多个小文件"""
        sequences = list(SeqIO.parse(query_file, "fasta"))

        # 计算每个块的大小
        total_sequences = len(sequences)
        chunks = []

        for i in range(0, total_sequences, chunk_size):
            chunk_sequences = sequences[i:i + chunk_size]
            if chunk_sequences:
                chunk_file = os.path.join(self.temp_dir, f"chunk_{i//chunk_size}.fasta")
                with open(chunk_file, "w") as f:
                    SeqIO.write(chunk_sequences, f, "fasta")
                chunks.append(chunk_file)

        return chunks

    def run_blast_chunk(self, chunk_file):
        """运行单个BLAST块"""
        output_file = chunk_file.replace(".fasta", ".blast.out")

        # 构建BLAST命令
        cmd = [
            "blastn",
            "-query", chunk_file,
            "-db", self.database_path,
            "-out", output_file,
            "-evalue", str(self.evalue_threshold),
            "-outfmt", "6",  # tabular格式
            "-num_threads", "1",  # 每个进程使用单线程
            "-max_target_seqs", "10"  # 限制每个查询的命中数
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
            if result.returncode == 0:
                return output_file
            else:
                print(f"BLAST failed for {chunk_file}: {result.stderr}")
                return None
        except subprocess.TimeoutExpired:
            print(f"BLAST timeout for {chunk_file}")
            return None

    def parallel_blast_search(self, query_file):
        """并行BLAST搜索主函数"""
        print(f"开始并行BLAST搜索，使用 {self.num_processes} 个进程")

        # 分割查询文件
        print("分割查询文件...")
        chunk_files = self.split_query_file(query_file)

        # 并行执行BLAST
        print(f"执行 {len(chunk_files)} 个BLAST任务...")
        results = []

        with ProcessPoolExecutor(max_workers=self.num_processes) as executor:
            # 提交所有任务
            future_to_chunk = {
                executor.submit(self.run_blast_chunk, chunk_file): chunk_file
                for chunk_file in chunk_files
            }

            # 收集结果
            for future in future_to_chunk:
                result = future.result()
                if result:
                    results.append(result)

        # 合并结果
        print("合并结果...")
        final_output = os.path.join(self.temp_dir, "parallel_blast_results.tab")
        self.merge_blast_results(results, final_output)

        # 清理临时文件
        self.cleanup_temp_files()

        return final_output

    def merge_blast_results(self, result_files, output_file):
        """合并BLAST结果"""
        all_results = []

        for result_file in result_files:
            if os.path.exists(result_file):
                try:
                    df = pd.read_csv(result_file, sep='\t', header=None,
                                   names=['query', 'subject', 'identity', 'alignment_length',
                                        'mismatches', 'gap_openings', 'q_start', 'q_end',
                                        's_start', 's_end', 'evalue', 'bitscore'])
                    all_results.append(df)
                except Exception as e:
                    print(f"读取结果文件失败: {result_file}, 错误: {e}")

        if all_results:
            merged_df = pd.concat(all_results, ignore_index=True)
            merged_df = merged_df.sort_values('evalue')
            merged_df.to_csv(output_file, sep='\t', index=False)
            print(f"合并了 {len(merged_df)} 个比对结果")
        else:
            print("没有找到有效的BLAST结果")

    def cleanup_temp_files(self):
        """清理临时文件"""
        import shutil
        try:
            shutil.rmtree(self.temp_dir)
            print("临时文件已清理")
        except Exception as e:
            print(f"清理临时文件失败: {e}")

    def get_top_hits(self, blast_result_file, top_n=100):
        """获取前N个最佳命中"""
        try:
            df = pd.read_csv(blast_result_file, sep='\t')
            top_hits = df.head(top_n)
            return top_hits
        except Exception as e:
            print(f"读取BLAST结果失败: {e}")
            return None

# 使用示例
def parallel_blast_example():
    # 初始化并行BLAST
    blast_searcher = ParallelBLAST(
        database_path="/path/to/nt_database",
        num_processes=8,
        evalue_threshold=1e-10
    )

    # 执行并行搜索
    query_file = "query_sequences.fasta"
    result_file = blast_searcher.parallel_blast_search(query_file)

    # 获取最佳命中
    top_hits = blast_searcher.get_top_hits(result_file, top_n=50)
    if top_hits is not None:
        print("最佳命中结果:")
        print(top_hits[['query', 'subject', 'identity', 'evalue', 'bitscore']].head(10))

if __name__ == "__main__":
    parallel_blast_example()
```

#### BLAST性能优化策略

```python
class BLASTOptimizer:
    """BLAST性能优化器"""

    @staticmethod
    def optimize_blast_parameters(sequence_length):
        """根据序列长度优化BLAST参数"""
        if sequence_length < 100:
            return {
                'word_size': 7,
                'evalue': 1e-5,
                'max_target_seqs': 100
            }
        elif sequence_length < 1000:
            return {
                'word_size': 11,
                'evalue': 1e-10,
                'max_target_seqs': 50
            }
        else:
            return {
                'word_size': 16,
                'evalue': 1e-15,
                'max_target_seqs': 20
            }

    @staticmethod
    def adaptive_chunking(sequences, target_runtime=600):
        """自适应分块策略"""
        # 估算每个序列的处理时间
        avg_processing_time = 0.1  # 秒/序列（根据实际情况调整）

        # 计算目标块大小
        total_sequences = len(sequences)
        estimated_total_time = total_sequences * avg_processing_time
        scaling_factor = target_runtime / estimated_total_time

        # 调整块大小
        base_chunk_size = 1000
        adjusted_chunk_size = max(100, int(base_chunk_size * scaling_factor))

        return adjusted_chunk_size

    @staticmethod
    def database_indexing_optimization(database_path):
        """数据库索引优化"""
        # 创建BLAST数据库索引
        cmd = [
            "makeblastdb",
            "-in", database_path,
            "-dbtype", "nucl",
            "-parse_seqids",
            "-index", "true"
        ]
        subprocess.run(cmd, check=True)
```

### 7.1.2 Smith-Waterman算法并行化

#### Smith-Waterman算法原理

Smith-Waterman算法是一种动态规划算法，用于进行局部序列比对，能够找到两个序列中相似性最高的区域。

**算法特点**：
- 全局最优：保证找到最佳局部比对
- 时间复杂度：O(m×n)
- 空间复杂度：O(m×n)

#### 并行Smith-Waterman实现

```cpp
// 并行Smith-Waterman算法（OpenMP实现）
#include <omp.h>
#include <vector>
#include <algorithm>
#include <climits>

class ParallelSmithWaterman {
private:
    static const int MATCH_SCORE = 2;
    static const int MISMATCH_SCORE = -1;
    static const int GAP_PENALTY = -1;

    std::vector<std::vector<int>> score_matrix;
    std::vector<std::vector<int>> traceback_matrix;

public:
    struct AlignmentResult {
        int score;
        int start1, end1;
        int start2, end2;
        std::string alignment1;
        std::string alignment2;
    };

    AlignmentResult align_sequences(const std::string& seq1,
                                  const std::string& seq2) {
        int len1 = seq1.length();
        int len2 = seq2.length();

        // 初始化矩阵
        score_matrix.resize(len1 + 1, std::vector<int>(len2 + 1, 0));
        traceback_matrix.resize(len1 + 1, std::vector<int>(len2 + 1, 0));

        // 并行填充得分矩阵
        #pragma omp parallel for collapse(2)
        for (int i = 1; i <= len1; i++) {
            for (int j = 1; j <= len2; j++) {
                // 计算匹配/错配得分
                int match_score = (seq1[i-1] == seq2[j-1]) ? MATCH_SCORE : MISMATCH_SCORE;

                // 三个可能的来源
                int score_diagonal = score_matrix[i-1][j-1] + match_score;
                int score_up = score_matrix[i-1][j] + GAP_PENALTY;
                int score_left = score_matrix[i][j-1] + GAP_PENALTY;

                // 选择最大得分（允许为0）
                int max_score = std::max({0, score_diagonal, score_up, score_left});
                score_matrix[i][j] = max_score;

                // 记录回溯方向
                if (max_score == 0) {
                    traceback_matrix[i][j] = 0;  // 终止
                } else if (max_score == score_diagonal) {
                    traceback_matrix[i][j] = 1;  // 对角线
                } else if (max_score == score_up) {
                    traceback_matrix[i][j] = 2;  // 向上
                } else {
                    traceback_matrix[i][j] = 3;  // 向左
                }
            }
        }

        // 找到最高分位置
        int max_score = 0;
        int max_i = 0, max_j = 0;

        #pragma omp parallel for collapse(2) reduction(max:max_score)
        for (int i = 1; i <= len1; i++) {
            for (int j = 1; j <= len2; j++) {
                if (score_matrix[i][j] > max_score) {
                    max_score = score_matrix[i][j];
                    max_i = i;
                    max_j = j;
                }
            }
        }

        // 回溯构建比对
        return traceback_alignment(seq1, seq2, max_i, max_j, max_score);
    }

private:
    AlignmentResult traceback_alignment(const std::string& seq1,
                                      const std::string& seq2,
                                      int start_i, int start_j,
                                      int max_score) {
        AlignmentResult result;
        result.score = max_score;

        std::string align1, align2;
        int i = start_i, j = start_j;

        // 回溯
        while (i > 0 && j > 0 && traceback_matrix[i][j] != 0) {
            int direction = traceback_matrix[i][j];

            if (direction == 1) {  // 对角线
                align1 = seq1[i-1] + align1;
                align2 = seq2[j-1] + align2;
                i--; j--;
            } else if (direction == 2) {  // 向上
                align1 = seq1[i-1] + align1;
                align2 = "-" + align2;
                i--;
            } else {  // 向左
                align1 = "-" + align1;
                align2 = seq2[j-1] + align2;
                j--;
            }
        }

        result.alignment1 = align1;
        result.alignment2 = align2;
        result.start1 = i + 1;
        result.end1 = start_i;
        result.start2 = j + 1;
        result.end2 = start_j;

        return result;
    }
};

// GPU加速的Smith-Waterman（CUDA实现）
__global__ void smith_waterman_kernel(
    char* seq1, char* seq2,
    int* score_matrix,
    int* traceback_matrix,
    int len1, int len2,
    int match_score, int mismatch_score, int gap_penalty) {

    int i = blockIdx.y * blockDim.y + threadIdx.y + 1;
    int j = blockIdx.x * blockDim.x + threadIdx.x + 1;

    if (i <= len1 && j <= len2) {
        // 计算索引
        int idx = i * (len2 + 1) + j;

        // 计算匹配/错配得分
        int match = (seq1[i-1] == seq2[j-1]) ? match_score : mismatch_score;

        // 获取相邻位置的得分
        int score_diagonal = score_matrix[(i-1) * (len2 + 1) + (j-1)] + match;
        int score_up = score_matrix[(i-1) * (len2 + 1) + j] + gap_penalty;
        int score_left = score_matrix[i * (len2 + 1) + (j-1)] + gap_penalty;

        // 计算最大得分
        int max_score = max(0, max(score_diagonal, max(score_up, score_left)));
        score_matrix[idx] = max_score;

        // 记录回溯方向
        if (max_score == 0) {
            traceback_matrix[idx] = 0;
        } else if (max_score == score_diagonal) {
            traceback_matrix[idx] = 1;
        } else if (max_score == score_up) {
            traceback_matrix[idx] = 2;
        } else {
            traceback_matrix[idx] = 3;
        }
    }
}

class CUDASmithWaterman {
public:
    AlignmentResult align_sequences(const std::string& seq1,
                                  const std::string& seq2) {
        int len1 = seq1.length();
        int len2 = seq2.length();

        // 分配GPU内存
        char *d_seq1, *d_seq2;
        int *d_score_matrix, *d_traceback_matrix;

        cudaMalloc(&d_seq1, len1 * sizeof(char));
        cudaMalloc(&d_seq2, len2 * sizeof(char));
        cudaMalloc(&d_score_matrix, (len1 + 1) * (len2 + 1) * sizeof(int));
        cudaMalloc(&d_traceback_matrix, (len1 + 1) * (len2 + 1) * sizeof(int));

        // 拷贝数据到GPU
        cudaMemcpy(d_seq1, seq1.c_str(), len1 * sizeof(char), cudaMemcpyHostToDevice);
        cudaMemcpy(d_seq2, seq2.c_str(), len2 * sizeof(char), cudaMemcpyHostToDevice);

        // 初始化矩阵
        cudaMemset(d_score_matrix, 0, (len1 + 1) * (len2 + 1) * sizeof(int));
        cudaMemset(d_traceback_matrix, 0, (len1 + 1) * (len2 + 1) * sizeof(int));

        // 配置CUDA执行参数
        dim3 blockSize(16, 16);
        dim3 gridSize((len2 + 15) / 16, (len1 + 15) / 16);

        // 执行CUDA核函数
        smith_waterman_kernel<<<gridSize, blockSize>>>(
            d_seq1, d_seq2, d_score_matrix, d_traceback_matrix,
            len1, len2, 2, -1, -1
        );

        cudaDeviceSynchronize();

        // 查找最大得分
        int max_score;
        find_max_score<<<1, 1>>>(d_score_matrix, &max_score, (len1 + 1) * (len2 + 1));

        // 拷贝结果回主机
        std::vector<int> h_score_matrix((len1 + 1) * (len2 + 1));
        cudaMemcpy(h_score_matrix.data(), d_score_matrix,
                   (len1 + 1) * (len2 + 1) * sizeof(int), cudaMemcpyDeviceToHost);

        // 清理GPU内存
        cudaFree(d_seq1);
        cudaFree(d_seq2);
        cudaFree(d_score_matrix);
        cudaFree(d_traceback_matrix);

        return AlignmentResult{max_score, 0, 0, 0, 0, "", ""};
    }
};
```

### 7.1.3 序列数据库索引

#### k-mer索引构建

```python
# k-mer索引构建和快速查找
import numpy as np
from collections import defaultdict
import pickle
import os

class KMerIndex:
    def __init__(self, k=25):
        """
        初始化k-mer索引

        Args:
            k: k-mer长度
        """
        self.k = k
        self.kmer_to_positions = defaultdict(list)
        self.sequence_data = {}
        self.total_sequences = 0

    def build_index_from_fasta(self, fasta_file, max_sequences=None):
        """从FASTA文件构建k-mer索引"""
        print(f"构建k-mer索引 (k={self.k})...")

        sequence_count = 0
        for record in SeqIO.parse(fasta_file, "fasta"):
            if max_sequences and sequence_count >= max_sequences:
                break

            sequence_id = record.id
            sequence = str(record.seq).upper()

            # 存储序列信息
            self.sequence_data[sequence_id] = {
                'sequence': sequence,
                'length': len(sequence)
            }

            # 构建k-mer索引
            for i in range(len(sequence) - self.k + 1):
                kmer = sequence[i:i + self.k]
                if self._is_valid_kmer(kmer):
                    self.kmer_to_positions[kmer].append((sequence_id, i))

            sequence_count += 1

            if sequence_count % 1000 == 0:
                print(f"已处理 {sequence_count} 个序列")

        self.total_sequences = sequence_count
        print(f"k-mer索引构建完成，共处理 {self.total_sequences} 个序列")
        print(f"索引包含 {len(self.kmer_to_positions)} 个不同的k-mer")

    def _is_valid_kmer(self, kmer):
        """检查k-mer是否有效（不含N）"""
        return 'N' not in kmer and len(kmer) == self.k

    def search_kmer(self, query_kmer):
        """搜索特定的k-mer"""
        return self.kmer_to_positions.get(query_kmer, [])

    def search_sequence(self, query_sequence, min_matches=3):
        """搜索查询序列"""
        query_kmers = self._extract_kmers(query_sequence)

        # 统计每个数据库序列的匹配次数
        sequence_matches = defaultdict(int)

        for kmer in query_kmers:
            positions = self.search_kmer(kmer)
            for seq_id, pos in positions:
                sequence_matches[seq_id] += 1

        # 过滤结果
        results = []
        for seq_id, matches in sequence_matches.items():
            if matches >= min_matches:
                results.append({
                    'sequence_id': seq_id,
                    'matches': matches,
                    'match_percentage': matches / len(query_kmers) * 100
                })

        # 按匹配数排序
        results.sort(key=lambda x: x['matches'], reverse=True)
        return results

    def _extract_kmers(self, sequence):
        """从序列中提取k-mer"""
        kmers = set()
        for i in range(len(sequence) - self.k + 1):
            kmer = sequence[i:i + self.k]
            if self._is_valid_kmer(kmer):
                kmers.add(kmer)
        return list(kmers)

    def save_index(self, filename):
        """保存索引到文件"""
        data = {
            'k': self.k,
            'kmer_to_positions': dict(self.kmer_to_positions),
            'sequence_data': self.sequence_data,
            'total_sequences': self.total_sequences
        }

        with open(filename, 'wb') as f:
            pickle.dump(data, f)

        print(f"索引已保存到 {filename}")

    def load_index(self, filename):
        """从文件加载索引"""
        with open(filename, 'rb') as f:
            data = pickle.load(f)

        self.k = data['k']
        self.kmer_to_positions = defaultdict(list, data['kmer_to_positions'])
        self.sequence_data = data['sequence_data']
        self.total_sequences = data['total_sequences']

        print(f"索引已从 {filename} 加载")

    def get_kmer_statistics(self):
        """获取k-mer统计信息"""
        kmer_counts = [len(positions) for positions in self.kmer_to_positions.values()]
        return {
            'total_kmers': len(self.kmer_to_positions),
            'max_occurrences': max(kmer_counts) if kmer_counts else 0,
            'min_occurrences': min(kmer_counts) if kmer_counts else 0,
            'avg_occurrences': np.mean(kmer_counts) if kmer_counts else 0,
            'unique_kmers': sum(1 for count in kmer_counts if count == 1)
        }

# 使用示例
def kmer_index_example():
    # 构建索引
    index = KMerIndex(k=25)

    # 从FASTA文件构建索引
    index.build_index_from_fasta("database_sequences.fasta", max_sequences=10000)

    # 保存索引
    index.save_index("kmer_index.pkl")

    # 加载索引
    # index.load_index("kmer_index.pkl")

    # 统计信息
    stats = index.get_kmer_statistics()
    print("k-mer统计信息:", stats)

    # 搜索示例
    query_seq = "ATCGATCGATCGATCGATCGATCGATCG"
    results = index.search_sequence(query_seq, min_matches=5)

    print(f"找到 {len(results)} 个匹配序列:")
    for result in results[:10]:
        print(f"  {result['sequence_id']}: {result['matches']} 匹配 ({result['match_percentage']:.2f}%)")

if __name__ == "__main__":
    kmer_index_example()
```

#### 布隆过滤器优化

```python
# 布隆过滤器用于快速k-mer存在性检查
import mmh3
import math
from bitarray import bitarray

class BloomFilter:
    def __init__(self, capacity, error_rate=0.001):
        """
        初始化布隆过滤器

        Args:
            capacity: 预期插入的元素数量
            error_rate: 期望的误判率
        """
        self.capacity = capacity
        self.error_rate = error_rate

        # 计算位数组大小和哈希函数数量
        self.num_bits = self._calculate_bits(capacity, error_rate)
        self.num_hash_functions = self._calculate_hash_functions(self.num_bits, capacity)

        # 初始化位数组
        self.bit_array = bitarray(self.num_bits)
        self.bit_array.setall(0)

    def _calculate_bits(self, n, p):
        """计算位数组大小"""
        return int(-n * math.log(p) / (math.log(2) ** 2))

    def _calculate_hash_functions(self, m, n):
        """计算哈希函数数量"""
        return int(m / n * math.log(2))

    def add(self, item):
        """添加元素到布隆过滤器"""
        for i in range(self.num_hash_functions):
            # 使用不同的种子生成多个哈希值
            hash_value = mmh3.hash(str(item), i) % self.num_bits
            self.bit_array[hash_value] = 1

    def contains(self, item):
        """检查元素是否可能在布隆过滤器中"""
        for i in range(self.num_hash_functions):
            hash_value = mmh3.hash(str(item), i) % self.num_bits
            if self.bit_array[hash_value] == 0:
                return False
        return True

class OptimizedKMerIndex:
    def __init__(self, k=25):
        self.k = k
        self.kmer_bloom_filter = BloomFilter(capacity=1000000, error_rate=0.001)
        self.kmer_positions = {}

    def build_optimized_index(self, sequences):
        """构建优化的k-mer索引"""
        print("构建优化的k-mer索引...")

        for seq_id, sequence in sequences.items():
            for i in range(len(sequence) - self.k + 1):
                kmer = sequence[i:i + self.k]
                if self._is_valid_kmer(kmer):
                    # 先用布隆过滤器快速检查
                    if not self.kmer_bloom_filter.contains(kmer):
                        self.kmer_bloom_filter.add(kmer)

                    # 存储位置信息
                    if kmer not in self.kmer_positions:
                        self.kmer_positions[kmer] = []
                    self.kmer_positions[kmer].append((seq_id, i))

        print(f"优化索引构建完成，包含 {len(self.kmer_positions)} 个k-mer")

    def fast_search(self, query_kmer):
        """快速搜索k-mer"""
        # 先用布隆过滤器快速检查
        if not self.kmer_bloom_filter.contains(query_kmer):
            return []  # 肯定不存在

        # 布隆过滤器说可能存在，再检查实际索引
        return self.kmer_positions.get(query_kmer, [])

    def _is_valid_kmer(self, kmer):
        return 'N' not in kmer and len(kmer) == self.k
```

## 7.2 基因组装优化

### 7.2.1 De Bruijn图并行构建

#### De Bruijn图理论

De Bruijn图是基因组组装的核心数据结构，将序列分解为k-mers作为节点，重叠的k-mers之间建立边。

**图构建步骤**：
1. **k-mer提取**：从测序读段中提取所有k-mers
2. **图构建**：建立k-mer之间的连接关系
3. **图简化**：去除错误和冗余
4. **路径搜索**：寻找欧拉路径

#### 并行De Bruijn图构建

```python
# 并行De Bruijn图构建
import multiprocessing as mp
from collections import defaultdict, Counter
import networkx as nx
from concurrent.futures import ProcessPoolExecutor
import pickle

class ParallelDeBruijnGraph:
    def __init__(self, k=31, min_coverage=3):
        """
        初始化并行De Bruijn图

        Args:
            k: k-mer长度
            min_coverage: 最小覆盖度阈值
        """
        self.k = k
        self.min_coverage = min_coverage
        self.graph = nx.DiGraph()
        self.kmer_counts = Counter()

    def extract_kmers_from_reads(self, reads_chunk):
        """从读段块中提取k-mers"""
        local_kmers = Counter()

        for read in reads_chunk:
            read = read.upper()
            for i in range(len(read) - self.k + 1):
                kmer = read[i:i + self.k]
                if self._is_valid_kmer(kmer):
                    local_kmers[kmer] += 1

        return local_kmers

    def _is_valid_kmer(self, kmer):
        """检查k-mer是否有效"""
        return all(base in 'ACGT' for base in kmer) and len(kmer) == self.k

    def parallel_kmer_counting(self, reads, num_processes=None):
        """并行k-mer计数"""
        if num_processes is None:
            num_processes = mp.cpu_count()

        # 分割读段
        chunk_size = len(reads) // num_processes
        chunks = [
            reads[i:i + chunk_size]
            for i in range(0, len(reads), chunk_size)
        ]

        # 并行处理
        print(f"使用 {num_processes} 个进程进行k-mer计数...")
        with ProcessPoolExecutor(max_workers=num_processes) as executor:
            results = list(executor.map(self.extract_kmers_from_reads, chunks))

        # 合并结果
        for local_kmers in results:
            self.kmer_counts.update(local_kmers)

        print(f"k-mer计数完成，共找到 {len(self.kmer_counts)} 个不同的k-mer")

    def build_graph_from_kmers(self):
        """从k-mers构建De Bruijn图"""
        print("构建De Bruijn图...")

        # 过滤低覆盖度的k-mers
        filtered_kmers = {
            kmer: count for kmer, count in self.kmer_counts.items()
            if count >= self.min_coverage
        }

        print(f"过滤后剩余 {len(filtered_kmers)} 个k-mer")

        # 构建图
        nodes_added = set()

        for kmer in filtered_kmers:
            if kmer not in nodes_added:
                self.graph.add_node(kmer, coverage=filtered_kmers[kmer])
                nodes_added.add(kmer)

            # 添加边：kmer -> kmer的后缀
            suffix = kmer[1:]
            prefix = kmer[:-1]

            # 寻找可能的邻居
            for base in 'ACGT':
                next_kmer = suffix + base
                prev_kmer = base + prefix

                if next_kmer in filtered_kmers and next_kmer not in nodes_added:
                    self.graph.add_node(next_kmer, coverage=filtered_kmers[next_kmer])
                    nodes_added.add(next_kmer)

                if prev_kmer in filtered_kmers and prev_kmer not in nodes_added:
                    self.graph.add_node(prev_kmer, coverage=filtered_kmers[prev_kmer])
                    nodes_added.add(prev_kmer)

                # 添加边
                if next_kmer in filtered_kmers:
                    self.graph.add_edge(kmer, next_kmer)

        print(f"De Bruijn图构建完成，包含 {self.graph.number_of_nodes()} 个节点和 {self.graph.number_of_edges()} 条边")

    def simplify_graph(self):
        """简化De Bruijn图"""
        print("简化De Bruijn图...")

        # 移除孤立节点
        isolated_nodes = list(nx.isolates(self.graph))
        self.graph.remove_nodes_from(isolated_nodes)

        # 简化简单的路径（压缩线性路径）
        self._compress_linear_paths()

        print(f"简化后图包含 {self.graph.number_of_nodes()} 个节点和 {self.graph.number_of_edges()} 条边")

    def _compress_linear_paths(self):
        """压缩线性路径"""
        nodes_to_remove = []
        edges_to_add = []

        for node in list(self.graph.nodes()):
            in_degree = self.graph.in_degree(node)
            out_degree = self.graph.out_degree(node)

            # 如果是线性节点（入度=出度=1）
            if in_degree == 1 and out_degree == 1:
                predecessors = list(self.graph.predecessors(node))
                successors = list(self.graph.successors(node))

                if predecessors and successors:
                    pred = predecessors[0]
                    succ = successors[0]

                    # 添加压缩边
                    edges_to_add.append((pred, succ))

                    # 移除中间节点
                    nodes_to_remove.append(node)

        # 执行压缩
        self.graph.add_edges_from(edges_to_add)
        self.graph.remove_nodes_from(nodes_to_remove)

    def find_contigs(self):
        """从图中提取contigs"""
        print("提取contigs...")

        contigs = []
        visited = set()

        # 寻找起始节点（入度为0或较小的节点）
        start_nodes = [node for node in self.graph.nodes() if self.graph.in_degree(node) == 0]

        # 如果没有入度为0的节点，选择覆盖度最高的节点
        if not start_nodes:
            start_nodes = [max(self.graph.nodes(), key=lambda n: self.graph.nodes[n]['coverage'])]

        for start_node in start_nodes:
            if start_node in visited:
                continue

            # 从起始节点开始寻找路径
            path = self._find_path_from_node(start_node, visited)
            if len(path) >= self.k:
                contig = self._path_to_contig(path)
                contigs.append(contig)

        print(f"找到 {len(contigs)} 个contigs")
        return contigs

    def _find_path_from_node(self, start_node, visited):
        """从节点开始寻找路径"""
        path = [start_node]
        current = start_node
        visited.add(current)

        while True:
            successors = list(self.graph.successors(current))

            # 如果没有后继节点，停止
            if not successors:
                break

            # 如果有多个后继，选择覆盖度最高的
            if len(successors) > 1:
                best_successor = max(successors,
                                   key=lambda n: self.graph.nodes[n]['coverage'])
            else:
                best_successor = successors[0]

            # 如果后继节点已访问过，停止
            if best_successor in visited:
                break

            path.append(best_successor)
            visited.add(best_successor)
            current = best_successor

        return path

    def _path_to_contig(self, path):
        """将路径转换为contig序列"""
        if not path:
            return ""

        contig = path[0]
        for i in range(1, len(path)):
            contig += path[i][-1]  # 添加最后一个字符

        return contig

    def save_graph(self, filename):
        """保存图到文件"""
        data = {
            'k': self.k,
            'min_coverage': self.min_coverage,
            'graph': self.graph,
            'kmer_counts': dict(self.kmer_counts)
        }

        with open(filename, 'wb') as f:
            pickle.dump(data, f)

        print(f"De Bruijn图已保存到 {filename}")

    def load_graph(self, filename):
        """从文件加载图"""
        with open(filename, 'rb') as f:
            data = pickle.load(f)

        self.k = data['k']
        self.min_coverage = data['min_coverage']
        self.graph = data['graph']
        self.kmer_counts = Counter(data['kmer_counts'])

        print(f"De Bruijn图已从 {filename} 加载")

# 使用示例
def debruijn_graph_example():
    # 模拟测序数据
    reads = [
        "ACGTACGTACGTACGTACGT",
        "CGTACGTACGTACGTACGTA",
        "GTACGTACGTACGTACGTAC",
        # ... 更多读段
    ]

    # 创建并行De Bruijn图
    graph_builder = ParallelDeBruijnGraph(k=15, min_coverage=2)

    # 并行k-mer计数
    graph_builder.parallel_kmer_counting(reads)

    # 构建图
    graph_builder.build_graph_from_kmers()

    # 简化图
    graph_builder.simplify_graph()

    # 提取contigs
    contigs = graph_builder.find_contigs()

    print("组装结果:")
    for i, contig in enumerate(contigs):
        print(f"Contig {i+1}: {contig[:50]}... (长度: {len(contig)})")

    # 保存图
    graph_builder.save_graph("debruijn_graph.pkl")

if __name__ == "__main__":
    debruijn_graph_example()
```

### 7.2.2 OLC算法并行实现

#### OLC算法概述

Overlap-Layout-Consensus（OLC）算法是长读段组装的主要方法，特别适合PacBio和Oxford Nanopore等长读段技术。

**OLC步骤**：
1. **Overlap**：计算读段间的重叠
2. **Layout**：构建读段的布局
3. **Consensus**：生成一致序列

#### 并行OLC实现

```cpp
// 并行OLC算法实现
#include <omp.h>
#include <vector>
#include <string>
#include <algorithm>
#include <unordered_map>
#include <thread>

class ParallelOLCAssembler {
private:
    struct Read {
        std::string sequence;
        int id;
        std::string header;
    };

    struct Overlap {
        int read1_id, read2_id;
        int overlap_length;
        int start1, end1, start2, end2;
        bool is_reverse_complement;
        float identity;
    };

    std::vector<Read> reads;
    std::vector<Overlap> overlaps;
    std::vector<bool> used_reads;

public:
    ParallelOLCAssembler() {}

    void load_reads(const std::string& fasta_file) {
        // 读取FASTA文件
        std::ifstream file(fasta_file);
        std::string line, header, sequence;

        while (std::getline(file, line)) {
            if (line[0] == '>') {
                if (!sequence.empty()) {
                    reads.push_back({sequence, reads.size(), header});
                    sequence.clear();
                }
                header = line.substr(1);
            } else {
                sequence += line;
            }
        }

        if (!sequence.empty()) {
            reads.push_back({sequence, reads.size(), header});
        }

        used_reads.resize(reads.size(), false);
        std::cout << "加载了 " << reads.size() << " 个读段" << std::endl;
    }

    void parallel_overlap_detection(int min_overlap = 50, float min_identity = 0.8) {
        std::cout << "开始并行重叠检测..." << std::endl;

        #pragma omp parallel for schedule(dynamic)
        for (int i = 0; i < reads.size(); i++) {
            for (int j = i + 1; j < reads.size(); j++) {
                // 计算正向重叠
                auto overlap1 = find_overlap(reads[i].sequence, reads[j].sequence, min_overlap);
                if (overlap1.overlap_length > 0 && overlap1.identity >= min_identity) {
                    overlap1.read1_id = i;
                    overlap1.read2_id = j;
                    overlap1.is_reverse_complement = false;

                    #pragma omp critical
                    {
                        overlaps.push_back(overlap1);
                    }
                }

                // 计算反向互补重叠
                std::string rc_seq = reverse_complement(reads[j].sequence);
                auto overlap2 = find_overlap(reads[i].sequence, rc_seq, min_overlap);
                if (overlap2.overlap_length > 0 && overlap2.identity >= min_identity) {
                    overlap2.read1_id = i;
                    overlap2.read2_id = j;
                    overlap2.is_reverse_complement = true;

                    #pragma omp critical
                    {
                        overlaps.push_back(overlap2);
                    }
                }
            }
        }

        std::cout << "找到 " << overlaps.size() << " 个重叠" << std::endl;
    }

private:
    Overlap find_overlap(const std::string& seq1, const std::string& seq2, int min_overlap) {
        Overlap result{0, 0, 0, 0, 0, 0, false, 0.0};

        // 尝试不同的重叠长度
        for (int overlap_len = std::min(seq1.length(), seq2.length()); overlap_len >= min_overlap; overlap_len--) {
            // 尝试seq1的后缀与seq2的前缀重叠
            std::string suffix1 = seq1.substr(seq1.length() - overlap_len);
            std::string prefix2 = seq2.substr(0, overlap_len);

            if (suffix1 == prefix2) {
                result.overlap_length = overlap_len;
                result.start1 = seq1.length() - overlap_len;
                result.end1 = seq1.length();
                result.start2 = 0;
                result.end2 = overlap_len;
                result.identity = 1.0;
                return result;
            }

            // 尝试seq1的前缀与seq2的后缀重叠
            std::string prefix1 = seq1.substr(0, overlap_len);
            std::string suffix2 = seq2.substr(seq2.length() - overlap_len);

            if (prefix1 == suffix2) {
                result.overlap_length = overlap_len;
                result.start1 = 0;
                result.end1 = overlap_len;
                result.start2 = seq2.length() - overlap_len;
                result.end2 = seq2.length();
                result.identity = 1.0;
                return result;
            }
        }

        return result;
    }

    std::string reverse_complement(const std::string& seq) {
        std::string rc = seq;
        std::reverse(rc.begin(), rc.end());
        for (char& c : rc) {
            switch (c) {
                case 'A': c = 'T'; break;
                case 'T': c = 'A'; break;
                case 'C': c = 'G'; break;
                case 'G': c = 'C'; break;
            }
        }
        return rc;
    }

public:
    std::vector<std::string> parallel_layout_and_consensus() {
        std::cout << "开始并行布局和一致性序列生成..." << std::endl;

        // 构建重叠图
        std::unordered_map<int, std::vector<Overlap>> graph;
        for (const auto& overlap : overlaps) {
            graph[overlap.read1_id].push_back(overlap);
        }

        std::vector<std::string> contigs;

        // 并行处理不同的连通分量
        #pragma omp parallel for
        for (int start_read = 0; start_read < reads.size(); start_read++) {
            if (!used_reads[start_read]) {
                std::string contig = assemble_from_read(start_read, graph);
                if (!contig.empty()) {
                    #pragma omp critical
                    {
                        contigs.push_back(contig);
                    }
                }
            }
        }

        std::cout << "组装完成，生成 " << contigs.size() << " 个contigs" << std::endl;
        return contigs;
    }

private:
    std::string assemble_from_read(int start_read, const std::unordered_map<int, std::vector<Overlap>>& graph) {
        if (used_reads[start_read]) {
            return "";
        }

        std::vector<int> path;
        std::vector<Overlap> path_overlaps;

        int current_read = start_read;
        path.push_back(current_read);
        used_reads[current_read] = true;

        while (true) {
            // 寻找最佳的下一个读段
            Overlap best_overlap;
            best_overlap.overlap_length = 0;

            if (graph.find(current_read) != graph.end()) {
                for (const auto& overlap : graph.at(current_read)) {
                    if (!used_reads[overlap.read2_id] && overlap.overlap_length > best_overlap.overlap_length) {
                        best_overlap = overlap;
                    }
                }
            }

            if (best_overlap.overlap_length == 0) {
                break;  // 没有更多重叠
            }

            // 添加到路径
            path.push_back(best_overlap.read2_id);
            path_overlaps.push_back(best_overlap);
            used_reads[best_overlap.read2_id] = true;
            current_read = best_overlap.read2_id;
        }

        // 生成一致序列
        if (path.size() < 2) {
            return "";
        }

        return generate_consensus_sequence(path, path_overlaps);
    }

    std::string generate_consensus_sequence(const std::vector<int>& path,
                                          const std::vector<Overlap>& overlaps) {
        if (path.empty()) return "";

        std::string consensus = reads[path[0]].sequence;

        for (size_t i = 0; i < overlaps.size(); i++) {
            const auto& overlap = overlaps[i];
            const std::string& next_seq = reads[path[i + 1]].sequence;

            // 合并序列
            if (overlap.start2 == 0) {
                // 正向重叠
                consensus += next_seq.substr(overlap.end2);
            } else {
                // 反向重叠，需要反向互补
                std::string rc_seq = reverse_complement(next_seq);
                consensus += rc_seq.substr(overlap.end2);
            }
        }

        return consensus;
    }
};

// 使用示例
void olc_assembly_example() {
    ParallelOLCAssembler assembler;

    // 加载读段
    assembler.load_reads("long_reads.fasta");

    // 并行重叠检测
    assembler.parallel_overlap_detection(50, 0.8);

    // 并行组装
    std::vector<std::string> contigs = assembler.parallel_layout_and_consensus();

    // 保存结果
    std::ofstream output("contigs.fasta");
    for (size_t i = 0; i < contigs.size(); i++) {
        output << ">contig_" << i + 1 << std::endl;
        output << contigs[i] << std::endl;
    }
    output.close();

    std::cout << "组装结果已保存到 contigs.fasta" << std::endl;
}
```

### 7.2.3 基因组拼接优化策略

#### 错误校正

```python
# 基因组拼接优化：错误校正
class ErrorCorrection:
    def __init__(self, k=25, min_coverage=3):
        self.k = k
        self.min_coverage = min_coverage
        self.kmer_counts = {}

    def count_kmers(self, reads):
        """统计k-mer频率"""
        for read in reads:
            read = read.upper()
            for i in range(len(read) - self.k + 1):
                kmer = read[i:i + self.k]
                if self._is_valid_kmer(kmer):
                    self.kmer_counts[kmer] = self.kmer_counts.get(kmer, 0) + 1

    def correct_read_errors(self, read):
        """校正读段中的错误"""
        corrected_read = list(read.upper())
        errors_found = 0

        for i in range(len(read) - self.k + 1):
            kmer = read[i:i + self.k]

            if not self._is_valid_kmer(kmer):
                continue

            # 如果k-mer频率很低，可能是错误
            if self.kmer_counts.get(kmer, 0) < self.min_coverage:
                # 寻找最相似的高频k-mer
                best_kmer = self._find_best_kmer(kmer)

                if best_kmer and self.kmer_counts.get(best_kmer, 0) >= self.min_coverage:
                    # 校正中心位置的碱基
                    center_pos = i + self.k // 2
                    if center_pos < len(corrected_read):
                        corrected_read[center_pos] = best_kmer[self.k // 2]
                        errors_found += 1

        return ''.join(corrected_read), errors_found

    def _find_best_kmer(self, kmer):
        """寻找最相似的高频k-mer"""
        best_kmer = None
        best_score = -1

        # 尝试所有可能的单碱基替换
        for i in range(len(kmer)):
            for base in 'ACGT':
                if base != kmer[i]:
                    candidate = kmer[:i] + base + kmer[i+1:]
                    frequency = self.kmer_counts.get(candidate, 0)
                    if frequency > best_score:
                        best_score = frequency
                        best_kmer = candidate

        return best_kmer

    def _is_valid_kmer(self, kmer):
        return all(base in 'ACGT' for base in kmer) and len(kmer) == self.k

# 使用示例
def error_correction_example():
    reads = ["ACGTACGTACGT", "ACGTACGAACGT", "ACGTACGTTCGT"]  # 包含错误的读段

    # 错误校正
    corrector = ErrorCorrection(k=8, min_coverage=2)
    corrector.count_kmers(reads)

    corrected_reads = []
    total_errors = 0

    for read in reads:
        corrected_read, errors = corrector.correct_read_errors(read)
        corrected_reads.append(corrected_read)
        total_errors += errors

    print(f"校正了 {total_errors} 个错误")
    for i, (original, corrected) in enumerate(zip(reads, corrected_reads)):
        print(f"读段 {i+1}: {original} -> {corrected}")
```

#### 嵌合体检测和去除

```python
# 嵌合体检测
class ChimeraDetection:
    def __init__(self, k=16):
        self.k = k

    def detect_chimeras(self, contigs, reference_kmers):
        """检测嵌合体contigs"""
        chimeras = []

        for contig_id, contig in enumerate(contigs):
            if len(contig) < 1000:  # 太短的contig跳过
                continue

            # 滑动窗口检测嵌合体断点
            breakpoints = self._find_chimera_breakpoints(contig, reference_kmers)

            if breakpoints:
                chimeras.append({
                    'contig_id': contig_id,
                    'contig_length': len(contig),
                    'breakpoints': breakpoints,
                    'is_chimera': True
                })

        return chimeras

    def _find_chimera_breakpoints(self, contig, reference_kmers):
        """寻找嵌合体断点"""
        breakpoints = []
        window_size = 100
        step_size = 50

        for i in range(0, len(contig) - window_size, step_size):
            window = contig[i:i + window_size]
            kmers_in_window = self._extract_kmers(window)

            # 计算窗口内k-mer的频率分布
            frequency_scores = []
            for kmer in kmers_in_window:
                frequency = reference_kmers.get(kmer, 0)
                frequency_scores.append(frequency)

            # 检测频率分布的突变
            if len(frequency_scores) > 10:
                median_score = np.median(frequency_scores)
                std_score = np.std(frequency_scores)

                # 如果标准差很大，可能存在断点
                if std_score > median_score * 0.5:
                    breakpoints.append(i + window_size // 2)

        return breakpoints

    def _extract_kmers(self, sequence):
        kmers = set()
        for i in range(len(sequence) - self.k + 1):
            kmer = sequence[i:i + self.k]
            if all(base in 'ACGT' for base in kmer):
                kmers.add(kmer)
        return list(kmers)
```

## 7.3 变异检测加速

### 7.3.1 并行SNP检测

#### SNP检测原理

单核苷酸多态性（SNP）检测是基因组分析的重要任务，用于识别个体间的遗传差异。

**检测流程**：
1. 读段比对到参考基因组
2. 识别候选变异位点
3. 计算基因型概率
4. 过滤和注释

#### 并行SNP检测实现

```python
# 并行SNP检测
import pysam
import numpy as np
from multiprocessing import Pool, cpu_count
import pandas as pd
from scipy import stats
from collections import defaultdict, Counter

class ParallelSNPDetection:
    def __init__(self, reference_genome, min_quality=20, min_depth=10, min_af=0.05):
        """
        初始化并行SNP检测器

        Args:
            reference_genome: 参考基因组文件路径
            min_quality: 最小碱基质量
            min_depth: 最小覆盖深度
            min_af: 最小等位基因频率
        """
        self.reference_genome = reference_genome
        self.min_quality = min_quality
        self.min_depth = min_depth
        self.min_af = min_af
        self.ref_sequence = None

    def load_reference_genome(self):
        """加载参考基因组"""
        from Bio import SeqIO
        records = list(SeqIO.parse(self.reference_genome, "fasta"))
        if records:
            self.ref_sequence = str(records[0].seq).upper()
            print(f"参考基因组加载完成，长度: {len(self.ref_sequence)}")
        else:
            raise ValueError("无法加载参考基因组")

    def process_genomic_region(self, region_params):
        """处理基因组区域"""
        bam_file, start_pos, end_pos, region_id = region_params

        if not self.ref_sequence:
            self.load_reference_genome()

        snps = []

        try:
            # 打开BAM文件
            samfile = pysam.AlignmentFile(bam_file, "rb")

            # 处理每个位置
            for pos in range(start_pos, end_pos):
                if pos >= len(self.ref_sequence):
                    break

                ref_base = self.ref_sequence[pos]
                if ref_base not in 'ACGT':
                    continue

                # 获取pileup信息
                pileup_column = samfile.pileup(
                    reference=None,
                    start=pos,
                    end=pos + 1,
                    max_depth=10000,
                    stepper='all',
                    truncate=True
                )

                for pileup_column in samfile.pileup(
                    reference=None,
                    start=pos,
                    end=pos + 1,
                    max_depth=10000,
                    stepper='all',
                    truncate=True
                ):
                    if pileup_column.pos != pos:
                        continue

                    # 收集碱基信息
                    bases = []
                    qualities = []

                    for pileupread in pileup_column.pileups:
                        if not pileupread.is_del and not pileupread.is_refskip:
                            if pileupread.alignment.query_qualities:
                                base_qual = pileupread.alignment.query_qualities[pileupread.query_position]
                                if base_qual >= self.min_quality:
                                    base = pileupread.alignment.query_sequence[pileupread.query_position]
                                    if base in 'ACGT':
                                        bases.append(base)
                                        qualities.append(base_qual)

                    # 检测SNP
                    if len(bases) >= self.min_depth:
                        snp_result = self._detect_snp(pos, ref_base, bases, qualities)
                        if snp_result:
                            snps.append(snp_result)

            samfile.close()

        except Exception as e:
            print(f"处理区域 {region_id} 时出错: {e}")

        return snps

    def _detect_snp(self, position, ref_base, bases, qualities):
        """检测单个位置的SNP"""
        if not bases:
            return None

        # 统计碱基频率
        base_counts = Counter(bases)
        total_bases = len(bases)

        # 找出非参考碱基
        alt_bases = {base: count for base, count in base_counts.items() if base != ref_base}

        if not alt_bases:
            return None

        # 计算等位基因频率
        results = []
        for alt_base, count in alt_bases.items():
            af = count / total_bases

            # 过滤低频变异
            if af < self.min_af:
                continue

            # 计算质量得分
            avg_quality = np.mean([q for b, q in zip(bases, qualities) if b == alt_base])

            # 计算p值（假设符合二项分布）
            p_value = 1 - stats.binom.cdf(count - 1, total_bases, 0.01)

            results.append({
                'position': position,
                'ref_base': ref_base,
                'alt_base': alt_base,
                'depth': total_bases,
                'alt_count': count,
                'allele_frequency': af,
                'avg_quality': avg_quality,
                'p_value': p_value
            })

        # 返回最显著的SNP
        if results:
            best_snp = max(results, key=lambda x: x['allele_frequency'])
            return best_snp

        return None

    def parallel_snp_detection(self, bam_file, region_size=10000):
        """并行SNP检测主函数"""
        if not self.ref_sequence:
            self.load_reference_genome()

        genome_length = len(self.ref_sequence)
        num_regions = (genome_length + region_size - 1) // region_size

        # 创建区域参数
        region_params = []
        for i in range(num_regions):
            start = i * region_size
            end = min(start + region_size, genome_length)
            region_params.append((bam_file, start, end, i))

        print(f"将基因组分为 {num_regions} 个区域进行并行处理")

        # 并行处理
        num_processes = min(cpu_count(), num_regions)
        with Pool(processes=num_processes) as pool:
            results = pool.map(self.process_genomic_region, region_params)

        # 合并结果
        all_snps = []
        for region_snps in results:
            all_snps.extend(region_snps)

        print(f"检测到 {len(all_snps)} 个SNP")

        return all_snps

    def filter_snps(self, snps, quality_threshold=30, depth_threshold=20):
        """过滤SNP结果"""
        filtered_snps = []

        for snp in snps:
            if (snp['avg_quality'] >= quality_threshold and
                snp['depth'] >= depth_threshold and
                snp['p_value'] < 0.05):

                filtered_snps.append(snp)

        print(f"过滤后剩余 {len(filtered_snps)} 个高质量SNP")
        return filtered_snps

    def annotate_snps(self, snps, gene_annotations=None):
        """注释SNP"""
        annotated_snps = []

        for snp in snps:
            annotation = {
                'position': snp['position'],
                'ref_base': snp['ref_base'],
                'alt_base': snp['alt_base'],
                'depth': snp['depth'],
                'allele_frequency': snp['allele_frequency'],
                'quality': snp['avg_quality']
            }

            # 简单的功能注释
            if gene_annotations:
                annotation['gene'] = self._find_gene(snp['position'], gene_annotations)
                annotation['effect'] = self._predict_effect(snp, annotation['gene'])

            annotated_snps.append(annotation)

        return annotated_snps

    def _find_gene(self, position, gene_annotations):
        """查找位置所在的基因"""
        for gene in gene_annotations:
            if gene['start'] <= position <= gene['end']:
                return gene['name']
        return "intergenic"

    def _predict_effect(self, snp, gene):
        """预测SNP的效应"""
        if gene == "intergenic":
            return "intergenic_variant"

        # 简单的效应预测
        effects = []

        if snp['ref_base'] in 'AT' and snp['alt_base'] in 'GC':
            effects.append("transition")
        elif snp['ref_base'] in 'GC' and snp['alt_base'] in 'AT':
            effects.append("transversion")
        else:
            effects.append("substitution")

        return ", ".join(effects)

    def save_snp_results(self, snps, output_file):
        """保存SNP结果"""
        df = pd.DataFrame(snps)
        df.to_csv(output_file, index=False, sep='\t')
        print(f"SNP结果已保存到 {output_file}")

# 使用示例
def parallel_snp_example():
    # 初始化SNP检测器
    snp_detector = ParallelSNPDetection(
        reference_genome="reference.fasta",
        min_quality=20,
        min_depth=10,
        min_af=0.05
    )

    # 并行检测SNP
    bam_file = "sample.bam"
    snps = snp_detector.parallel_snp_detection(bam_file, region_size=50000)

    # 过滤结果
    filtered_snps = snp_detector.filter_snps(snps, quality_threshold=30)

    # 保存结果
    snp_detector.save_snp_results(filtered_snps, "snps_results.tsv")

    print(f"检测到 {len(filtered_snps)} 个高质量SNP")

if __name__ == "__main__":
    parallel_snp_example()
```

### 7.3.2 Indel检测算法优化

#### Indel检测原理

插入缺失（Indel）检测比SNP检测更复杂，因为需要考虑读段的CIGAR字符串和比对模式。

**检测挑战**：
- 读段比对的复杂性
- Indel大小的变异性
- 周围序列的复杂性

#### 优化的Indel检测

```python
# 优化的Indel检测
class OptimizedIndelDetection:
    def __init__(self, min_support=3, min_af=0.05):
        self.min_support = min_support
        self.min_af = min_af

    def detect_indels_from_pileup(self, pileup_column):
        """从pileup数据检测Indel"""
        indels = []

        # 收集所有的插入和缺失事件
        insertions = defaultdict(int)
        deletions = defaultdict(int)

        for pileupread in pileup_column.pileups:
            if pileupread.indel != 0:
                if pileupread.indel > 0:
                    # 插入事件
                    insertion_seq = pileupread.alignment.query_sequence[
                        pileupread.query_position + 1:
                        pileupread.query_position + 1 + pileupread.indel
                    ]
                    insertions[insertion_seq] += 1
                else:
                    # 缺失事件
                    deletion_length = abs(pileupread.indel)
                    deletions[deletion_length] += 1

        # 分析插入
        total_reads = pileup_column.nsegments
        for seq, count in insertions.items():
            if count >= self.min_support:
                af = count / total_reads
                if af >= self.min_af:
                    indels.append({
                        'type': 'insertion',
                        'sequence': seq,
                        'length': len(seq),
                        'count': count,
                        'frequency': af,
                        'position': pileup_column.pos
                    })

        # 分析缺失
        for length, count in deletions.items():
            if count >= self.min_support:
                af = count / total_reads
                if af >= self.min_af:
                    indels.append({
                        'type': 'deletion',
                        'length': length,
                        'count': count,
                        'frequency': af,
                        'position': pileup_column.pos
                    })

        return indels

    def parallel_indel_detection(self, bam_file, reference_genome):
        """并行Indel检测"""
        import multiprocessing as mp

        # 获取染色体信息
        samfile = pysam.AlignmentFile(bam_file, "rb")
        chromosomes = samfile.references
        samfile.close()

        # 分割染色体进行并行处理
        tasks = []
        for chrom in chromosomes:
            chrom_length = samfile.get_reference_length(chrom)
            tasks.append((bam_file, chrom, 0, chrom_length))

        # 并行处理
        with mp.Pool() as pool:
            results = pool.map(self._process_chromosome_indels, tasks)

        # 合并结果
        all_indels = []
        for chromosome_indels in results:
            all_indels.extend(chromosome_indels)

        return all_indels

    def _process_chromosome_indels(self, task):
        """处理单个染色体的Indel检测"""
        bam_file, chrom, start, end = task
        chromosome_indels = []

        samfile = pysam.AlignmentFile(bam_file, "rb")

        # 按窗口处理
        window_size = 10000
        for window_start in range(start, end, window_size):
            window_end = min(window_start + window_size, end)

            try:
                for pileup_column in samfile.pileup(
                    chrom, window_start, window_end,
                    max_depth=10000, stepper='all'
                ):
                    indels = self.detect_indels_from_pileup(pileup_column)
                    chromosome_indels.extend(indels)

            except Exception as e:
                print(f"处理 {chrom}:{window_start}-{window_end} 时出错: {e}")

        samfile.close()
        return chromosome_indels
```

### 7.3.3 结构变异分析并行化

#### 结构变异类型

结构变异（Structural Variation, SV）包括：
- **插入（Insertions）**：>50bp的序列插入
- **缺失（Deletions）**：>50bp的序列缺失
- **倒位（Inversions）**：序列方向反转
- **易位（Translocations）**：序列位置改变
- **拷贝数变异（CNV）**：拷贝数的增加或减少

#### 并行结构变异检测

```python
# 并行结构变异检测
class ParallelStructuralVariantDetection:
    def __init__(self, min_sv_size=50, min_support=5):
        self.min_sv_size = min_sv_size
        self.min_support = min_support

    def detect_sv_from_bam(self, bam_file):
        """从BAM文件检测结构变异"""
        samfile = pysam.AlignmentFile(bam_file, "rb")

        # 收集异常比对
        discordant_pairs = self._find_discordant_pairs(samfile)
        split_reads = self._find_split_reads(samfile)

        samfile.close()

        # 分析结构变异
        svs = []

        # 分析插入
        insertions = self._analyze_insertions(discordant_pairs)
        svs.extend(insertions)

        # 分析缺失
        deletions = self._analyze_deletions(discordant_pairs)
        svs.extend(deletions)

        # 分析倒位
        inversions = self._analyze_inversions(split_reads)
        svs.extend(inversions)

        # 分析易位
        translocations = self._analyze_translocations(discordant_pairs)
        svs.extend(translocations)

        return svs

    def _find_discordant_pairs(self, samfile):
        """寻找异常比对的读段对"""
        discordant_pairs = []

        for read in samfile:
            if read.is_paired and not read.is_unmapped and not read.mate_is_unmapped:
                # 检查插入片段大小
                insert_size = abs(read.template_length)

                # 正常插入片段大小通常在200-500bp
                if insert_size > 1000 or insert_size < 50:
                    discordant_pairs.append({
                        'read1': read,
                        'read2': samfile.mate(read),
                        'insert_size': insert_size
                    })

        return discordant_pairs

    def _find_split_reads(self, samfile):
        """寻找分割读段"""
        split_reads = []

        for read in samfile:
            if read.cigar and len(read.cigar) > 1:
                # 检查CIGAR字符串中的分割
                for operation, length in read.cigar:
                    if operation in [3, 4]:  # 3=跳跃，4=软剪切
                        if length > self.min_sv_size:
                            split_reads.append({
                                'read': read,
                                'operation': operation,
                                'length': length
                            })

        return split_reads

    def _analyze_insertions(self, discordant_pairs):
        """分析插入变异"""
        insertions = []

        # 按染色体和位置分组
        insertion_candidates = defaultdict(list)

        for pair in discordant_pairs:
            if pair['insert_size'] > 1000:  # 大插入
                key = (pair['read1'].reference_name, pair['read1'].reference_start)
                insertion_candidates[key].append(pair)

        # 聚类分析
        for key, candidates in insertion_candidates.items():
            if len(candidates) >= self.min_support:
                chromosome, position = key
                avg_insert_size = np.mean([c['insert_size'] for c in candidates])

                insertions.append({
                    'type': 'insertion',
                    'chromosome': chromosome,
                    'position': position,
                    'size': avg_insert_size,
                    'support': len(candidates),
                    'confidence': 'high' if len(candidates) > self.min_support * 2 else 'medium'
                })

        return insertions

    def _analyze_deletions(self, discordant_pairs):
        """分析缺失变异"""
        deletions = []

        # 寻找反向的读段对
        deletion_candidates = defaultdict(list)

        for pair in discordant_pairs:
            read1, read2 = pair['read1'], pair['read2']

            if (read1.is_reverse != read2.is_reverse and
                read1.reference_start > read2.reference_start):
                # 可能的缺失
                key = (read1.reference_name, read2.reference_start)
                deletion_candidates[key].append(pair)

        # 聚类分析
        for key, candidates in deletion_candidates.items():
            if len(candidates) >= self.min_support:
                chromosome, start_pos = key
                end_positions = [c['read1'].reference_start for c in candidates]
                end_pos = int(np.median(end_positions))

                deletions.append({
                    'type': 'deletion',
                    'chromosome': chromosome,
                    'start': start_pos,
                    'end': end_pos,
                    'size': end_pos - start_pos,
                    'support': len(candidates),
                    'confidence': 'high' if len(candidates) > self.min_support * 2 else 'medium'
                })

        return deletions

    def _analyze_inversions(self, split_reads):
        """分析倒位变异"""
        inversions = []

        # 寻找支持倒位的分割读段
        inversion_candidates = defaultdict(list)

        for split in split_reads:
            read = split['read']
            if read.cigar[0][0] == 4 and read.cigar[-1][0] == 4:  # 软剪切
                key = (read.reference_name, read.reference_start)
                inversion_candidates[key].append(split)

        # 分析倒位
        for key, candidates in inversion_candidates.items():
            if len(candidates) >= self.min_support:
                chromosome, position = key

                inversions.append({
                    'type': 'inversion',
                    'chromosome': chromosome,
                    'position': position,
                    'support': len(candidates),
                    'confidence': 'medium'
                })

        return inversions

    def _analyze_translocations(self, discordant_pairs):
        """分析易位变异"""
        translocations = []

        # 寻找不同染色体间的异常连接
        translocation_candidates = defaultdict(list)

        for pair in discordant_pairs:
            read1, read2 = pair['read1'], pair['read2']

            if (read1.reference_name != read2.reference_name and
                read1.reference_name != '*' and read2.reference_name != '*'):
                key = tuple(sorted([
                    (read1.reference_name, read1.reference_start),
                    (read2.reference_name, read2.reference_start)
                ]))
                translocation_candidates[key].append(pair)

        # 分析易位
        for key, candidates in translocation_candidates.items():
            if len(candidates) >= self.min_support:
                translocations.append({
                    'type': 'translocation',
                    'breakpoints': key,
                    'support': len(candidates),
                    'confidence': 'high' if len(candidates) > self.min_support * 2 else 'medium'
                })

        return translocations

# 使用示例
def parallel_sv_detection_example():
    sv_detector = ParallelStructuralVariantDetection(
        min_sv_size=50,
        min_support=5
    )

    bam_file = "sample.bam"
    svs = sv_detector.detect_sv_from_bam(bam_file)

    print(f"检测到 {len(svs)} 个结构变异:")
    for sv in svs:
        print(f"  {sv['type']}: {sv}")

    return svs
```

## 7.4 本章小结

本章详细介绍了基因组分析中的并行化技术：

1. **序列比对并行化**：
   - 并行BLAST搜索的实现和优化
   - GPU加速的Smith-Waterman算法
   - k-mer索引构建和快速查找

2. **基因组装优化**：
   - 并行De Bruijn图构建
   - OLC算法的并行实现
   - 错误校正和嵌合体检测

3. **变异检测加速**：
   - 并行SNP检测实现
   - Indel检测算法优化
   - 结构变异分析并行化

这些技术大大提高了基因组分析的效率，使得处理大规模测序数据成为可能。在实际应用中，需要根据具体的数据类型和分析需求选择合适的并行化策略。

## 练习题

1. **编程题**：实现一个基于MPI的并行BLAST搜索程序。
2. **分析题**：比较De Bruijn图和OLC算法的优缺点。
3. **设计题**：设计一个GPU加速的序列比对算法。
4. **优化题**：为现有的SNP检测算法添加并行化支持。
5. **研究题**：调研最新的长读段组装算法及其并行化实现。